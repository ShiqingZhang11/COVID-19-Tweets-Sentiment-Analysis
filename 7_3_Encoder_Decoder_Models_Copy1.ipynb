{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ShiqingZhang11/COVID-19-Tweets-Sentiment-Analysis/blob/main/7_3_Encoder_Decoder_Models_Copy1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XL_VZEvWQKvG"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Shivanandroy/T5-Finetuning-PyTorch/blob/main/notebook/T5_Fine_tuning_with_PyTorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "jp-MarkdownHeadingCollapsed": true,
        "id": "n5frP6IhQKvI"
      },
      "source": [
        "# Table of Contents\n",
        "- 1. [Project Purpose](#toc1_)    \n",
        "- 2. [Library Initialization](#toc2_)    \n",
        "- 3. [LLMs Loading](#toc3_)    \n",
        "- 4. [Item List Loading](#toc4_)    \n",
        "- 5. [Reformulation of the Question](#toc5_)    \n",
        "- 6. [Likert Scale](#toc6_)    \n",
        "- 7. [Test](#toc7_)\n",
        "  - 7.0 [Machine Learning & Deep Learning Models - BASELINE](#toc7_0_)\n",
        "  - 7.1 [Decoder-Only Models](#toc7_1_)\n",
        "  - 7.2 [Encoder-Only Models](#toc7_2_)\n",
        "  - 7.3 [Encoder-Decoder Models](#toc7_3_)\n",
        "\n",
        "<!-- vscode-jupyter-toc-config\n",
        "\tnumbering=true\n",
        "\tanchor=true\n",
        "\tflat=false\n",
        "\tminLevel=1\n",
        "\tmaxLevel=7\n",
        "\t/vscode-jupyter-toc-config -->\n",
        "<!-- THIS CELL WILL BE REPLACED ON TOC UPDATE. DO NOT WRITE YOUR TEXT IN THIS CELL -->"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "jp-MarkdownHeadingCollapsed": true,
        "id": "qtkQlCxrQKvJ"
      },
      "source": [
        "# 1. [Project purpose](#toc1_)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pXEeScLCQKvJ"
      },
      "source": [
        "Individuals with schizophrenia face a 15-20 year reduction in life expectancy and recovery rates as low as 13.5%, underscoring the urgent need for better care standards. The ambition of PRIME-AI (Person-Reported Insights and Measures Enhanced by AI) is to radically transform the care of individuals with schizophrenia by focusing on their experiences as people, not just as diagnoses and patients.\n",
        "\n",
        "By integrating Artificial Intelligence (AI) with established psychometric methods, we aim to set a new standard in evaluating health-related quality of life (HRQoL), a patient-reported outcome typically measured through questionnaires. PRIME-AI combines Computerized Adaptive Testing (CAT) based on Item Response Theory (IRT) with advanced Natural Language Processing (NLP) technologies, optimizing the strengths of both methodologies. CAT-IRT offers precision in selecting the most relevant questions based on individual responses and provides valid scores through a unique metric. The NLP technologies, which may include large language models, rule-based systems, machine learning, and hybrid methods, enhance the ability to capture and analyze complex patient qualitative data. NLP approaches, including the use of conversational agents, enhance engagement by adapting and reformulating questions for better comprehension, particularly for individuals with cognitive impairments. NLP enables the synthesis of rich qualitative insights, providing a deeper understanding of individual answers.\n",
        "\n",
        "The groundbreaking integration of CAT-IRT and AI enables a comprehensive analysis of HRQoL by combining, for the first time, scientifically rigorous quantitative measures with deeply personalized qualitative insights crucial for clinical decision-making. PRIME-AI represents a major advancement in addressing the needs of these extremely vulnerable and neglected individuals. This inclusive approach is key to enhancing the quality and personalization of care, ultimately facilitating successful recovery.\n",
        "\n",
        "![](interaction_project_dag.png)\n",
        "\n",
        "This notebook will focus on the conversion of natural language sentences from patients to a Likert Scale operable for the IRT algorithm."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "jp-MarkdownHeadingCollapsed": true,
        "id": "4vq7HO2FQKvJ"
      },
      "source": [
        "# 2. [Library Initialization](#toc2_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "shellscript"
        },
        "id": "B8tRtU-bQKvK"
      },
      "outputs": [],
      "source": [
        "# pip install ollama langchain_community langchain_core pydantic pandas"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "jp-MarkdownHeadingCollapsed": true,
        "id": "QmtSlM-zQKvL"
      },
      "source": [
        "# 3. [LLMs loading](#toc3_)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W8osaqwrQKvL"
      },
      "source": [
        "We use `ollama` and `langchain` frameworks to interract with LLM.\n",
        "\n",
        "Use the following command to initiate `ollama` :\n",
        "\n",
        "\n",
        "<p style=\"color:red;\">ollama serve</p>\n",
        "\n",
        "\n",
        "Download a model to use :\n",
        "\n",
        "\n",
        "<p style=\"color:red;\">ollama run mistral</p>\n",
        "\n",
        "\n",
        "Use model with langchain API."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rSiHT2LbQKvM"
      },
      "source": [
        "# 4. [Item List Loading](#toc4_)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wmW9_gVFQKvM"
      },
      "source": [
        "The file `liste item avec reformulations.xlsx` contains all the items from the different questionaires. Just modify the `sheet_name` to change questionaire."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OXdjGDtxQKvM",
        "outputId": "08b0cfcd-9cd6-4451-dbd7-c3aad041d91a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'/mnt/c/Users/Shiqi/Desktop/APHM/env001/bin/python'"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import sys\n",
        "sys.executable"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TDx5lStlQKvN",
        "outputId": "93a1df85-bb42-4aa1-aa3a-9d347e0cdc32"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CPU times: user 839 ms, sys: 322 ms, total: 1.16 s\n",
            "Wall time: 8.83 s\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Numéro patient</th>\n",
              "      <th>item</th>\n",
              "      <th>modalités de réponse</th>\n",
              "      <th>libellé</th>\n",
              "      <th>réponse</th>\n",
              "      <th>échelle de Likert</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>ACC1</td>\n",
              "      <td>0,1,2</td>\n",
              "      <td>Avez-vous trouvé que vous avez pu facilement c...</td>\n",
              "      <td>Oui, j’ai toujours pu joindre un professionnel...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>ACC1</td>\n",
              "      <td>0,1,2</td>\n",
              "      <td>Avez-vous trouvé que vous avez pu facilement c...</td>\n",
              "      <td>Non, j'ai souvent eu du mal à joindre un profe...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>ACC1</td>\n",
              "      <td>0,1,2</td>\n",
              "      <td>Avez-vous trouvé que vous avez pu facilement c...</td>\n",
              "      <td>Parfois, c'était facile, mais il y avait aussi...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>ACC1</td>\n",
              "      <td>0,1,2</td>\n",
              "      <td>Avez-vous trouvé que vous avez pu facilement c...</td>\n",
              "      <td>Oui, je n'ai jamais eu de problème pour contac...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>ACC1</td>\n",
              "      <td>0,1,2</td>\n",
              "      <td>Avez-vous trouvé que vous avez pu facilement c...</td>\n",
              "      <td>Non, j’ai souvent dû attendre longtemps avant ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5995</th>\n",
              "      <td>46</td>\n",
              "      <td>RD16</td>\n",
              "      <td>0,1,2</td>\n",
              "      <td>Avez-vous trouvé que vos opinions ont été pris...</td>\n",
              "      <td>Oui, à chaque consultation, mes opinions étaie...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5996</th>\n",
              "      <td>47</td>\n",
              "      <td>RD16</td>\n",
              "      <td>0,1,2</td>\n",
              "      <td>Avez-vous trouvé que vos opinions ont été pris...</td>\n",
              "      <td>Non, je me suis souvent senti(e) exclu(e) des ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5997</th>\n",
              "      <td>48</td>\n",
              "      <td>RD16</td>\n",
              "      <td>0,1,2</td>\n",
              "      <td>Avez-vous trouvé que vos opinions ont été pris...</td>\n",
              "      <td>Globalement, je pense que mes opinions étaient...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5998</th>\n",
              "      <td>49</td>\n",
              "      <td>RD16</td>\n",
              "      <td>0,1,2</td>\n",
              "      <td>Avez-vous trouvé que vos opinions ont été pris...</td>\n",
              "      <td>Oui, mes opinions étaient toujours écoutées et...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5999</th>\n",
              "      <td>50</td>\n",
              "      <td>RD16</td>\n",
              "      <td>0,1,2</td>\n",
              "      <td>Avez-vous trouvé que vos opinions ont été pris...</td>\n",
              "      <td>Non, je ne pense pas que mes opinions aient eu...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>6000 rows × 6 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "      Numéro patient  item modalités de réponse  \\\n",
              "0                  1  ACC1                0,1,2   \n",
              "1                  2  ACC1                0,1,2   \n",
              "2                  3  ACC1                0,1,2   \n",
              "3                  4  ACC1                0,1,2   \n",
              "4                  5  ACC1                0,1,2   \n",
              "...              ...   ...                  ...   \n",
              "5995              46  RD16                0,1,2   \n",
              "5996              47  RD16                0,1,2   \n",
              "5997              48  RD16                0,1,2   \n",
              "5998              49  RD16                0,1,2   \n",
              "5999              50  RD16                0,1,2   \n",
              "\n",
              "                                                libellé  \\\n",
              "0     Avez-vous trouvé que vous avez pu facilement c...   \n",
              "1     Avez-vous trouvé que vous avez pu facilement c...   \n",
              "2     Avez-vous trouvé que vous avez pu facilement c...   \n",
              "3     Avez-vous trouvé que vous avez pu facilement c...   \n",
              "4     Avez-vous trouvé que vous avez pu facilement c...   \n",
              "...                                                 ...   \n",
              "5995  Avez-vous trouvé que vos opinions ont été pris...   \n",
              "5996  Avez-vous trouvé que vos opinions ont été pris...   \n",
              "5997  Avez-vous trouvé que vos opinions ont été pris...   \n",
              "5998  Avez-vous trouvé que vos opinions ont été pris...   \n",
              "5999  Avez-vous trouvé que vos opinions ont été pris...   \n",
              "\n",
              "                                                réponse  échelle de Likert  \n",
              "0     Oui, j’ai toujours pu joindre un professionnel...                  2  \n",
              "1     Non, j'ai souvent eu du mal à joindre un profe...                  0  \n",
              "2     Parfois, c'était facile, mais il y avait aussi...                  1  \n",
              "3     Oui, je n'ai jamais eu de problème pour contac...                  2  \n",
              "4     Non, j’ai souvent dû attendre longtemps avant ...                  0  \n",
              "...                                                 ...                ...  \n",
              "5995  Oui, à chaque consultation, mes opinions étaie...                  2  \n",
              "5996  Non, je me suis souvent senti(e) exclu(e) des ...                  0  \n",
              "5997  Globalement, je pense que mes opinions étaient...                  1  \n",
              "5998  Oui, mes opinions étaient toujours écoutées et...                  2  \n",
              "5999  Non, je ne pense pas que mes opinions aient eu...                  0  \n",
              "\n",
              "[6000 rows x 6 columns]"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "%%time\n",
        "import pandas as pd\n",
        "import io\n",
        "from tkinter import Tk, filedialog\n",
        "root = Tk()\n",
        "root.withdraw()\n",
        "file_path = filedialog.askopenfilename(title=\"Select an Excel file\", filetypes=[(\"Excel files\", \"*.xlsx *.xls\")])\n",
        "item_bank = pd.read_excel(file_path, header=0).dropna()\n",
        "item_bank"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z3vHh1OKQKvN",
        "outputId": "3df8ddca-06bc-4b4d-ee9f-d31c94765a95"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 1400x600 with 2 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "value_counts = item_bank['modalités de réponse'].value_counts()\n",
        "distribution = pd.crosstab(item_bank['modalités de réponse'], item_bank['échelle de Likert'])\n",
        "\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))\n",
        "ax1.pie(value_counts, labels=value_counts.index, autopct='%1.1f%%', startangle=90)\n",
        "ax1.set_title('Distribution of Modalités de Réponse')\n",
        "ax1.legend(value_counts.index, title=\"Modalités de Réponse\", loc=\"center left\", bbox_to_anchor=(1, 0, 0.5, 1))\n",
        "ax2 = distribution.plot(kind='bar', stacked=True, ax=ax2)\n",
        "ax2.set_title('Distribution of Échelle de Likert by Modalités de Réponse')\n",
        "ax2.set_xlabel('Modalités de Réponse')\n",
        "ax2.set_ylabel('Count')\n",
        "ax2.legend(title='Échelle de Likert')\n",
        "\n",
        "for container in ax2.containers:\n",
        "    ax2.bar_label(container, label_type='center')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "jp-MarkdownHeadingCollapsed": true,
        "id": "hnFQTRhjQKvO"
      },
      "source": [
        "# 5. [Reformulation of the question](#toc5_)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ewYRh3sFQKvO"
      },
      "source": [
        "If the patient didn't great understand the question, we need to rephrase it.\n",
        "\n",
        "We choose a LLM Few-Shot Learning approach for this task. For each item, we used a big model language (ChatGPT-4o) for generating a dozen of possible reformulation.\n",
        "After a phase of prompt engenering, we have succeeded to the following code.\n",
        "\n",
        "Just modify the `num` variable to select an item in the choosen questionaire."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bi6fxaVBQKvO",
        "outputId": "f10f84a7-c7be-4153-beb3-dba07e89f3be"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Original question : Avez-vous trouvé que vous avez pu facilement contacter un professionnel de santé lorsque vous en avez ressenti(e) le besoin ?\n",
            "Groupe de réponse attendu : 0,1,2\n",
            "Reformulation de la question : Pouviez-vous dire si il était simple pour vous d'avoir accès à une aide médicale quand cela fallait vraiment l'aider?\n",
            "CPU times: user 210 ms, sys: 15.3 ms, total: 226 ms\n",
            "Wall time: 1.65 s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "from pydantic import BaseModel, ValidationError, conint\n",
        "from langchain_ollama import OllamaLLM\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "# Item number - You can modify it\n",
        "num = 20\n",
        "\n",
        "item = item_bank[\"libellé\"].iloc[num]\n",
        "# reformulation = item_bank[\"reformulations\"].iloc[num]\n",
        "reformulation = item_bank[\"réponse\"].iloc[num]\n",
        "item_response_group = item_bank[\"modalités de réponse\"].iloc[num]\n",
        "prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"Tu es un médecin en psychiatrie qui fait passer un questionnaire de satisfaction à un patient qui vient d'être hospitalisé dans ton service.\"),\n",
        "    (\"system\", \"Tes réponses seront exclusivement en Français en style décontracté.\"),\n",
        "    (\"system\", \"Reformule cette question pour qu'elle soit plus compréhensible pour le patient. Assure-toi d'utiliser des mots simples et un langage clair.\"),\n",
        "    (\"system\", f\"Voila des exemples de phrases dont tu peux t'inspirer pour la reformulation : {reformulation}.\"),\n",
        "    (\"system\", \"Utilise le vouvoiement dans tes réponses. Vouvoie le patient.\"),\n",
        "    (\"system\", \"Répond avec une phrase en Français et rien d'autre, pas d'explication.\"),\n",
        "    (\"user\", \"Voila la question que le patient ne comprend pas bien : {input}\")\n",
        "])\n",
        "\n",
        "output_parser = StrOutputParser()\n",
        "\n",
        "chain = prompt | OllamaLLM(model=\"phi3.5\", temperature=0.0) | output_parser\n",
        "\n",
        "answer = chain.invoke({\"input\", item})\n",
        "answer = answer.strip()\n",
        "print(f\"Original question : {item}\")\n",
        "print(f\"Groupe de réponse attendu : {item_response_group}\")\n",
        "print(f\"Reformulation de la question : {answer}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "jp-MarkdownHeadingCollapsed": true,
        "id": "Pqpc7s-2QKvP"
      },
      "source": [
        "# 6. [Likert Scale](#toc6_)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p_M9vQ-ZQKvP"
      },
      "source": [
        "LLM generating Likert Scale.\n",
        "\n",
        "Please note that the range of Likert score vary by items. This range is given by the `item_response_group` variable."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t1HtiRHEQKvP",
        "outputId": "d154d9d5-b389-4321-8aaf-61b890c691d5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Likert score : 1\n",
            "1 (Cela indique un certain niveau d'accord, bien qu'il puisse y avoir des améliorations).  \n",
            "Pourcentage de confiance : 70%  \n",
            "Explication en Français: Le patient exprime une certaine satisfaction avec le système actuel tout en reconnaissant que les choses pourraient être encore meilleures. Cela suggère un niveau modéré d'accord, ce qui justifie la note '1'. Cependant, étant donné qu'ils ne sont pas totalement satisfaits et voient des possibilités de progrès, leur confiance dans cette réponse est légèrement inférieure à 100%, soit environ 70%.\n",
            "Validated Likert Score: 1\n",
            "CPU times: user 86.5 ms, sys: 0 ns, total: 86.5 ms\n",
            "Wall time: 2.18 s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "if item_response_group == \"0,1,2\":\n",
        "\n",
        "\tprompt = ChatPromptTemplate.from_messages([\n",
        "\t\t(\"system\", f\"Nous posons dans le cadre d'une évaluation de la qualité des soins la question suivante à un patient sortant d'une hospitalisation en service de psychiatrie : {item}.\"),\n",
        "\t\t(\"system\", \"En utilisant la réponse suivante, classe la sur une échelle de Likert de 0 à 2 avec 0=pas d'accord, 1= ni d'accord ni pas d'accord, 2=d'accord:\"),\n",
        "\t\t(\"system\", \"Répond avec seulement un chiffre entre 0 à 2 sans commentaire ni explication.\"),\n",
        "\t\t(\"system\", \"Voici quelques exemples de réponse attendue : 1. 2. 0.\"),\n",
        "\t\t(\"system\", \"Si le patient ne répond pas à la question, la réponse est N\"),\n",
        "\t\t(\"system\", \"Je veux que tu donnes à la suite de ta réponse une explication en Français uniquement.\"),\n",
        "\t\t(\"system\", \"Donne moi en pourcentage la confiance que tu a en ta réponse.\"),\n",
        "\t\t(\"user\", \"{input}\")\n",
        "\t])\n",
        "\n",
        "elif item_response_group == \"0,1,2,3,4\":\n",
        "    prompt = ChatPromptTemplate.from_messages([\n",
        "\t\t(\"system\", f\"Nous posons dans le cadre d'une évaluation de la qualité des soins la question suivante à un patient sortant d'une hospitalisation en service de psychiatrie : {item}.\"),\n",
        "\t\t(\"system\", \"En utilisant la réponse suivante, classe la sur une échelle de Likert de 0 à 4 avec 0=pas du tout d'accord, 1=pas d'accord, 2=ni d'accord ni pas d'accord, 3=d'accord, 4=tout à fait d'accord\"),\n",
        "\t\t(\"system\", \"Répond avec seulement un chiffre entre 0 à 4 sans commentaire ni explication.\"),\n",
        "\t\t(\"system\", \"Voici quelques exemples de réponse attendue : 1. 2. 0. 3. 4.\"),\n",
        "\t\t(\"system\", \"Si le patient ne répond pas à la question, la réponse est N\"),\n",
        "\t\t(\"system\", \"Je veux que tu donnes à la suite de ta réponse une explication en Français uniquement.\"),\n",
        "\t\t(\"system\", \"Donne moi en pourcentage la confiance que tu a en ta réponse.\"),\n",
        "\t\t(\"user\", \"{input}\")\n",
        "\t])\n",
        "\n",
        "output_parser = StrOutputParser()\n",
        "\n",
        "chain = prompt | OllamaLLM(model=\"phi3.5\", temperature=0.0) | output_parser\n",
        "\n",
        "answer = chain.invoke({\"input\":\"ouais ça va c'est pas trop mal\"})\n",
        "likert_score = answer.strip()[0]\n",
        "print(f\"Likert score : {likert_score}\")\n",
        "print(answer)\n",
        "class LikertResponse(BaseModel):\n",
        "    score: conint(ge=0, le=4)\n",
        "\n",
        "# Validation avec Pydantic\n",
        "try:\n",
        "    validated_response = LikertResponse(score=int(likert_score))\n",
        "    print(f\"Validated Likert Score: {validated_response.score}\")\n",
        "except ValidationError as e:\n",
        "    print(f\"Validation Error: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z1PC3UftQKvP"
      },
      "source": [
        "A faire :\n",
        "\n",
        "- Génération données synthétiques sur l'échelle de Likert\n",
        "- Génration données synthétiques sur les items\n",
        "- Utilisation de parser Pydantic pour formatter la sortie des LLM. [Structurer la sortie des LLM](https://simmering.dev/blog/structured_output/)\n",
        "  - [Llama index](https://docs.llamaindex.ai/en/stable/examples/output_parsing/llm_program/)\n",
        "  - [Langchain](https://python.langchain.com/v0.1/docs/modules/model_io/output_parsers/types/pydantic/)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vrXVwbeqQKvP"
      },
      "source": [
        "# 7. [Test](#toc7_)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pY_tGUTBQKvP"
      },
      "source": [
        "## 7.3 [Encoder-Decoder Models](#toc7_3_)\n",
        "<div style=\"background-color: lightSkyBlue; width: 100%; height: 50px; padding: 30px; display: center; justify-content: center; align-items: center;\">\n",
        "  <p style=\"text-align: center; font-size: 30px; font-weight: bold;\">\n",
        "    Encoder-Decoder Models\n",
        "  </p>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z9Bg4cl-QKvP",
        "outputId": "58262f23-f045-4d54-88a2-9b232e98fdf3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading t5-small...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "t5-small has been saved locally at ./Downloaded_Models/local_t5_small_tokenizer and ./Downloaded_Models/local_t5_small_model\n",
            "Downloading t5-base...\n",
            "t5-base has been saved locally at ./Downloaded_Models/local_t5_base_tokenizer and ./Downloaded_Models/local_t5_base_model\n",
            "Downloading facebook/bart-base...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
            "The tokenizer class you load from this checkpoint is 'MBart50Tokenizer'. \n",
            "The class this function is called from is 'MBartTokenizer'.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "facebook/bart-base has been saved locally at ./Downloaded_Models/local_bart_tokenizer and ./Downloaded_Models/local_bart_model\n",
            "Downloading facebook/mbart-large-50...\n",
            "facebook/mbart-large-50 has been saved locally at ./Downloaded_Models/local_mbart_tokenizer and ./Downloaded_Models/local_mbart_model\n",
            "Downloading plguillou/t5-base-fr-sum-cnndm...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
            "The tokenizer class you load from this checkpoint is 'MBart50Tokenizer'. \n",
            "The class this function is called from is 'MBartTokenizer'.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "plguillou/t5-base-fr-sum-cnndm has been saved locally at ./Downloaded_Models/local_french_t5_tokenizer and ./Downloaded_Models/local_french_t5_model\n",
            "Downloading facebook/mbart-large-50-many-to-many-mmt...\n",
            "facebook/mbart-large-50-many-to-many-mmt has been saved locally at ./Downloaded_Models/local_french_bart_tokenizer and ./Downloaded_Models/local_french_bart_model\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from transformers import (\n",
        "    BertTokenizer, BertForSequenceClassification,\n",
        "    FlaubertTokenizer, FlaubertForSequenceClassification,\n",
        "    CamembertTokenizer, CamembertForSequenceClassification,\n",
        "    RobertaTokenizer, RobertaForSequenceClassification,\n",
        "    T5Tokenizer, T5ForConditionalGeneration,\n",
        "    BartTokenizer, BartForConditionalGeneration,\n",
        "    MBartTokenizer, MBartForConditionalGeneration\n",
        ")\n",
        "\n",
        "# Directory to store downloaded models\n",
        "download_dir = \"./Downloaded_Models\"\n",
        "if not os.path.exists(download_dir):\n",
        "    os.makedirs(download_dir)\n",
        "\n",
        "models = [\n",
        "    # T5 small model\n",
        "    (\"t5-small\", T5Tokenizer, T5ForConditionalGeneration, f\"{download_dir}/local_t5_small_tokenizer\", f\"{download_dir}/local_t5_small_model\"),\n",
        "\n",
        "    # T5 base model\n",
        "    (\"t5-base\", T5Tokenizer, T5ForConditionalGeneration, f\"{download_dir}/local_t5_base_tokenizer\", f\"{download_dir}/local_t5_base_model\"),\n",
        "\n",
        "    # BART model\n",
        "    (\"facebook/bart-base\", BartTokenizer, BartForConditionalGeneration, f\"{download_dir}/local_bart_tokenizer\", f\"{download_dir}/local_bart_model\"),\n",
        "\n",
        "    # mBART model\n",
        "    (\"facebook/mbart-large-50\", MBartTokenizer, MBartForConditionalGeneration, f\"{download_dir}/local_mbart_tokenizer\", f\"{download_dir}/local_mbart_model\"),\n",
        "\n",
        "    # French T5 model\n",
        "    (\"plguillou/t5-base-fr-sum-cnndm\", T5Tokenizer, T5ForConditionalGeneration, f\"{download_dir}/local_french_t5_tokenizer\", f\"{download_dir}/local_french_t5_model\"),\n",
        "\n",
        "    # French BART model (multilingual, includes French)\n",
        "    (\"facebook/mbart-large-50-many-to-many-mmt\", MBartTokenizer, MBartForConditionalGeneration, f\"{download_dir}/local_french_bart_tokenizer\", f\"{download_dir}/local_french_bart_model\")\n",
        "]\n",
        "\n",
        "# Iterate through the list of models and download them\n",
        "for model_name, tokenizer_class, model_class, tokenizer_path, model_path in models:\n",
        "    print(f\"Downloading {model_name}...\")\n",
        "\n",
        "    # Ensure tokenizer and model are loaded correctly\n",
        "    try:\n",
        "        tokenizer = tokenizer_class.from_pretrained(model_name)\n",
        "        model = model_class.from_pretrained(model_name)\n",
        "\n",
        "        # Save tokenizer and model locally\n",
        "        tokenizer.save_pretrained(tokenizer_path)\n",
        "        model.save_pretrained(model_path)\n",
        "\n",
        "        print(f\"{model_name} has been saved locally at {tokenizer_path} and {model_path}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error downloading {model_name}: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SN13dLMWQKvQ",
        "outputId": "52882773-d6f0-4a02-f3cc-d08e26a982f8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CUDA Available: True\n",
            "Number of CUDA devices visible: 1\n",
            "GPU ID: 0\n",
            "CUDA Device 0 Name: NVIDIA GeForce RTX 4080 Laptop GPU\n",
            "Total memory: 11.99 GB\n",
            "Reserved memory: 0.82 GB\n",
            "Allocated memory: 0.69 GB\n",
            "Free memory: 0.12 GB\n",
            "Using device: cuda\n",
            "Initial memory reserved: 0.82 GB\n",
            "Initial memory allocated: 0.69 GB\n",
            "After clearing cache memory reserved: 0.82 GB\n",
            "After clearing cache memory allocated: 0.69 GB\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "if torch.cuda.is_available():\n",
        "    print(\"CUDA Available:\", torch.cuda.is_available())\n",
        "    num_devices = torch.cuda.device_count()\n",
        "    print(f\"Number of CUDA devices visible: {num_devices}\")\n",
        "    gpu_id = torch.cuda.current_device()\n",
        "    print(\"GPU ID:\", gpu_id)\n",
        "    for i in range(num_devices):\n",
        "        print(f\"CUDA Device {i} Name: {torch.cuda.get_device_name(i)}\")\n",
        "    total_memory = torch.cuda.get_device_properties(gpu_id).total_memory\n",
        "    reserved_memory = torch.cuda.memory_reserved(gpu_id)\n",
        "    allocated_memory = torch.cuda.memory_allocated(gpu_id)\n",
        "    free_memory = reserved_memory - allocated_memory\n",
        "\n",
        "    print(f\"Total memory: {total_memory / (1024**3):.2f} GB\")\n",
        "    print(f\"Reserved memory: {reserved_memory / (1024**3):.2f} GB\")\n",
        "    print(f\"Allocated memory: {allocated_memory / (1024**3):.2f} GB\")\n",
        "    print(f\"Free memory: {free_memory / (1024**3):.2f} GB\")\n",
        "else:\n",
        "    logger.info(\"No CUDA-enabled device found.\")\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "import torch\n",
        "import gc\n",
        "\n",
        "# Check initial memory usage\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"Initial memory reserved: {torch.cuda.memory_reserved() / (1024**3):.2f} GB\")\n",
        "    print(f\"Initial memory allocated: {torch.cuda.memory_allocated() / (1024**3):.2f} GB\")\n",
        "\n",
        "# # Delete unnecessary variables if there are any\n",
        "# del variable_name  # Replace variable_name with actual variable names, if any\n",
        "\n",
        "# Manually run garbage collection\n",
        "gc.collect()\n",
        "\n",
        "# Clear cached memory\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "# Check memory usage after clearing cache\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"After clearing cache memory reserved: {torch.cuda.memory_reserved() / (1024**3):.2f} GB\")\n",
        "    print(f\"After clearing cache memory allocated: {torch.cuda.memory_allocated() / (1024**3):.2f} GB\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HONJA9xkQKvQ",
        "outputId": "e5346ec6-2edf-4ef6-aea1-925e56cb22cf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'sequence': 'Oui, tout était bien organisé, avec une séparation judicieuse entre les espaces de travail et de détente.', 'labels': ['postive', 'strong postive', 'neutral', 'negative', 'strong negative'], 'scores': [0.5249003767967224, 0.4519050121307373, 0.01705465465784073, 0.0034005900379270315, 0.0027394078206270933]}\n",
            "Predicted label: postive with score: 0.5249003767967224\n"
          ]
        }
      ],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "# Define the zero-shot classification pipeline\n",
        "classifier = pipeline(\n",
        "    task=\"zero-shot-classification\",\n",
        "    device=0,\n",
        "    model=\"facebook/bart-large-mnli\"\n",
        ")\n",
        "\n",
        "# Define the text and the new labels\n",
        "text = \"Oui, tout était bien organisé, avec une séparation judicieuse entre les espaces de travail et de détente.\"\n",
        "labels = [\"strong negative\", \"negative\", \"neutral\", \"postive\", \"strong postive\"]\n",
        "\n",
        "# Classify the text using the new labels\n",
        "result = classifier(text, labels, multi_label=False)\n",
        "\n",
        "# Find the label with the highest score\n",
        "highest_score_index = result['scores'].index(max(result['scores']))\n",
        "best_label = result['labels'][highest_score_index]\n",
        "\n",
        "# Display the result\n",
        "print(result)\n",
        "print(f\"Predicted label: {best_label} with score: {result['scores'][highest_score_index]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qp7UW5LAQKvQ",
        "outputId": "9558471a-feba-4a61-9b61-399068520b59"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision 714eb0f (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\n",
            "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[{'label': 'POSITIVE', 'score': 0.9955005049705505}]\n"
          ]
        }
      ],
      "source": [
        "classifier = pipeline(\"text-classification\",device=0)\n",
        "# classifier = pipeline(\"sentiment-analysis\",device=0)\n",
        "result_1 = classifier(\"je ne veux pas travailler aujourd'hui!\")\n",
        "print(result_1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OTI2QPo5QKvR"
      },
      "source": [
        "### 7.3.1 [BART - Zero-Shot Classification](#toc7_3_1_)\n",
        "<div style=\"background-color: lightskyblue; width: 100%; height: 30px; padding: 10px; display: center; justify-content: center; align-items: center;\">\n",
        "  <p style=\"text-align: center; font-size: 20px; font-weight:bold;\">\n",
        "    BART - Zero-Shot Classification\n",
        "  </p>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "Pdcgw-9lQKvR",
        "outputId": "4852a521-5b23-4db0-e603-3fe756360c3f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing Batch 1 / 30...\n",
            "Batch 1's accuracy is 64.50%\n",
            "Overall accuracy so far is 64.50%\n",
            "Processing Batch 2 / 30...\n",
            "Batch 2's accuracy is 58.50%\n",
            "Overall accuracy so far is 61.50%\n",
            "Processing Batch 3 / 30...\n",
            "Batch 3's accuracy is 63.00%\n",
            "Overall accuracy so far is 62.00%\n",
            "Processing Batch 4 / 30...\n",
            "Batch 4's accuracy is 68.00%\n",
            "Overall accuracy so far is 63.50%\n",
            "Processing Batch 5 / 30...\n",
            "Batch 5's accuracy is 65.50%\n",
            "Overall accuracy so far is 63.90%\n",
            "Processing Batch 6 / 30...\n",
            "Batch 6's accuracy is 53.00%\n",
            "Overall accuracy so far is 62.08%\n",
            "Processing Batch 7 / 30...\n",
            "Batch 7's accuracy is 29.50%\n",
            "Overall accuracy so far is 57.43%\n",
            "Processing Batch 8 / 30...\n",
            "Batch 8's accuracy is 31.00%\n",
            "Overall accuracy so far is 54.12%\n",
            "Processing Batch 9 / 30...\n",
            "Batch 9's accuracy is 54.00%\n",
            "Overall accuracy so far is 54.11%\n",
            "Processing Batch 10 / 30...\n",
            "Batch 10's accuracy is 58.50%\n",
            "Overall accuracy so far is 54.55%\n",
            "Processing Batch 11 / 30...\n",
            "Batch 11's accuracy is 61.00%\n",
            "Overall accuracy so far is 55.14%\n",
            "Processing Batch 12 / 30...\n",
            "Batch 12's accuracy is 65.00%\n",
            "Overall accuracy so far is 55.96%\n",
            "Processing Batch 13 / 30...\n",
            "Batch 13's accuracy is 55.50%\n",
            "Overall accuracy so far is 55.92%\n",
            "Processing Batch 14 / 30...\n",
            "Batch 14's accuracy is 62.00%\n",
            "Overall accuracy so far is 56.36%\n",
            "Processing Batch 15 / 30...\n",
            "Batch 15's accuracy is 60.50%\n",
            "Overall accuracy so far is 56.63%\n",
            "Processing Batch 16 / 30...\n",
            "Batch 16's accuracy is 61.00%\n",
            "Overall accuracy so far is 56.91%\n",
            "Processing Batch 17 / 30...\n",
            "Batch 17's accuracy is 63.00%\n",
            "Overall accuracy so far is 57.26%\n",
            "Processing Batch 18 / 30...\n",
            "Batch 18's accuracy is 65.00%\n",
            "Overall accuracy so far is 57.69%\n",
            "Processing Batch 19 / 30...\n",
            "Batch 19's accuracy is 56.50%\n",
            "Overall accuracy so far is 57.63%\n",
            "Processing Batch 20 / 30...\n",
            "Batch 20's accuracy is 35.00%\n",
            "Overall accuracy so far is 56.50%\n",
            "Processing Batch 21 / 30...\n",
            "Batch 21's accuracy is 43.00%\n",
            "Overall accuracy so far is 55.86%\n",
            "Processing Batch 22 / 30...\n",
            "Batch 22's accuracy is 42.50%\n",
            "Overall accuracy so far is 55.25%\n",
            "Processing Batch 23 / 30...\n",
            "Batch 23's accuracy is 62.00%\n",
            "Overall accuracy so far is 55.54%\n",
            "Processing Batch 24 / 30...\n",
            "Batch 24's accuracy is 64.00%\n",
            "Overall accuracy so far is 55.90%\n",
            "Processing Batch 25 / 30...\n",
            "Batch 25's accuracy is 52.00%\n",
            "Overall accuracy so far is 55.74%\n",
            "Processing Batch 26 / 30...\n",
            "Batch 26's accuracy is 42.00%\n",
            "Overall accuracy so far is 55.21%\n",
            "Processing Batch 27 / 30...\n",
            "Batch 27's accuracy is 62.00%\n",
            "Overall accuracy so far is 55.46%\n",
            "Processing Batch 28 / 30...\n",
            "Batch 28's accuracy is 57.00%\n",
            "Overall accuracy so far is 55.52%\n",
            "Processing Batch 29 / 30...\n",
            "Batch 29's accuracy is 61.50%\n",
            "Overall accuracy so far is 55.72%\n",
            "Processing Batch 30 / 30...\n",
            "Batch 30's accuracy is 66.00%\n",
            "Overall accuracy so far is 56.07%\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Numéro patient</th>\n",
              "      <th>item</th>\n",
              "      <th>modalités de réponse</th>\n",
              "      <th>libellé</th>\n",
              "      <th>réponse</th>\n",
              "      <th>échelle de Likert</th>\n",
              "      <th>predict_label</th>\n",
              "      <th>BART_predict</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>ACC1</td>\n",
              "      <td>0,1,2</td>\n",
              "      <td>Avez-vous trouvé que vous avez pu facilement c...</td>\n",
              "      <td>Oui, j’ai toujours pu joindre un professionnel...</td>\n",
              "      <td>2</td>\n",
              "      <td>negative</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>ACC1</td>\n",
              "      <td>0,1,2</td>\n",
              "      <td>Avez-vous trouvé que vous avez pu facilement c...</td>\n",
              "      <td>Non, j'ai souvent eu du mal à joindre un profe...</td>\n",
              "      <td>0</td>\n",
              "      <td>negative</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>ACC1</td>\n",
              "      <td>0,1,2</td>\n",
              "      <td>Avez-vous trouvé que vous avez pu facilement c...</td>\n",
              "      <td>Parfois, c'était facile, mais il y avait aussi...</td>\n",
              "      <td>1</td>\n",
              "      <td>negative</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>ACC1</td>\n",
              "      <td>0,1,2</td>\n",
              "      <td>Avez-vous trouvé que vous avez pu facilement c...</td>\n",
              "      <td>Oui, je n'ai jamais eu de problème pour contac...</td>\n",
              "      <td>2</td>\n",
              "      <td>positive</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>ACC1</td>\n",
              "      <td>0,1,2</td>\n",
              "      <td>Avez-vous trouvé que vous avez pu facilement c...</td>\n",
              "      <td>Non, j’ai souvent dû attendre longtemps avant ...</td>\n",
              "      <td>0</td>\n",
              "      <td>negative</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5995</th>\n",
              "      <td>46</td>\n",
              "      <td>RD16</td>\n",
              "      <td>0,1,2</td>\n",
              "      <td>Avez-vous trouvé que vos opinions ont été pris...</td>\n",
              "      <td>Oui, à chaque consultation, mes opinions étaie...</td>\n",
              "      <td>2</td>\n",
              "      <td>positive</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5996</th>\n",
              "      <td>47</td>\n",
              "      <td>RD16</td>\n",
              "      <td>0,1,2</td>\n",
              "      <td>Avez-vous trouvé que vos opinions ont été pris...</td>\n",
              "      <td>Non, je me suis souvent senti(e) exclu(e) des ...</td>\n",
              "      <td>0</td>\n",
              "      <td>negative</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5997</th>\n",
              "      <td>48</td>\n",
              "      <td>RD16</td>\n",
              "      <td>0,1,2</td>\n",
              "      <td>Avez-vous trouvé que vos opinions ont été pris...</td>\n",
              "      <td>Globalement, je pense que mes opinions étaient...</td>\n",
              "      <td>1</td>\n",
              "      <td>negative</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5998</th>\n",
              "      <td>49</td>\n",
              "      <td>RD16</td>\n",
              "      <td>0,1,2</td>\n",
              "      <td>Avez-vous trouvé que vos opinions ont été pris...</td>\n",
              "      <td>Oui, mes opinions étaient toujours écoutées et...</td>\n",
              "      <td>2</td>\n",
              "      <td>positive</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5999</th>\n",
              "      <td>50</td>\n",
              "      <td>RD16</td>\n",
              "      <td>0,1,2</td>\n",
              "      <td>Avez-vous trouvé que vos opinions ont été pris...</td>\n",
              "      <td>Non, je ne pense pas que mes opinions aient eu...</td>\n",
              "      <td>0</td>\n",
              "      <td>negative</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>6000 rows × 8 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "      Numéro patient  item modalités de réponse  \\\n",
              "0                  1  ACC1                0,1,2   \n",
              "1                  2  ACC1                0,1,2   \n",
              "2                  3  ACC1                0,1,2   \n",
              "3                  4  ACC1                0,1,2   \n",
              "4                  5  ACC1                0,1,2   \n",
              "...              ...   ...                  ...   \n",
              "5995              46  RD16                0,1,2   \n",
              "5996              47  RD16                0,1,2   \n",
              "5997              48  RD16                0,1,2   \n",
              "5998              49  RD16                0,1,2   \n",
              "5999              50  RD16                0,1,2   \n",
              "\n",
              "                                                libellé  \\\n",
              "0     Avez-vous trouvé que vous avez pu facilement c...   \n",
              "1     Avez-vous trouvé que vous avez pu facilement c...   \n",
              "2     Avez-vous trouvé que vous avez pu facilement c...   \n",
              "3     Avez-vous trouvé que vous avez pu facilement c...   \n",
              "4     Avez-vous trouvé que vous avez pu facilement c...   \n",
              "...                                                 ...   \n",
              "5995  Avez-vous trouvé que vos opinions ont été pris...   \n",
              "5996  Avez-vous trouvé que vos opinions ont été pris...   \n",
              "5997  Avez-vous trouvé que vos opinions ont été pris...   \n",
              "5998  Avez-vous trouvé que vos opinions ont été pris...   \n",
              "5999  Avez-vous trouvé que vos opinions ont été pris...   \n",
              "\n",
              "                                                réponse  échelle de Likert  \\\n",
              "0     Oui, j’ai toujours pu joindre un professionnel...                  2   \n",
              "1     Non, j'ai souvent eu du mal à joindre un profe...                  0   \n",
              "2     Parfois, c'était facile, mais il y avait aussi...                  1   \n",
              "3     Oui, je n'ai jamais eu de problème pour contac...                  2   \n",
              "4     Non, j’ai souvent dû attendre longtemps avant ...                  0   \n",
              "...                                                 ...                ...   \n",
              "5995  Oui, à chaque consultation, mes opinions étaie...                  2   \n",
              "5996  Non, je me suis souvent senti(e) exclu(e) des ...                  0   \n",
              "5997  Globalement, je pense que mes opinions étaient...                  1   \n",
              "5998  Oui, mes opinions étaient toujours écoutées et...                  2   \n",
              "5999  Non, je ne pense pas que mes opinions aient eu...                  0   \n",
              "\n",
              "     predict_label  BART_predict  \n",
              "0         negative             0  \n",
              "1         negative             0  \n",
              "2         negative             0  \n",
              "3         positive             2  \n",
              "4         negative             0  \n",
              "...            ...           ...  \n",
              "5995      positive             2  \n",
              "5996      negative             0  \n",
              "5997      negative             0  \n",
              "5998      positive             2  \n",
              "5999      negative             0  \n",
              "\n",
              "[6000 rows x 8 columns]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CPU times: user 5min 37s, sys: 41 s, total: 6min 18s\n",
            "Wall time: 6min 18s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "import os\n",
        "import pandas as pd\n",
        "from transformers import pipeline\n",
        "\n",
        "# Define the zero-shot classification pipeline\n",
        "classifier = pipeline(\n",
        "    task=\"zero-shot-classification\",\n",
        "    device=0,  # Use GPU if available\n",
        "    model=\"facebook/bart-large-mnli\"\n",
        ")\n",
        "\n",
        "# Function to get the appropriate labels based on 'modalités de réponse'\n",
        "def get_labels(modalites_de_reponse):\n",
        "    if modalites_de_reponse == \"0,1,2\":\n",
        "        return [\"negative\", \"neutral\", \"positive\"]\n",
        "    elif modalites_de_reponse == \"0,1,2,3,4\":\n",
        "        return [\"strong negative\", \"negative\", \"neutral\", \"positive\", \"strong positive\"]\n",
        "    else:\n",
        "        return [\"negative\", \"neutral\", \"positive\"]  # Default if no match\n",
        "\n",
        "# Function to map labels to integers for the BART_predict column\n",
        "def map_label_to_int(modalites_de_reponse, label):\n",
        "    if modalites_de_reponse == \"0,1,2\":\n",
        "        label_map = {\"negative\": 0, \"neutral\": 1, \"positive\": 2}\n",
        "    elif modalites_de_reponse == \"0,1,2,3,4\":\n",
        "        label_map = {\"strong negative\": 0, \"negative\": 1, \"neutral\": 2, \"positive\": 3, \"strong positive\": 4}\n",
        "    else:\n",
        "        label_map = {\"negative\": 0, \"neutral\": 1, \"positive\": 2}  # Default map if no match\n",
        "    return label_map.get(label, -1)  # Return -1 if no valid mapping is found\n",
        "\n",
        "# Function to classify each response, select the label with the highest score, and map to integer\n",
        "def classify_response(row):\n",
        "    labels = get_labels(row['modalités de réponse'])  # Get labels based on 'modalités de réponse'\n",
        "    result = classifier(row['réponse'], labels, multi_label=True)  # Use multi_label=True\n",
        "    highest_score_index = result['scores'].index(max(result['scores']))  # Select the highest score\n",
        "    best_label = result['labels'][highest_score_index]\n",
        "\n",
        "    # Map the best label to an integer for BART_predict column\n",
        "    mapped_int = map_label_to_int(row['modalités de réponse'], best_label)\n",
        "    return pd.Series([best_label, mapped_int])\n",
        "\n",
        "# Function to calculate accuracy\n",
        "def compute_accuracy(df):\n",
        "    correct_predictions = (df['BART_predict'] == df['échelle de Likert']).sum()\n",
        "    total_predictions = len(df)\n",
        "    return (correct_predictions / total_predictions) * 100 if total_predictions > 0 else 0\n",
        "\n",
        "# Set batch size\n",
        "batch_size = 200  # Adjust based on memory capacity\n",
        "\n",
        "# Initialize empty lists to hold batch results\n",
        "predict_label_list = []\n",
        "bart_predict_list = []\n",
        "overall_correct_predictions = 0\n",
        "overall_total_predictions = 0\n",
        "\n",
        "# Process the data in batches\n",
        "num_batches = len(item_bank) // batch_size + (1 if len(item_bank) % batch_size != 0 else 0)\n",
        "\n",
        "for batch_num in range(num_batches):\n",
        "    start_idx = batch_num * batch_size\n",
        "    end_idx = min(start_idx + batch_size, len(item_bank))\n",
        "\n",
        "    print(f\"Processing Batch {batch_num + 1} / {num_batches}...\")\n",
        "\n",
        "    # Proess the current batch\n",
        "    batch_data = item_bank.iloc[start_idx:end_idx].copy()  # Copy the batch data to avoid modifying slices\n",
        "    batch_results = batch_data.apply(classify_response, axis=1)\n",
        "\n",
        "    # Append batch results to the lists\n",
        "    predict_label_list.extend(batch_results[0])\n",
        "    bart_predict_list.extend(batch_results[1])\n",
        "\n",
        "    # Compute batch accuracy\n",
        "    batch_data['BART_predict'] = batch_results[1]\n",
        "    batch_accuracy = compute_accuracy(batch_data)\n",
        "    print(f\"Batch {batch_num + 1}'s accuracy is {batch_accuracy:.2f}%\")\n",
        "\n",
        "    # Update overall correct predictions and total predictions\n",
        "    overall_correct_predictions += (batch_data['BART_predict'] == batch_data['échelle de Likert']).sum()\n",
        "    overall_total_predictions += (end_idx - start_idx)\n",
        "\n",
        "    # Print running overall accuracy\n",
        "    overall_accuracy = (overall_correct_predictions / overall_total_predictions) * 100 if overall_total_predictions > 0 else 0\n",
        "    print(f\"Overall accuracy so far is {overall_accuracy:.2f}%\")\n",
        "\n",
        "# After processing all batches, update the original DataFrame\n",
        "item_bank['predict_label'] = predict_label_list\n",
        "item_bank['BART_predict'] = bart_predict_list\n",
        "\n",
        "# Save the results to an Excel file\n",
        "output_file = os.path.join('Outputs', 'Predicted_Results', 'BART_zeroshot_classification.xlsx')\n",
        "item_bank.to_excel(output_file, index=False)\n",
        "\n",
        "from IPython.display import display\n",
        "display(item_bank)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0o9xz59SQKvR"
      },
      "source": [
        "### 7.3.2 [BART - Fine-Tuning & Prompt Engineering](#toc7_3_2_)\n",
        "<div style=\"background-color: lightskyblue; width: 100%; height: 30px; padding: 10px; display: center; justify-content: center; align-items: center;\">\n",
        "  <p style=\"text-align: center; font-size: 20px; font-weight:bold;\">\n",
        "    BART - Fine-Tuning & Prompt Engineering\n",
        "  </p>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "V5CmmgoTQKvR",
        "outputId": "713bf9cc-bcc8-42c1-d687-7bc87be55d94",
        "colab": {
          "referenced_widgets": [
            "f4349f3e655f4345bfd0d696e81f9eef",
            "cc96d1ee82e244d9985681228f946707"
          ]
        }
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-10-15 16:26:17.081632: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2024-10-15 16:26:22.264888: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-10-15 16:26:23.792234: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-10-15 16:26:24.191031: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-10-15 16:26:26.871557: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-10-15 16:26:38.309637: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f4349f3e655f4345bfd0d696e81f9eef",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/60 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "We need to remove 1 to truncate the input but the first sequence has a length 1. \n",
            "We need to remove 1 to truncate the input but the first sequence has a length 1. \n",
            "We need to remove 1 to truncate the input but the first sequence has a length 1. \n",
            "We need to remove 1 to truncate the input but the first sequence has a length 1. \n",
            "We need to remove 1 to truncate the input but the first sequence has a length 1. \n",
            "We need to remove 1 to truncate the input but the first sequence has a length 1. \n",
            "We need to remove 1 to truncate the input but the first sequence has a length 1. \n",
            "We need to remove 1 to truncate the input but the first sequence has a length 1. \n",
            "We need to remove 1 to truncate the input but the first sequence has a length 1. \n",
            "We need to remove 1 to truncate the input but the first sequence has a length 1. \n",
            "We need to remove 1 to truncate the input but the first sequence has a length 1. \n",
            "We need to remove 1 to truncate the input but the first sequence has a length 1. \n",
            "We need to remove 1 to truncate the input but the first sequence has a length 1. \n",
            "We need to remove 1 to truncate the input but the first sequence has a length 1. \n",
            "We need to remove 1 to truncate the input but the first sequence has a length 1. \n",
            "We need to remove 1 to truncate the input but the first sequence has a length 1. \n",
            "We need to remove 1 to truncate the input but the first sequence has a length 1. \n",
            "We need to remove 1 to truncate the input but the first sequence has a length 1. \n",
            "We need to remove 1 to truncate the input but the first sequence has a length 1. \n",
            "We need to remove 1 to truncate the input but the first sequence has a length 1. \n",
            "We need to remove 1 to truncate the input but the first sequence has a length 1. \n",
            "We need to remove 1 to truncate the input but the first sequence has a length 1. \n",
            "We need to remove 1 to truncate the input but the first sequence has a length 1. \n",
            "We need to remove 1 to truncate the input but the first sequence has a length 1. \n",
            "We need to remove 1 to truncate the input but the first sequence has a length 1. \n",
            "We need to remove 1 to truncate the input but the first sequence has a length 1. \n",
            "We need to remove 1 to truncate the input but the first sequence has a length 1. \n",
            "We need to remove 1 to truncate the input but the first sequence has a length 1. \n",
            "We need to remove 1 to truncate the input but the first sequence has a length 1. \n",
            "We need to remove 1 to truncate the input but the first sequence has a length 1. \n",
            "We need to remove 1 to truncate the input but the first sequence has a length 1. \n",
            "We need to remove 1 to truncate the input but the first sequence has a length 1. \n",
            "We need to remove 1 to truncate the input but the first sequence has a length 1. \n",
            "We need to remove 1 to truncate the input but the first sequence has a length 1. \n",
            "We need to remove 1 to truncate the input but the first sequence has a length 1. \n",
            "We need to remove 1 to truncate the input but the first sequence has a length 1. \n",
            "We need to remove 1 to truncate the input but the first sequence has a length 1. \n",
            "We need to remove 1 to truncate the input but the first sequence has a length 1. \n",
            "We need to remove 1 to truncate the input but the first sequence has a length 1. \n",
            "We need to remove 1 to truncate the input but the first sequence has a length 1. \n",
            "We need to remove 1 to truncate the input but the first sequence has a length 1. \n",
            "We need to remove 1 to truncate the input but the first sequence has a length 1. \n",
            "We need to remove 1 to truncate the input but the first sequence has a length 1. \n",
            "We need to remove 1 to truncate the input but the first sequence has a length 1. \n",
            "We need to remove 1 to truncate the input but the first sequence has a length 1. \n",
            "We need to remove 1 to truncate the input but the first sequence has a length 1. \n",
            "We need to remove 1 to truncate the input but the first sequence has a length 1. \n",
            "We need to remove 1 to truncate the input but the first sequence has a length 1. \n",
            "We need to remove 1 to truncate the input but the first sequence has a length 1. \n",
            "We need to remove 1 to truncate the input but the first sequence has a length 1. \n",
            "We need to remove 1 to truncate the input but the first sequence has a length 1. \n",
            "We need to remove 1 to truncate the input but the first sequence has a length 1. \n",
            "We need to remove 1 to truncate the input but the first sequence has a length 1. \n",
            "We need to remove 1 to truncate the input but the first sequence has a length 1. \n",
            "We need to remove 1 to truncate the input but the first sequence has a length 1. \n",
            "We need to remove 1 to truncate the input but the first sequence has a length 1. \n",
            "We need to remove 1 to truncate the input but the first sequence has a length 1. \n",
            "We need to remove 1 to truncate the input but the first sequence has a length 1. \n",
            "We need to remove 1 to truncate the input but the first sequence has a length 1. \n",
            "We need to remove 1 to truncate the input but the first sequence has a length 1. \n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "cc96d1ee82e244d9985681228f946707",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/40 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "We need to remove 1 to truncate the input but the first sequence has a length 1. \n",
            "We need to remove 1 to truncate the input but the first sequence has a length 1. \n",
            "We need to remove 1 to truncate the input but the first sequence has a length 1. \n",
            "We need to remove 1 to truncate the input but the first sequence has a length 1. \n",
            "We need to remove 1 to truncate the input but the first sequence has a length 1. \n",
            "We need to remove 1 to truncate the input but the first sequence has a length 1. \n",
            "We need to remove 1 to truncate the input but the first sequence has a length 1. \n",
            "We need to remove 1 to truncate the input but the first sequence has a length 1. \n",
            "We need to remove 1 to truncate the input but the first sequence has a length 1. \n",
            "We need to remove 1 to truncate the input but the first sequence has a length 1. \n",
            "We need to remove 1 to truncate the input but the first sequence has a length 1. \n",
            "We need to remove 1 to truncate the input but the first sequence has a length 1. \n",
            "We need to remove 1 to truncate the input but the first sequence has a length 1. \n",
            "We need to remove 1 to truncate the input but the first sequence has a length 1. \n",
            "We need to remove 1 to truncate the input but the first sequence has a length 1. \n",
            "We need to remove 1 to truncate the input but the first sequence has a length 1. \n",
            "We need to remove 1 to truncate the input but the first sequence has a length 1. \n",
            "We need to remove 1 to truncate the input but the first sequence has a length 1. \n",
            "We need to remove 1 to truncate the input but the first sequence has a length 1. \n",
            "We need to remove 1 to truncate the input but the first sequence has a length 1. \n",
            "We need to remove 1 to truncate the input but the first sequence has a length 1. \n",
            "We need to remove 1 to truncate the input but the first sequence has a length 1. \n",
            "We need to remove 1 to truncate the input but the first sequence has a length 1. \n",
            "We need to remove 1 to truncate the input but the first sequence has a length 1. \n",
            "We need to remove 1 to truncate the input but the first sequence has a length 1. \n",
            "We need to remove 1 to truncate the input but the first sequence has a length 1. \n",
            "We need to remove 1 to truncate the input but the first sequence has a length 1. \n",
            "We need to remove 1 to truncate the input but the first sequence has a length 1. \n",
            "We need to remove 1 to truncate the input but the first sequence has a length 1. \n",
            "We need to remove 1 to truncate the input but the first sequence has a length 1. \n",
            "We need to remove 1 to truncate the input but the first sequence has a length 1. \n",
            "We need to remove 1 to truncate the input but the first sequence has a length 1. \n",
            "We need to remove 1 to truncate the input but the first sequence has a length 1. \n",
            "We need to remove 1 to truncate the input but the first sequence has a length 1. \n",
            "We need to remove 1 to truncate the input but the first sequence has a length 1. \n",
            "We need to remove 1 to truncate the input but the first sequence has a length 1. \n",
            "We need to remove 1 to truncate the input but the first sequence has a length 1. \n",
            "We need to remove 1 to truncate the input but the first sequence has a length 1. \n",
            "We need to remove 1 to truncate the input but the first sequence has a length 1. \n",
            "We need to remove 1 to truncate the input but the first sequence has a length 1. \n",
            "/mnt/c/Users/Shiqi/Desktop/APHM/env001/lib/python3.12/site-packages/transformers/training_args.py:1545: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='20' max='20' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [20/20 06:05, Epoch 5/5]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>No log</td>\n",
              "      <td>1.940155</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>No log</td>\n",
              "      <td>1.980498</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>No log</td>\n",
              "      <td>1.622405</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>No log</td>\n",
              "      <td>1.078124</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.783950</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/mnt/c/Users/Shiqi/Desktop/APHM/env001/lib/python3.12/site-packages/transformers/modeling_utils.py:2618: UserWarning: Moving the following attributes in the config to the generation config: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0}. You are seeing this warning because you've set generation parameters in the model config, as opposed to in the generation config.\n",
            "  warnings.warn(\n",
            "There were missing keys in the checkpoint model loaded: ['model.encoder.embed_tokens.weight', 'model.decoder.embed_tokens.weight', 'lm_head.weight'].\n",
            "INFO:__main__:Results saved to '/mnt/c/Users/Shiqi/Desktop/APHM/Outputs/Predicted_Results/BART_predicted_sentiments.xlsx'\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CPU times: user 57.6 s, sys: 22 s, total: 1min 19s\n",
            "Wall time: 9min 14s\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>réponse</th>\n",
              "      <th>échelle de Likert</th>\n",
              "      <th>modalités de réponse</th>\n",
              "      <th>label</th>\n",
              "      <th>predict_label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Oui, j’ai toujours pu joindre un professionnel...</td>\n",
              "      <td>2</td>\n",
              "      <td>0,1,2</td>\n",
              "      <td>positive</td>\n",
              "      <td>neutral</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Non, j'ai souvent eu du mal à joindre un profe...</td>\n",
              "      <td>0</td>\n",
              "      <td>0,1,2</td>\n",
              "      <td>negative</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Parfois, c'était facile, mais il y avait aussi...</td>\n",
              "      <td>1</td>\n",
              "      <td>0,1,2</td>\n",
              "      <td>neutral</td>\n",
              "      <td>neutral</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Oui, je n'ai jamais eu de problème pour contac...</td>\n",
              "      <td>2</td>\n",
              "      <td>0,1,2</td>\n",
              "      <td>positive</td>\n",
              "      <td>neutral</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Non, j’ai souvent dû attendre longtemps avant ...</td>\n",
              "      <td>0</td>\n",
              "      <td>0,1,2</td>\n",
              "      <td>negative</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                             réponse  échelle de Likert  \\\n",
              "0  Oui, j’ai toujours pu joindre un professionnel...                  2   \n",
              "1  Non, j'ai souvent eu du mal à joindre un profe...                  0   \n",
              "2  Parfois, c'était facile, mais il y avait aussi...                  1   \n",
              "3  Oui, je n'ai jamais eu de problème pour contac...                  2   \n",
              "4  Non, j’ai souvent dû attendre longtemps avant ...                  0   \n",
              "\n",
              "  modalités de réponse     label predict_label  \n",
              "0                0,1,2  positive       neutral  \n",
              "1                0,1,2  negative      negative  \n",
              "2                0,1,2   neutral       neutral  \n",
              "3                0,1,2  positive       neutral  \n",
              "4                0,1,2  negative      negative  "
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "%%time\n",
        "import os\n",
        "import pandas as pd\n",
        "from transformers import BartTokenizer, BartForConditionalGeneration, Seq2SeqTrainer, Seq2SeqTrainingArguments\n",
        "from transformers import EarlyStoppingCallback\n",
        "from datasets import Dataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "import torch\n",
        "import logging\n",
        "\n",
        "# -----------------------------------\n",
        "# 1. Configuration and Setup\n",
        "# -----------------------------------\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "log_dir = os.getcwd()\n",
        "predicted_results_dir = os.path.join(log_dir, \"Outputs/Predicted_Results\")\n",
        "os.makedirs(predicted_results_dir, exist_ok=True)\n",
        "\n",
        "# Logging Setup\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# -----------------------------------\n",
        "# 2. Load BART Model and Tokenizer\n",
        "# -----------------------------------\n",
        "tokenizer = BartTokenizer.from_pretrained(\"facebook/bart-large\")\n",
        "bart_model = BartForConditionalGeneration.from_pretrained(\"facebook/bart-large\").to(device)\n",
        "\n",
        "# -----------------------------------\n",
        "# 3. Mapping Likert Values to Labels\n",
        "# -----------------------------------\n",
        "def map_likert_to_labels(row):\n",
        "    modalites_de_reponse = row['modalités de réponse']\n",
        "    likert_value = row['échelle de Likert']\n",
        "\n",
        "    if modalites_de_reponse == \"0,1,2\":\n",
        "        return [\"negative\", \"neutral\", \"positive\"][likert_value]\n",
        "    elif modalites_de_reponse == \"0,1,2,3,4\":\n",
        "        return [\"strong negative\", \"negative\", \"neutral\", \"positive\", \"strong positive\"][likert_value]\n",
        "\n",
        "# Load the dataset (item_bank)\n",
        "df = item_bank.head(100)[['libellé', 'réponse', 'échelle de Likert', 'modalités de réponse']]\n",
        "\n",
        "# Apply the Likert value-to-label mapping\n",
        "df['label'] = df.apply(map_likert_to_labels, axis=1)\n",
        "\n",
        "# Split the data into training and test sets\n",
        "train_df, test_df = train_test_split(df, test_size=0.4, shuffle=True, random_state=42)\n",
        "\n",
        "# Convert the data into Hugging Face Dataset format\n",
        "train_dataset = Dataset.from_pandas(train_df)\n",
        "test_dataset = Dataset.from_pandas(test_df)\n",
        "\n",
        "# -----------------------------------\n",
        "# 4. Fine-tuning BART Model with Explicit Constraints\n",
        "# -----------------------------------\n",
        "def tokenize_function(examples):\n",
        "    inputs = [\"Question: \" + q + \" Response: \" + r for q, r in zip(examples['libellé'], examples['réponse'])]\n",
        "    outputs = [str(label) for label in examples['label']]\n",
        "    model_inputs = tokenizer(inputs, max_length=128, truncation=True, padding=\"max_length\")\n",
        "    labels = tokenizer(outputs, max_length=2, truncation=True, padding=\"max_length\").input_ids\n",
        "    model_inputs['labels'] = labels\n",
        "    return model_inputs\n",
        "\n",
        "train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
        "test_dataset = test_dataset.map(tokenize_function, batched=True)\n",
        "\n",
        "# Set format for Trainer to understand the labels\n",
        "train_dataset.set_format(type=\"torch\", columns=['input_ids', 'attention_mask', 'labels'])\n",
        "test_dataset.set_format(type=\"torch\", columns=['input_ids', 'attention_mask', 'labels'])\n",
        "\n",
        "# Fine-tuning BART with Early Stopping and Learning Rate Scheduler\n",
        "training_args = Seq2SeqTrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=16,\n",
        "    num_train_epochs=5,\n",
        "    weight_decay=0.01,\n",
        "    learning_rate=5e-5,\n",
        "    lr_scheduler_type=\"linear\",\n",
        "    load_best_model_at_end=True,\n",
        "    save_total_limit=2,\n",
        "    save_strategy=\"epoch\",\n",
        "    metric_for_best_model=\"eval_loss\"\n",
        ")\n",
        "\n",
        "trainer = Seq2SeqTrainer(\n",
        "    model=bart_model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=test_dataset,\n",
        "    tokenizer=tokenizer,\n",
        "    callbacks=[EarlyStoppingCallback(early_stopping_patience=2)]\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "trainer.train()\n",
        "\n",
        "# Save the fine-tuned model\n",
        "bart_model.save_pretrained(\"./fine_tuned_bart_likert\")\n",
        "tokenizer.save_pretrained(\"./fine_tuned_bart_likert\")\n",
        "\n",
        "# -----------------------------------\n",
        "# 5. Predicting Sentiment with Constrained Labels Using BART\n",
        "# -----------------------------------\n",
        "from difflib import get_close_matches\n",
        "def generate_sentiment(input_text, modalites_de_reponse):\n",
        "    if modalites_de_reponse == \"0,1,2\":\n",
        "        possible_labels = [\"negative\", \"neutral\", \"positive\"]\n",
        "    elif modalites_de_reponse == \"0,1,2,3,4\":\n",
        "        possible_labels = [\"strong negative\", \"negative\", \"neutral\", \"positive\", \"strong positive\"]\n",
        "\n",
        "    # Create the prompt\n",
        "    prompt = f\"Given the response: '{input_text}', predict the sentiment from {possible_labels}.\"\n",
        "\n",
        "    # Tokenize the input\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
        "\n",
        "    # Generate the output (sentiment label)\n",
        "    outputs = bart_model.generate(**inputs, max_length=10, num_beams=5, early_stopping=True)\n",
        "\n",
        "    # Decode the output and extract the generated sentiment\n",
        "    generated = tokenizer.decode(outputs[0], skip_special_tokens=True).strip().lower()\n",
        "\n",
        "    # Try to match the generated sentiment with one of the possible labels (using close matching)\n",
        "    matched_label = get_close_matches(generated, possible_labels, n=1, cutoff=0.6)\n",
        "\n",
        "    # Return the best match or None if no close match is found\n",
        "    return matched_label[0] if matched_label else None\n",
        "\n",
        "# Apply BART sentiment prediction to each row and create a new column 'predict_label'\n",
        "df['predict_label'] = df.apply(lambda row: generate_sentiment(row['réponse'], row['modalités de réponse']), axis=1)\n",
        "\n",
        "# -----------------------------------\n",
        "# 6. Save the Results\n",
        "# -----------------------------------\n",
        "output_file = os.path.join(predicted_results_dir, \"BART_predicted_sentiments.xlsx\")\n",
        "df.to_excel(output_file, index=False)\n",
        "logger.info(f\"Results saved to '{output_file}'\")\n",
        "\n",
        "# Display a sample of the results\n",
        "df[['réponse', 'échelle de Likert', 'modalités de réponse', 'label', 'predict_label']].head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QXECLhUQQKvS",
        "outputId": "34976964-8428-420b-a851-0271ce697100",
        "colab": {
          "referenced_widgets": [
            "5d2076529e4d4db3ab0238feb24b7d67",
            "fb64eb9e926a40b9a4d3f30953e652b1"
          ]
        }
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-10-15 16:50:40.213420: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2024-10-15 16:50:45.600133: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-10-15 16:50:47.481898: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-10-15 16:50:47.957414: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-10-15 16:50:57.392021: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-10-15 16:51:25.706790: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5d2076529e4d4db3ab0238feb24b7d67",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/60 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "We need to remove 1 to truncate the input but the first sequence has a length 1. \n",
            "We need to remove 1 to truncate the input but the first sequence has a length 1. \n",
            "We need to remove 1 to truncate the input but the first sequence has a length 1. \n",
            "We need to remove 1 to truncate the input but the first sequence has a length 1. \n",
            "We need to remove 1 to truncate the input but the first sequence has a length 1. \n",
            "We need to remove 1 to truncate the input but the first sequence has a length 1. \n",
            "We need to remove 1 to truncate the input but the first sequence has a length 1. \n",
            "We need to remove 1 to truncate the input but the first sequence has a length 1. \n",
            "We need to remove 1 to truncate the input but the first sequence has a length 1. \n",
            "We need to remove 1 to truncate the input but the first sequence has a length 1. \n",
            "We need to remove 1 to truncate the input but the first sequence has a length 1. \n",
            "We need to remove 1 to truncate the input but the first sequence has a length 1. \n",
            "We need to remove 1 to truncate the input but the first sequence has a length 1. \n",
            "We need to remove 1 to truncate the input but the first sequence has a length 1. \n",
            "We need to remove 1 to truncate the input but the first sequence has a length 1. \n",
            "We need to remove 1 to truncate the input but the first sequence has a length 1. \n",
            "We need to remove 1 to truncate the input but the first sequence has a length 1. \n",
            "We need to remove 1 to truncate the input but the first sequence has a length 1. \n",
            "We need to remove 1 to truncate the input but the first sequence has a length 1. \n",
            "We need to remove 1 to truncate the input but the first sequence has a length 1. \n",
            "We need to remove 1 to truncate the input but the first sequence has a length 1. \n",
            "We need to remove 1 to truncate the input but the first sequence has a length 1. \n",
            "We need to remove 1 to truncate the input but the first sequence has a length 1. \n",
            "We need to remove 1 to truncate the input but the first sequence has a length 1. \n",
            "We need to remove 1 to truncate the input but the first sequence has a length 1. \n",
            "We need to remove 1 to truncate the input but the first sequence has a length 1. \n",
            "We need to remove 1 to truncate the input but the first sequence has a length 1. \n",
            "We need to remove 1 to truncate the input but the first sequence has a length 1. \n",
            "We need to remove 1 to truncate the input but the first sequence has a length 1. \n",
            "We need to remove 1 to truncate the input but the first sequence has a length 1. \n",
            "We need to remove 1 to truncate the input but the first sequence has a length 1. \n",
            "We need to remove 1 to truncate the input but the first sequence has a length 1. \n",
            "We need to remove 1 to truncate the input but the first sequence has a length 1. \n",
            "We need to remove 1 to truncate the input but the first sequence has a length 1. \n",
            "We need to remove 1 to truncate the input but the first sequence has a length 1. \n",
            "We need to remove 1 to truncate the input but the first sequence has a length 1. \n",
            "We need to remove 1 to truncate the input but the first sequence has a length 1. \n",
            "We need to remove 1 to truncate the input but the first sequence has a length 1. \n",
            "We need to remove 1 to truncate the input but the first sequence has a length 1. \n",
            "We need to remove 1 to truncate the input but the first sequence has a length 1. \n",
            "We need to remove 1 to truncate the input but the first sequence has a length 1. \n",
            "We need to remove 1 to truncate the input but the first sequence has a length 1. \n",
            "We need to remove 1 to truncate the input but the first sequence has a length 1. \n",
            "We need to remove 1 to truncate the input but the first sequence has a length 1. \n",
            "We need to remove 1 to truncate the input but the first sequence has a length 1. \n",
            "We need to remove 1 to truncate the input but the first sequence has a length 1. \n",
            "We need to remove 1 to truncate the input but the first sequence has a length 1. \n",
            "We need to remove 1 to truncate the input but the first sequence has a length 1. \n",
            "We need to remove 1 to truncate the input but the first sequence has a length 1. \n",
            "We need to remove 1 to truncate the input but the first sequence has a length 1. \n",
            "We need to remove 1 to truncate the input but the first sequence has a length 1. \n",
            "We need to remove 1 to truncate the input but the first sequence has a length 1. \n",
            "We need to remove 1 to truncate the input but the first sequence has a length 1. \n",
            "We need to remove 1 to truncate the input but the first sequence has a length 1. \n",
            "We need to remove 1 to truncate the input but the first sequence has a length 1. \n",
            "We need to remove 1 to truncate the input but the first sequence has a length 1. \n",
            "We need to remove 1 to truncate the input but the first sequence has a length 1. \n",
            "We need to remove 1 to truncate the input but the first sequence has a length 1. \n",
            "We need to remove 1 to truncate the input but the first sequence has a length 1. \n",
            "We need to remove 1 to truncate the input but the first sequence has a length 1. \n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "fb64eb9e926a40b9a4d3f30953e652b1",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/40 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "We need to remove 1 to truncate the input but the first sequence has a length 1. \n",
            "We need to remove 1 to truncate the input but the first sequence has a length 1. \n",
            "We need to remove 1 to truncate the input but the first sequence has a length 1. \n",
            "We need to remove 1 to truncate the input but the first sequence has a length 1. \n",
            "We need to remove 1 to truncate the input but the first sequence has a length 1. \n",
            "We need to remove 1 to truncate the input but the first sequence has a length 1. \n",
            "We need to remove 1 to truncate the input but the first sequence has a length 1. \n",
            "We need to remove 1 to truncate the input but the first sequence has a length 1. \n",
            "We need to remove 1 to truncate the input but the first sequence has a length 1. \n",
            "We need to remove 1 to truncate the input but the first sequence has a length 1. \n",
            "We need to remove 1 to truncate the input but the first sequence has a length 1. \n",
            "We need to remove 1 to truncate the input but the first sequence has a length 1. \n",
            "We need to remove 1 to truncate the input but the first sequence has a length 1. \n",
            "We need to remove 1 to truncate the input but the first sequence has a length 1. \n",
            "We need to remove 1 to truncate the input but the first sequence has a length 1. \n",
            "We need to remove 1 to truncate the input but the first sequence has a length 1. \n",
            "We need to remove 1 to truncate the input but the first sequence has a length 1. \n",
            "We need to remove 1 to truncate the input but the first sequence has a length 1. \n",
            "We need to remove 1 to truncate the input but the first sequence has a length 1. \n",
            "We need to remove 1 to truncate the input but the first sequence has a length 1. \n",
            "We need to remove 1 to truncate the input but the first sequence has a length 1. \n",
            "We need to remove 1 to truncate the input but the first sequence has a length 1. \n",
            "We need to remove 1 to truncate the input but the first sequence has a length 1. \n",
            "We need to remove 1 to truncate the input but the first sequence has a length 1. \n",
            "We need to remove 1 to truncate the input but the first sequence has a length 1. \n",
            "We need to remove 1 to truncate the input but the first sequence has a length 1. \n",
            "We need to remove 1 to truncate the input but the first sequence has a length 1. \n",
            "We need to remove 1 to truncate the input but the first sequence has a length 1. \n",
            "We need to remove 1 to truncate the input but the first sequence has a length 1. \n",
            "We need to remove 1 to truncate the input but the first sequence has a length 1. \n",
            "We need to remove 1 to truncate the input but the first sequence has a length 1. \n",
            "We need to remove 1 to truncate the input but the first sequence has a length 1. \n",
            "We need to remove 1 to truncate the input but the first sequence has a length 1. \n",
            "We need to remove 1 to truncate the input but the first sequence has a length 1. \n",
            "We need to remove 1 to truncate the input but the first sequence has a length 1. \n",
            "We need to remove 1 to truncate the input but the first sequence has a length 1. \n",
            "We need to remove 1 to truncate the input but the first sequence has a length 1. \n",
            "We need to remove 1 to truncate the input but the first sequence has a length 1. \n",
            "We need to remove 1 to truncate the input but the first sequence has a length 1. \n",
            "We need to remove 1 to truncate the input but the first sequence has a length 1. \n",
            "/mnt/c/Users/Shiqi/Desktop/APHM/env001/lib/python3.12/site-packages/transformers/training_args.py:1545: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='20' max='20' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [20/20 07:04, Epoch 5/5]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>No log</td>\n",
              "      <td>1.940154</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>No log</td>\n",
              "      <td>1.980498</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>No log</td>\n",
              "      <td>1.622355</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>No log</td>\n",
              "      <td>1.077362</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.783432</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/mnt/c/Users/Shiqi/Desktop/APHM/env001/lib/python3.12/site-packages/transformers/modeling_utils.py:2618: UserWarning: Moving the following attributes in the config to the generation config: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0}. You are seeing this warning because you've set generation parameters in the model config, as opposed to in the generation config.\n",
            "  warnings.warn(\n",
            "There were missing keys in the checkpoint model loaded: ['model.encoder.embed_tokens.weight', 'model.decoder.embed_tokens.weight', 'lm_head.weight'].\n",
            "INFO:__main__:Tokenized input length: 128 tokens for prompt: Given the response: 'Oui, j’ai toujours pu joindre un professionnel de santé lorsque j’en avais besoin, que ce soit par téléphone ou par email. Par exemple, une fois, j'ai eu une crise d'angoisse en pleine nuit, et j'ai rapidement été mis(e) en relation avec quelqu'un qui a pu m'aider à me calmer.', predict the sentiment from ['negative', 'neutral', 'positive'].\n",
            "INFO:__main__:Tokenized input length: 128 tokens for prompt: Given the response: 'Non, j'ai souvent eu du mal à joindre un professionnel quand j’en avais besoin. Un jour, j'ai essayé de les appeler pendant plus d'une heure, mais je suis tombé(e) sur un répondeur à chaque fois, ce qui m'a laissé(e) très anxieux(se).', predict the sentiment from ['negative', 'neutral', 'positive'].\n",
            "INFO:__main__:Tokenized input length: 128 tokens for prompt: Given the response: 'Parfois, c'était facile, mais il y avait aussi des moments où il était difficile d’avoir quelqu’un en ligne, notamment en dehors des heures normales de bureau.', predict the sentiment from ['negative', 'neutral', 'positive'].\n",
            "INFO:__main__:Tokenized input length: 128 tokens for prompt: Given the response: 'Oui, je n'ai jamais eu de problème pour contacter un professionnel de santé. Chaque fois que j'avais une question ou un besoin urgent, il y avait toujours quelqu'un disponible pour me répondre, et cela m’a beaucoup rassuré(e).', predict the sentiment from ['negative', 'neutral', 'positive'].\n",
            "INFO:__main__:Tokenized input length: 128 tokens for prompt: Given the response: 'Non, j’ai souvent dû attendre longtemps avant de pouvoir parler à quelqu'un. Par exemple, un jour où j’avais besoin d’ajuster mes médicaments, je n’ai pu parler à un médecin qu’après trois jours d'attente.', predict the sentiment from ['negative', 'neutral', 'positive'].\n",
            "INFO:__main__:Tokenized input length: 128 tokens for prompt: Given the response: 'Ça dépendait des moments. Parfois, c’était facile de les joindre, mais à d'autres moments, surtout lors des jours fériés ou week-ends, c'était presque impossible.', predict the sentiment from ['negative', 'neutral', 'positive'].\n",
            "INFO:__main__:Tokenized input length: 128 tokens for prompt: Given the response: 'Oui, à chaque fois que j'ai eu besoin de contacter un professionnel de santé, je n'ai jamais rencontré de difficulté. J’ai même pu obtenir des consultations téléphoniques rapidement, ce qui a vraiment facilité ma prise en charge.', predict the sentiment from ['negative', 'neutral', 'positive'].\n",
            "INFO:__main__:Tokenized input length: 128 tokens for prompt: Given the response: 'Non, c'était assez compliqué. J’ai souvent dû passer par plusieurs services avant de trouver la bonne personne à contacter, ce qui a créé beaucoup de frustration et d'inquiétude.', predict the sentiment from ['negative', 'neutral', 'positive'].\n",
            "INFO:__main__:Tokenized input length: 128 tokens for prompt: Given the response: 'La plupart du temps, c’était relativement facile, mais il y a eu des moments où l’on m’a renvoyé de service en service, et cela a retardé ma prise en charge.', predict the sentiment from ['negative', 'neutral', 'positive'].\n",
            "INFO:__main__:Tokenized input length: 128 tokens for prompt: Given the response: 'Oui, j’ai toujours pu contacter un professionnel lorsque j’en avais besoin, même en dehors des heures normales. Cela a beaucoup contribué à mon sentiment de sécurité durant ma période de traitement.', predict the sentiment from ['negative', 'neutral', 'positive'].\n",
            "INFO:__main__:Tokenized input length: 128 tokens for prompt: Given the response: 'Non, il y avait souvent des délais importants pour obtenir une réponse, même en cas d'urgence. Un jour, j'ai dû attendre plus de 24 heures pour avoir un retour sur un traitement qui me causait des effets secondaires graves.', predict the sentiment from ['negative', 'neutral', 'positive'].\n",
            "INFO:__main__:Tokenized input length: 128 tokens for prompt: Given the response: 'C’était généralement facile, mais il y a eu des moments où j’aurais aimé que ce soit plus simple, notamment lorsqu'il s'agissait de joindre un spécialiste en urgence.', predict the sentiment from ['negative', 'neutral', 'positive'].\n",
            "INFO:__main__:Tokenized input length: 128 tokens for prompt: Given the response: 'Oui, j'ai toujours pu parler à quelqu'un rapidement. Une fois, j'ai même eu une consultation en urgence alors que je n’avais pas de rendez-vous, ce qui m’a beaucoup aidé(e) à gérer une situation stressante.', predict the sentiment from ['negative', 'neutral', 'positive'].\n",
            "INFO:__main__:Tokenized input length: 128 tokens for prompt: Given the response: 'Non, il y a eu plusieurs fois où j'ai essayé de contacter un professionnel sans succès. Cela m’a vraiment frustré(e) et fait sentir que je n'étais pas bien pris(e) en charge.', predict the sentiment from ['negative', 'neutral', 'positive'].\n",
            "INFO:__main__:Tokenized input length: 128 tokens for prompt: Given the response: 'Parfois c'était simple, mais d'autres fois, j'avais l'impression que le personnel était débordé et que c'était difficile d'obtenir de l'aide rapidement.', predict the sentiment from ['negative', 'neutral', 'positive'].\n",
            "INFO:__main__:Tokenized input length: 128 tokens for prompt: Given the response: 'Oui, j’ai toujours trouvé que c’était facile de joindre un professionnel, même si c'était pour poser des questions simples. Leur disponibilité m’a vraiment rassuré(e) durant tout mon suivi.', predict the sentiment from ['negative', 'neutral', 'positive'].\n",
            "INFO:__main__:Tokenized input length: 128 tokens for prompt: Given the response: 'Non, j'ai souvent eu du mal à contacter quelqu'un, surtout en période de crise. J’ai souvent été redirigé(e) vers un répondeur ou un service qui ne pouvait pas répondre à mes questions urgentes.', predict the sentiment from ['negative', 'neutral', 'positive'].\n",
            "INFO:__main__:Tokenized input length: 128 tokens for prompt: Given the response: 'La plupart du temps, c’était facile, mais il y a eu des moments où j’aurais préféré que ce soit plus rapide, surtout en situation d’urgence.', predict the sentiment from ['negative', 'neutral', 'positive'].\n",
            "INFO:__main__:Tokenized input length: 128 tokens for prompt: Given the response: 'Oui, j’ai toujours pu contacter un professionnel de santé facilement, et ils étaient réactifs. Cela m’a beaucoup aidé(e), notamment lorsque j’avais des questions urgentes sur mon traitement.', predict the sentiment from ['negative', 'neutral', 'positive'].\n",
            "INFO:__main__:Tokenized input length: 128 tokens for prompt: Given the response: 'Non, j'ai souvent ressenti une absence totale de réponse lorsque j'essayais de contacter quelqu'un. Cela m’a laissé(e) désemparé(e), notamment en période de grande anxiété.', predict the sentiment from ['negative', 'neutral', 'positive'].\n",
            "INFO:__main__:Tokenized input length: 128 tokens for prompt: Given the response: 'Parfois, c'était facile de les joindre, mais il y a eu des moments où c’était frustrant, surtout quand j'avais vraiment besoin d’aide et qu’il n’y avait personne de disponible.', predict the sentiment from ['negative', 'neutral', 'positive'].\n",
            "INFO:__main__:Tokenized input length: 128 tokens for prompt: Given the response: 'Oui, c’était toujours facile d’avoir quelqu’un en ligne. Que ce soit pour des questions sur mon traitement ou mes rendez-vous, je n'ai jamais eu de souci à les joindre rapidement.', predict the sentiment from ['negative', 'neutral', 'positive'].\n",
            "INFO:__main__:Tokenized input length: 128 tokens for prompt: Given the response: 'Non, j’ai souvent dû attendre longtemps pour obtenir une réponse. Une fois, j'ai appelé plusieurs fois dans la même journée sans obtenir de réponse avant le lendemain.', predict the sentiment from ['negative', 'neutral', 'positive'].\n",
            "INFO:__main__:Tokenized input length: 128 tokens for prompt: Given the response: 'C'était facile dans l'ensemble, mais il y avait des moments où j'aurais préféré une réponse plus rapide, surtout quand j'étais en crise.', predict the sentiment from ['negative', 'neutral', 'positive'].\n",
            "INFO:__main__:Tokenized input length: 128 tokens for prompt: Given the response: 'Oui, à chaque fois que j'avais un besoin urgent, j'ai pu contacter un professionnel. Ils ont toujours été disponibles pour répondre à mes questions, même en dehors des heures normales.', predict the sentiment from ['negative', 'neutral', 'positive'].\n",
            "INFO:__main__:Tokenized input length: 128 tokens for prompt: Given the response: 'Non, il m'a fallu plusieurs tentatives pour joindre quelqu’un. J'ai souvent eu l'impression que les lignes étaient surchargées ou que le personnel était débordé, ce qui retardait ma prise en charge.', predict the sentiment from ['negative', 'neutral', 'positive'].\n",
            "INFO:__main__:Tokenized input length: 128 tokens for prompt: Given the response: 'Parfois, j’ai pu obtenir une réponse rapidement, mais il y a eu des fois où j’ai dû attendre plus longtemps que ce que j'aurais espéré, surtout en période de grande angoisse.', predict the sentiment from ['negative', 'neutral', 'positive'].\n",
            "INFO:__main__:Tokenized input length: 128 tokens for prompt: Given the response: 'Oui, j’ai toujours trouvé quelqu’un disponible pour m’aider rapidement, que ce soit par téléphone ou par message. Cela m’a beaucoup rassuré(e) dans les moments difficiles.', predict the sentiment from ['negative', 'neutral', 'positive'].\n",
            "INFO:__main__:Tokenized input length: 128 tokens for prompt: Given the response: 'Non, il y a eu plusieurs fois où j'ai appelé sans obtenir de réponse pendant des jours. Cela a créé beaucoup d'anxiété, car j'avais besoin de conseils sur mon traitement.', predict the sentiment from ['negative', 'neutral', 'positive'].\n",
            "INFO:__main__:Tokenized input length: 128 tokens for prompt: Given the response: 'En général, c'était assez simple de les contacter, mais il y a eu des moments où l'équipe soignante semblait débordée et il fallait attendre plus longtemps pour obtenir une réponse.', predict the sentiment from ['negative', 'neutral', 'positive'].\n",
            "INFO:__main__:Tokenized input length: 128 tokens for prompt: Given the response: 'Oui, j'ai pu facilement joindre un professionnel de santé à chaque fois que j’en ai eu besoin. Ils étaient toujours disponibles pour m’accompagner et me conseiller, même en dehors des heures habituelles.', predict the sentiment from ['negative', 'neutral', 'positive'].\n",
            "INFO:__main__:Tokenized input length: 128 tokens for prompt: Given the response: 'Non, j’ai souvent dû faire plusieurs tentatives avant de réussir à parler à quelqu’un. Cela a créé une frustration importante, surtout quand j'avais des urgences médicales.', predict the sentiment from ['negative', 'neutral', 'positive'].\n",
            "INFO:__main__:Tokenized input length: 128 tokens for prompt: Given the response: 'La plupart du temps, j’ai réussi à obtenir des réponses rapidement, mais il y a eu des moments où c'était plus difficile, surtout pendant les week-ends et les jours fériés.', predict the sentiment from ['negative', 'neutral', 'positive'].\n",
            "INFO:__main__:Tokenized input length: 128 tokens for prompt: Given the response: 'Oui, chaque fois que j'avais une question ou une inquiétude, je pouvais facilement joindre quelqu'un pour me répondre, ce qui m’a vraiment aidé(e) à gérer mes angoisses.', predict the sentiment from ['negative', 'neutral', 'positive'].\n",
            "INFO:__main__:Tokenized input length: 128 tokens for prompt: Given the response: 'Non, c'était compliqué de joindre quelqu’un rapidement. J'ai souvent eu l’impression que mes besoins n'étaient pas prioritaires, et cela a créé beaucoup d’anxiété.', predict the sentiment from ['negative', 'neutral', 'positive'].\n",
            "INFO:__main__:Tokenized input length: 128 tokens for prompt: Given the response: 'Parfois c’était facile de les joindre, mais il y a eu des moments où j'aurais aimé que ce soit plus simple, surtout lorsque j'étais en situation de crise et que j’avais besoin de réponses urgentes.', predict the sentiment from ['negative', 'neutral', 'positive'].\n",
            "INFO:__main__:Tokenized input length: 128 tokens for prompt: Given the response: 'Oui, à chaque fois que j’ai eu besoin d’aide, j’ai pu facilement contacter un professionnel. Le fait de pouvoir parler à quelqu’un rapidement m’a énormément rassuré(e).', predict the sentiment from ['negative', 'neutral', 'positive'].\n",
            "INFO:__main__:Tokenized input length: 128 tokens for prompt: Given the response: 'Non, j’ai souvent dû passer par plusieurs intermédiaires avant de trouver quelqu’un qui pouvait répondre à mes questions, ce qui a ralenti ma prise en charge.', predict the sentiment from ['negative', 'neutral', 'positive'].\n",
            "INFO:__main__:Tokenized input length: 128 tokens for prompt: Given the response: 'Globalement, j’ai réussi à obtenir des réponses rapidement, mais il y a eu des moments où l'attente était plus longue que prévu, surtout pendant les périodes de forte affluence.', predict the sentiment from ['negative', 'neutral', 'positive'].\n",
            "INFO:__main__:Tokenized input length: 128 tokens for prompt: Given the response: 'Oui, chaque fois que j'avais une inquiétude, j'ai pu parler rapidement à un professionnel de santé. Cela a vraiment aidé à diminuer mon stress, notamment en période de crise.', predict the sentiment from ['negative', 'neutral', 'positive'].\n",
            "INFO:__main__:Tokenized input length: 128 tokens for prompt: Given the response: 'Non, il m'a fallu plusieurs tentatives avant de réussir à obtenir de l’aide. Cela m’a laissé(e) démuni(e), car j’avais vraiment besoin d’une réponse urgente à propos de mon traitement.', predict the sentiment from ['negative', 'neutral', 'positive'].\n",
            "INFO:__main__:Tokenized input length: 128 tokens for prompt: Given the response: 'C'était parfois facile de les joindre, mais il y a eu des moments où j’ai dû attendre plusieurs jours avant de recevoir une réponse, ce qui a créé de l’inquiétude.', predict the sentiment from ['negative', 'neutral', 'positive'].\n",
            "INFO:__main__:Tokenized input length: 128 tokens for prompt: Given the response: 'Oui, j’ai toujours réussi à parler à un professionnel de santé quand j’en avais besoin, même pour des petites questions. Cela m’a beaucoup aidé(e) à me sentir accompagné(e).', predict the sentiment from ['negative', 'neutral', 'positive'].\n",
            "INFO:__main__:Tokenized input length: 128 tokens for prompt: Given the response: 'Non, je n’ai souvent pas réussi à obtenir de réponse dans un délai raisonnable. Il y avait des jours où j’appelais plusieurs fois sans jamais réussir à parler à quelqu’un.', predict the sentiment from ['negative', 'neutral', 'positive'].\n",
            "INFO:__main__:Tokenized input length: 128 tokens for prompt: Given the response: 'Globalement, ça s’est bien passé, mais il y a eu des moments où j’ai eu du mal à obtenir une réponse rapide, ce qui a parfois prolongé mon anxiété.', predict the sentiment from ['negative', 'neutral', 'positive'].\n",
            "INFO:__main__:Tokenized input length: 128 tokens for prompt: Given the response: 'Oui, chaque fois que j'ai eu une question, j’ai pu rapidement parler à quelqu'un qui pouvait me répondre. Cela m'a beaucoup aidé(e) à gérer mes préoccupations en lien avec mon traitement.', predict the sentiment from ['negative', 'neutral', 'positive'].\n",
            "INFO:__main__:Tokenized input length: 128 tokens for prompt: Given the response: 'Non, c’était très compliqué d'obtenir une réponse rapidement. Il y a eu des moments où j'avais besoin de conseils médicaux urgents et je n’ai pas pu parler à quelqu’un avant plusieurs jours.', predict the sentiment from ['negative', 'neutral', 'positive'].\n",
            "INFO:__main__:Tokenized input length: 128 tokens for prompt: Given the response: 'Parfois c’était simple, mais il y a eu des moments où j’ai dû attendre longtemps avant de pouvoir joindre un professionnel, ce qui a augmenté mon sentiment d’incertitude.', predict the sentiment from ['negative', 'neutral', 'positive'].\n",
            "INFO:__main__:Tokenized input length: 128 tokens for prompt: Given the response: 'Oui, à chaque fois que j'avais une inquiétude ou un besoin, j'ai pu contacter un professionnel sans difficulté. Cela a vraiment contribué à réduire mon stress.', predict the sentiment from ['negative', 'neutral', 'positive'].\n",
            "INFO:__main__:Tokenized input length: 128 tokens for prompt: Given the response: 'Non, il y avait souvent des délais avant de pouvoir parler à quelqu’un. Cela m’a fait me sentir laissé(e) pour compte, surtout dans les moments où j’avais des questions urgentes.', predict the sentiment from ['negative', 'neutral', 'positive'].\n",
            "INFO:__main__:Tokenized input length: 128 tokens for prompt: Given the response: 'Oui, j’ai pu facilement reporter mes rendez-vous chaque fois que j’en ai eu besoin. Par exemple, il y a une fois où j’étais malade et j'ai simplement appelé le secrétariat pour déplacer ma séance, et ils ont immédiatement trouvé une nouvelle date sans aucune difficulté.', predict the sentiment from ['negative', 'neutral', 'positive'].\n",
            "INFO:__main__:Tokenized input length: 128 tokens for prompt: Given the response: 'Non, c’était assez compliqué de faire reporter un rendez-vous. J’ai dû insister plusieurs fois et donner de nombreuses explications avant qu’ils acceptent de changer la date, ce qui a été une source de stress pour moi.', predict the sentiment from ['negative', 'neutral', 'positive'].\n",
            "INFO:__main__:Tokenized input length: 128 tokens for prompt: Given the response: 'Ça dépend des fois, parfois c’était facile de déplacer un rendez-vous, mais il y a eu des moments où j’ai dû attendre plusieurs jours avant de recevoir une nouvelle proposition de date.', predict the sentiment from ['negative', 'neutral', 'positive'].\n",
            "INFO:__main__:Tokenized input length: 128 tokens for prompt: Given the response: 'Oui, chaque fois que j'avais besoin de changer une date, le secrétariat a toujours été réactif et compréhensif. Ils m’ont proposé plusieurs créneaux pour que je puisse choisir ce qui me convenait le mieux.', predict the sentiment from ['negative', 'neutral', 'positive'].\n",
            "INFO:__main__:Tokenized input length: 128 tokens for prompt: Given the response: 'Non, j'ai rencontré plusieurs difficultés à faire changer mes rendez-vous. Une fois, j'ai eu un empêchement et malgré mes appels répétés, il n'y avait aucune disponibilité avant plusieurs semaines, ce qui a retardé ma prise en charge.', predict the sentiment from ['negative', 'neutral', 'positive'].\n",
            "INFO:__main__:Tokenized input length: 128 tokens for prompt: Given the response: 'C’était assez simple la plupart du temps, mais il y a eu des occasions où ils avaient du mal à trouver des disponibilités rapidement, et j’ai dû attendre un moment avant de pouvoir reprogrammer.', predict the sentiment from ['negative', 'neutral', 'positive'].\n",
            "INFO:__main__:Tokenized input length: 128 tokens for prompt: Given the response: 'Oui, j’ai toujours pu facilement reporter mes rendez-vous. Une fois, j'avais un conflit avec une autre consultation, et l'équipe a trouvé une nouvelle date sans aucune difficulté, ce qui a rendu le processus très fluide.', predict the sentiment from ['negative', 'neutral', 'positive'].\n",
            "INFO:__main__:Tokenized input length: 128 tokens for prompt: Given the response: 'Non, c'était compliqué. À chaque fois que j'ai essayé de déplacer un rendez-vous, on me disait que les créneaux étaient pleins pendant plusieurs semaines, ce qui m'a laissé(e) très frustré(e).', predict the sentiment from ['negative', 'neutral', 'positive'].\n",
            "INFO:__main__:Tokenized input length: 128 tokens for prompt: Given the response: 'Ça dépendait de la période. Quand ils étaient moins occupés, c’était facile, mais durant les périodes de forte affluence, il était parfois difficile de trouver un nouveau créneau rapidement.', predict the sentiment from ['negative', 'neutral', 'positive'].\n",
            "INFO:__main__:Tokenized input length: 128 tokens for prompt: Given the response: 'Oui, j’ai toujours trouvé que c’était facile de reporter mes rendez-vous, même à la dernière minute. Cela m'a beaucoup aidé(e) car j’avais parfois des contraintes imprévues qui nécessitaient un ajustement rapide.', predict the sentiment from ['negative', 'neutral', 'positive'].\n",
            "INFO:__main__:Tokenized input length: 128 tokens for prompt: Given the response: 'Non, j'ai rencontré de nombreuses difficultés à faire déplacer mes rendez-vous. Il y avait rarement des disponibilités proches, et j’ai souvent dû insister pour qu’on me propose quelque chose plus tôt.', predict the sentiment from ['negative', 'neutral', 'positive'].\n",
            "INFO:__main__:Tokenized input length: 128 tokens for prompt: Given the response: 'La plupart du temps, j’ai réussi à reporter mes rendez-vous sans trop de complications, mais il y a eu des moments où les délais de disponibilité étaient un peu longs.', predict the sentiment from ['negative', 'neutral', 'positive'].\n",
            "INFO:__main__:Tokenized input length: 128 tokens for prompt: Given the response: 'Oui, chaque fois que j’ai eu besoin de changer une date, ils étaient très réactifs et trouvaient toujours un créneau qui convenait rapidement. Cela m’a permis de gérer mon emploi du temps sans souci.', predict the sentiment from ['negative', 'neutral', 'positive'].\n",
            "INFO:__main__:Tokenized input length: 128 tokens for prompt: Given the response: 'Non, j’ai souvent eu du mal à faire déplacer mes rendez-vous. Une fois, ils m'ont dit qu’il n’y avait pas de place avant un mois, ce qui a retardé mon suivi de manière importante.', predict the sentiment from ['negative', 'neutral', 'positive'].\n",
            "INFO:__main__:Tokenized input length: 128 tokens for prompt: Given the response: 'Parfois c’était facile, mais il y a eu des moments où les disponibilités étaient rares, et j’ai dû attendre plusieurs jours avant d’obtenir une nouvelle date.', predict the sentiment from ['negative', 'neutral', 'positive'].\n",
            "INFO:__main__:Tokenized input length: 128 tokens for prompt: Given the response: 'Oui, chaque fois que j’avais un empêchement, je pouvais rapidement contacter le secrétariat pour déplacer mon rendez-vous. Ils étaient toujours arrangeants, même pour des demandes de dernière minute.', predict the sentiment from ['negative', 'neutral', 'positive'].\n",
            "INFO:__main__:Tokenized input length: 128 tokens for prompt: Given the response: 'Non, ça a souvent été compliqué. Il y avait rarement des créneaux disponibles rapidement, et cela m’a souvent forcé(e) à annuler complètement mes rendez-vous au lieu de les déplacer.', predict the sentiment from ['negative', 'neutral', 'positive'].\n",
            "INFO:__main__:Tokenized input length: 128 tokens for prompt: Given the response: 'Ça s'est bien passé dans l’ensemble, mais il y a eu des moments où il a été plus difficile de trouver un nouveau créneau à cause de l’affluence.', predict the sentiment from ['negative', 'neutral', 'positive'].\n",
            "INFO:__main__:Tokenized input length: 128 tokens for prompt: Given the response: 'Oui, j’ai toujours trouvé que c’était facile de faire reporter un rendez-vous, même à la dernière minute. Cela a vraiment aidé à m’adapter aux changements dans mon emploi du temps.', predict the sentiment from ['negative', 'neutral', 'positive'].\n",
            "INFO:__main__:Tokenized input length: 128 tokens for prompt: Given the response: 'Non, il m’a souvent été difficile de déplacer un rendez-vous, surtout quand j’avais besoin d’un créneau proche. Cela m’a causé beaucoup de stress car je devais jongler avec d’autres engagements importants.', predict the sentiment from ['negative', 'neutral', 'positive'].\n",
            "INFO:__main__:Tokenized input length: 128 tokens for prompt: Given the response: 'Parfois c’était facile, mais à d’autres moments, ils avaient du mal à trouver des créneaux, surtout pendant les périodes où il y avait beaucoup de demandes.', predict the sentiment from ['negative', 'neutral', 'positive'].\n",
            "INFO:__main__:Tokenized input length: 128 tokens for prompt: Given the response: 'Oui, chaque fois que j’avais besoin de changer un rendez-vous, j’ai pu facilement obtenir une nouvelle date sans problème. Le personnel était toujours très compréhensif.', predict the sentiment from ['negative', 'neutral', 'positive'].\n",
            "INFO:__main__:Tokenized input length: 128 tokens for prompt: Given the response: 'Non, ça a été compliqué. À plusieurs reprises, j’ai dû reporter mes rendez-vous, mais les délais pour obtenir une nouvelle date étaient très longs, ce qui m’a beaucoup frustré(e).', predict the sentiment from ['negative', 'neutral', 'positive'].\n",
            "INFO:__main__:Tokenized input length: 128 tokens for prompt: Given the response: 'La plupart du temps, j’ai pu déplacer mes rendez-vous sans trop de difficultés, mais il y a eu des moments où les nouvelles disponibilités étaient assez éloignées.', predict the sentiment from ['negative', 'neutral', 'positive'].\n",
            "INFO:__main__:Tokenized input length: 128 tokens for prompt: Given the response: 'Oui, j’ai toujours pu faire reporter mes rendez-vous sans problème. Que ce soit pour des raisons professionnelles ou personnelles, ils trouvaient toujours un moyen de m’arranger.', predict the sentiment from ['negative', 'neutral', 'positive'].\n",
            "INFO:__main__:Tokenized input length: 128 tokens for prompt: Given the response: 'Non, c'était souvent très difficile de trouver une nouvelle date quand je devais reporter un rendez-vous. Les créneaux disponibles étaient souvent éloignés, ce qui a perturbé mon suivi.', predict the sentiment from ['negative', 'neutral', 'positive'].\n",
            "INFO:__main__:Tokenized input length: 128 tokens for prompt: Given the response: 'Ça dépendait de la période. Parfois c'était facile, mais il y avait des moments où j'avais du mal à trouver un nouveau créneau dans un délai raisonnable.', predict the sentiment from ['negative', 'neutral', 'positive'].\n",
            "INFO:__main__:Tokenized input length: 128 tokens for prompt: Given the response: 'Oui, chaque fois que j’ai eu besoin de reporter un rendez-vous, le secrétariat a été très réactif. Ils ont toujours été disponibles pour m’aider à trouver une solution rapidement.', predict the sentiment from ['negative', 'neutral', 'positive'].\n",
            "INFO:__main__:Tokenized input length: 128 tokens for prompt: Given the response: 'Non, j’ai rencontré plusieurs difficultés pour faire changer mes rendez-vous. Souvent, ils me proposaient des dates beaucoup trop éloignées, ce qui a ralenti mon suivi.', predict the sentiment from ['negative', 'neutral', 'positive'].\n",
            "INFO:__main__:Tokenized input length: 128 tokens for prompt: Given the response: 'La plupart du temps, c’était assez simple de déplacer un rendez-vous, mais il y a eu des moments où les disponibilités étaient rares et j’ai dû attendre plus longtemps que prévu.', predict the sentiment from ['negative', 'neutral', 'positive'].\n",
            "INFO:__main__:Tokenized input length: 128 tokens for prompt: Given the response: 'Oui, j’ai toujours pu faire reporter mes rendez-vous sans difficulté. Ils étaient toujours disponibles pour me proposer une nouvelle date adaptée à mon emploi du temps.', predict the sentiment from ['negative', 'neutral', 'positive'].\n",
            "INFO:__main__:Tokenized input length: 128 tokens for prompt: Given the response: 'Non, j’ai souvent dû attendre des semaines pour obtenir une nouvelle date après avoir reporté un rendez-vous, ce qui a été frustrant car cela retardait mon suivi médical.', predict the sentiment from ['negative', 'neutral', 'positive'].\n",
            "INFO:__main__:Tokenized input length: 128 tokens for prompt: Given the response: 'Parfois c'était simple, mais il y a eu des périodes où il était difficile de trouver un nouveau créneau proche, surtout en période de forte affluence.', predict the sentiment from ['negative', 'neutral', 'positive'].\n",
            "INFO:__main__:Tokenized input length: 128 tokens for prompt: Given the response: 'Oui, chaque fois que j'avais un imprévu, j’ai pu facilement déplacer mes rendez-vous sans problème. Ils étaient très flexibles, ce qui m'a aidé(e) à bien gérer mon suivi.', predict the sentiment from ['negative', 'neutral', 'positive'].\n",
            "INFO:__main__:Tokenized input length: 128 tokens for prompt: Given the response: 'Non, il y avait souvent des délais importants pour obtenir un nouveau rendez-vous. Cela a créé beaucoup d’anxiété car j'avais besoin de consultations régulières.', predict the sentiment from ['negative', 'neutral', 'positive'].\n",
            "INFO:__main__:Tokenized input length: 128 tokens for prompt: Given the response: 'Ça dépendait de la disponibilité. Parfois c'était simple, mais à d'autres moments, j'avais du mal à obtenir un créneau proche pour reporter un rendez-vous.', predict the sentiment from ['negative', 'neutral', 'positive'].\n",
            "INFO:__main__:Tokenized input length: 128 tokens for prompt: Given the response: 'Oui, j’ai toujours trouvé que c’était facile de reporter un rendez-vous. Le personnel était très compréhensif et trouvait toujours des solutions rapidement.', predict the sentiment from ['negative', 'neutral', 'positive'].\n",
            "INFO:__main__:Tokenized input length: 128 tokens for prompt: Given the response: 'Non, à chaque fois que j'essayais de reporter un rendez-vous, on me disait que les créneaux disponibles étaient pleins pour plusieurs semaines, ce qui était très frustrant.', predict the sentiment from ['negative', 'neutral', 'positive'].\n",
            "INFO:__main__:Tokenized input length: 128 tokens for prompt: Given the response: 'Globalement, c’était facile de reporter un rendez-vous, mais il y a eu des moments où j’ai dû patienter plusieurs jours avant d’obtenir une nouvelle date.', predict the sentiment from ['negative', 'neutral', 'positive'].\n",
            "INFO:__main__:Tokenized input length: 128 tokens for prompt: Given the response: 'Oui, à chaque fois que j’avais un imprévu, j’ai pu facilement déplacer mes rendez-vous sans souci. Cela m’a permis de bien gérer mon emploi du temps tout en continuant mon traitement.', predict the sentiment from ['negative', 'neutral', 'positive'].\n",
            "INFO:__main__:Tokenized input length: 128 tokens for prompt: Given the response: 'Non, c'était souvent compliqué de trouver une nouvelle date qui convenait, surtout quand les créneaux étaient complets. Cela a causé beaucoup de retard dans mon suivi.', predict the sentiment from ['negative', 'neutral', 'positive'].\n",
            "INFO:__main__:Tokenized input length: 128 tokens for prompt: Given the response: 'Ça dépendait des périodes. Parfois, c'était facile de reporter un rendez-vous, mais à d'autres moments, il y avait beaucoup de demande et les délais étaient longs.', predict the sentiment from ['negative', 'neutral', 'positive'].\n",
            "INFO:__main__:Tokenized input length: 128 tokens for prompt: Given the response: 'Oui, j'ai toujours pu reporter mes rendez-vous sans difficulté. Cela m’a beaucoup aidé(e), surtout lorsque j’avais des contraintes professionnelles ou personnelles imprévues.', predict the sentiment from ['negative', 'neutral', 'positive'].\n",
            "INFO:__main__:Tokenized input length: 128 tokens for prompt: Given the response: 'Non, il m’a souvent été difficile de faire déplacer mes rendez-vous, surtout quand j’avais besoin de changer de date à la dernière minute. Les créneaux étaient souvent pleins pendant plusieurs semaines.', predict the sentiment from ['negative', 'neutral', 'positive'].\n",
            "INFO:__main__:Tokenized input length: 128 tokens for prompt: Given the response: 'Parfois c’était facile, mais il y a eu des moments où il était difficile de trouver des disponibilités proches, surtout pendant les périodes de forte affluence.', predict the sentiment from ['negative', 'neutral', 'positive'].\n",
            "INFO:__main__:Tokenized input length: 128 tokens for prompt: Given the response: 'Oui, chaque fois que j’avais un imprévu, j’ai pu reporter mes rendez-vous sans problème. Cela m’a beaucoup rassuré(e) car j'avais besoin de flexibilité.', predict the sentiment from ['negative', 'neutral', 'positive'].\n",
            "INFO:__main__:Tokenized input length: 128 tokens for prompt: Given the response: 'Non, c’était souvent difficile de reporter un rendez-vous, surtout quand les créneaux disponibles étaient limités. Cela m’a causé beaucoup de stress.', predict the sentiment from ['negative', 'neutral', 'positive'].\n",
            "INFO:__main__:Tokenized input length: 128 tokens for prompt: Given the response: 'Globalement, c’était facile de faire déplacer un rendez-vous, mais il y avait des moments où les disponibilités étaient plus rares, ce qui allongeait le temps d’attente.', predict the sentiment from ['negative', 'neutral', 'positive'].\n",
            "INFO:__main__:Tokenized input length: 128 tokens for prompt: Given the response: 'Oui, j’ai toujours pu faire reporter mes rendez-vous sans souci. Ils étaient toujours disponibles pour trouver une nouvelle date adaptée à mes besoins.', predict the sentiment from ['negative', 'neutral', 'positive'].\n",
            "INFO:__main__:Tokenized input length: 128 tokens for prompt: Given the response: 'Non, il m’a souvent été difficile de faire changer mes rendez-vous, surtout quand il y avait beaucoup de demandes et que les créneaux étaient pleins.', predict the sentiment from ['negative', 'neutral', 'positive'].\n",
            "INFO:__main__:Results saved to '/mnt/c/Users/Shiqi/Desktop/APHM/Outputs/Predicted_Results/BART_predicted_sentiments.xlsx'\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CPU times: user 58.3 s, sys: 20.5 s, total: 1min 18s\n",
            "Wall time: 12min 29s\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>réponse</th>\n",
              "      <th>échelle de Likert</th>\n",
              "      <th>modalités de réponse</th>\n",
              "      <th>label</th>\n",
              "      <th>predict_label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Oui, j’ai toujours pu joindre un professionnel...</td>\n",
              "      <td>2</td>\n",
              "      <td>0,1,2</td>\n",
              "      <td>positive</td>\n",
              "      <td>neutral</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Non, j'ai souvent eu du mal à joindre un profe...</td>\n",
              "      <td>0</td>\n",
              "      <td>0,1,2</td>\n",
              "      <td>negative</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Parfois, c'était facile, mais il y avait aussi...</td>\n",
              "      <td>1</td>\n",
              "      <td>0,1,2</td>\n",
              "      <td>neutral</td>\n",
              "      <td>neutral</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Oui, je n'ai jamais eu de problème pour contac...</td>\n",
              "      <td>2</td>\n",
              "      <td>0,1,2</td>\n",
              "      <td>positive</td>\n",
              "      <td>neutral</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Non, j’ai souvent dû attendre longtemps avant ...</td>\n",
              "      <td>0</td>\n",
              "      <td>0,1,2</td>\n",
              "      <td>negative</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                             réponse  échelle de Likert  \\\n",
              "0  Oui, j’ai toujours pu joindre un professionnel...                  2   \n",
              "1  Non, j'ai souvent eu du mal à joindre un profe...                  0   \n",
              "2  Parfois, c'était facile, mais il y avait aussi...                  1   \n",
              "3  Oui, je n'ai jamais eu de problème pour contac...                  2   \n",
              "4  Non, j’ai souvent dû attendre longtemps avant ...                  0   \n",
              "\n",
              "  modalités de réponse     label predict_label  \n",
              "0                0,1,2  positive       neutral  \n",
              "1                0,1,2  negative      negative  \n",
              "2                0,1,2   neutral       neutral  \n",
              "3                0,1,2  positive       neutral  \n",
              "4                0,1,2  negative      negative  "
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "%%time\n",
        "import os\n",
        "import pandas as pd\n",
        "from transformers import BartTokenizer, BartForConditionalGeneration, Seq2SeqTrainer, Seq2SeqTrainingArguments\n",
        "from transformers import EarlyStoppingCallback\n",
        "from datasets import Dataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "import torch\n",
        "import logging\n",
        "\n",
        "# -----------------------------------\n",
        "# 1. Configuration and Setup\n",
        "# -----------------------------------\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "log_dir = os.getcwd()\n",
        "predicted_results_dir = os.path.join(log_dir, \"Outputs/Predicted_Results\")\n",
        "os.makedirs(predicted_results_dir, exist_ok=True)\n",
        "\n",
        "# Logging Setup\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# -----------------------------------\n",
        "# 2. Load BART Model and Tokenizer\n",
        "# -----------------------------------\n",
        "tokenizer = BartTokenizer.from_pretrained(\"facebook/bart-large\")\n",
        "bart_model = BartForConditionalGeneration.from_pretrained(\"facebook/bart-large\").to(device)\n",
        "\n",
        "# -----------------------------------\n",
        "# 3. Mapping Likert Values to Labels\n",
        "# -----------------------------------\n",
        "def map_likert_to_labels(row):\n",
        "    modalites_de_reponse = row['modalités de réponse']\n",
        "    likert_value = row['échelle de Likert']\n",
        "\n",
        "    if modalites_de_reponse == \"0,1,2\":\n",
        "        return [\"negative\", \"neutral\", \"positive\"][likert_value]\n",
        "    elif modalites_de_reponse == \"0,1,2,3,4\":\n",
        "        return [\"strong negative\", \"negative\", \"neutral\", \"positive\", \"strong positive\"][likert_value]\n",
        "\n",
        "# Load the dataset (item_bank)\n",
        "df = item_bank.head(100)[['libellé', 'réponse', 'échelle de Likert', 'modalités de réponse']]\n",
        "\n",
        "# Apply the Likert value-to-label mapping\n",
        "df['label'] = df.apply(map_likert_to_labels, axis=1)\n",
        "\n",
        "# Split the data into training and test sets\n",
        "train_df, test_df = train_test_split(df, test_size=0.4, shuffle=True, random_state=42)\n",
        "\n",
        "# Convert the data into Hugging Face Dataset format\n",
        "train_dataset = Dataset.from_pandas(train_df)\n",
        "test_dataset = Dataset.from_pandas(test_df)\n",
        "\n",
        "# -----------------------------------\n",
        "# 4. Fine-tuning BART Model with Explicit Constraints\n",
        "# -----------------------------------\n",
        "def tokenize_function(examples):\n",
        "    inputs = [\"Question: \" + q + \" Response: \" + r for q, r in zip(examples['libellé'], examples['réponse'])]\n",
        "    outputs = [str(label) for label in examples['label']]\n",
        "    model_inputs = tokenizer(inputs, max_length=128, truncation=True, padding=\"max_length\")\n",
        "    labels = tokenizer(outputs, max_length=2, truncation=True, padding=\"max_length\").input_ids\n",
        "    model_inputs['labels'] = labels\n",
        "    return model_inputs\n",
        "\n",
        "train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
        "test_dataset = test_dataset.map(tokenize_function, batched=True)\n",
        "\n",
        "# Set format for Trainer to understand the labels\n",
        "train_dataset.set_format(type=\"torch\", columns=['input_ids', 'attention_mask', 'labels'])\n",
        "test_dataset.set_format(type=\"torch\", columns=['input_ids', 'attention_mask', 'labels'])\n",
        "\n",
        "# Fine-tuning BART with Early Stopping and Learning Rate Scheduler\n",
        "training_args = Seq2SeqTrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=16,\n",
        "    num_train_epochs=5,\n",
        "    weight_decay=0.01,\n",
        "    learning_rate=5e-5,\n",
        "    lr_scheduler_type=\"linear\",\n",
        "    load_best_model_at_end=True,\n",
        "    save_total_limit=2,\n",
        "    save_strategy=\"epoch\",\n",
        "    metric_for_best_model=\"eval_loss\"\n",
        ")\n",
        "\n",
        "trainer = Seq2SeqTrainer(\n",
        "    model=bart_model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=test_dataset,\n",
        "    tokenizer=tokenizer,\n",
        "    callbacks=[EarlyStoppingCallback(early_stopping_patience=2)]\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "trainer.train()\n",
        "\n",
        "# Save the fine-tuned model\n",
        "bart_model.save_pretrained(\"./fine_tuned_bart_likert\")\n",
        "tokenizer.save_pretrained(\"./fine_tuned_bart_likert\")\n",
        "\n",
        "# -----------------------------------\n",
        "# 5. Predicting Sentiment with Constrained Labels Using BART\n",
        "# -----------------------------------\n",
        "from difflib import get_close_matches\n",
        "def generate_sentiment(input_text, modalites_de_reponse):\n",
        "    if modalites_de_reponse == \"0,1,2\":\n",
        "        possible_labels = [\"negative\", \"neutral\", \"positive\"]\n",
        "    elif modalites_de_reponse == \"0,1,2,3,4\":\n",
        "        possible_labels = [\"strong negative\", \"negative\", \"neutral\", \"positive\", \"strong positive\"]\n",
        "\n",
        "    # Create the prompt\n",
        "    prompt = f\"Given the response: '{input_text}', predict the sentiment from {possible_labels}.\"\n",
        "\n",
        "    # Tokenize the input\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\", max_length=128, truncation=True, padding=\"max_length\").to(device)\n",
        "\n",
        "    # Log tokenization info for debugging\n",
        "    input_length = inputs[\"input_ids\"].shape[1]\n",
        "    logger.info(f\"Tokenized input length: {input_length} tokens for prompt: {prompt}\")\n",
        "\n",
        "    if input_length <= 1:\n",
        "        logger.warning(f\"Input too short after tokenization: {input_text}\")\n",
        "        return None\n",
        "\n",
        "    # Generate the output (sentiment label)\n",
        "    outputs = bart_model.generate(**inputs, max_length=50, num_beams=5, early_stopping=True)\n",
        "\n",
        "    # Decode the output and extract the generated sentiment\n",
        "    generated = tokenizer.decode(outputs[0], skip_special_tokens=True).strip().lower()\n",
        "\n",
        "    # Try to match the generated sentiment with one of the possible labels (using close matching)\n",
        "    matched_label = get_close_matches(generated, possible_labels, n=1, cutoff=0.6)\n",
        "\n",
        "    # Return the best match or None if no close match is found\n",
        "    return matched_label[0] if matched_label else None\n",
        "\n",
        "# Apply BART sentiment prediction to each row and create a new column 'predict_label'\n",
        "df['predict_label'] = df.apply(lambda row: generate_sentiment(row['réponse'], row['modalités de réponse']), axis=1)\n",
        "\n",
        "# -----------------------------------\n",
        "# 6. Save the Results\n",
        "# -----------------------------------\n",
        "output_file = os.path.join(predicted_results_dir, \"BART_predicted_sentiments.xlsx\")\n",
        "df.to_excel(output_file, index=False)\n",
        "logger.info(f\"Results saved to '{output_file}'\")\n",
        "\n",
        "# Display a sample of the results\n",
        "df[['réponse', 'échelle de Likert', 'modalités de réponse', 'label', 'predict_label']].head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-U-4nT8pQKvS",
        "outputId": "9cad2d14-fd72-44ab-cf32-d7eb52e09d98",
        "colab": {
          "referenced_widgets": [
            "3578b200f8a648e79ff5273711e0173d",
            "72fef21b279042508422c9d35cd6a106"
          ]
        }
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3578b200f8a648e79ff5273711e0173d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/60 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "We need to remove 1 to truncate the input but the first sequence has a length 1. \n",
            "We need to remove 1 to truncate the input but the first sequence has a length 1. \n",
            "We need to remove 1 to truncate the input but the first sequence has a length 1. \n",
            "We need to remove 1 to truncate the input but the first sequence has a length 1. \n",
            "We need to remove 1 to truncate the input but the first sequence has a length 1. \n",
            "We need to remove 1 to truncate the input but the first sequence has a length 1. \n",
            "We need to remove 1 to truncate the input but the first sequence has a length 1. \n",
            "We need to remove 1 to truncate the input but the first sequence has a length 1. \n",
            "We need to remove 1 to truncate the input but the first sequence has a length 1. \n",
            "We need to remove 1 to truncate the input but the first sequence has a length 1. \n",
            "We need to remove 1 to truncate the input but the first sequence has a length 1. \n",
            "We need to remove 1 to truncate the input but the first sequence has a length 1. \n",
            "We need to remove 1 to truncate the input but the first sequence has a length 1. \n",
            "We need to remove 1 to truncate the input but the first sequence has a length 1. \n",
            "We need to remove 1 to truncate the input but the first sequence has a length 1. \n",
            "We need to remove 1 to truncate the input but the first sequence has a length 1. \n",
            "We need to remove 1 to truncate the input but the first sequence has a length 1. \n",
            "We need to remove 1 to truncate the input but the first sequence has a length 1. \n",
            "We need to remove 1 to truncate the input but the first sequence has a length 1. \n",
            "We need to remove 1 to truncate the input but the first sequence has a length 1. \n",
            "We need to remove 1 to truncate the input but the first sequence has a length 1. \n",
            "We need to remove 1 to truncate the input but the first sequence has a length 1. \n",
            "We need to remove 1 to truncate the input but the first sequence has a length 1. \n",
            "We need to remove 1 to truncate the input but the first sequence has a length 1. \n",
            "We need to remove 1 to truncate the input but the first sequence has a length 1. \n",
            "We need to remove 1 to truncate the input but the first sequence has a length 1. \n",
            "We need to remove 1 to truncate the input but the first sequence has a length 1. \n",
            "We need to remove 1 to truncate the input but the first sequence has a length 1. \n",
            "We need to remove 1 to truncate the input but the first sequence has a length 1. \n",
            "We need to remove 1 to truncate the input but the first sequence has a length 1. \n",
            "We need to remove 1 to truncate the input but the first sequence has a length 1. \n",
            "We need to remove 1 to truncate the input but the first sequence has a length 1. \n",
            "We need to remove 1 to truncate the input but the first sequence has a length 1. \n",
            "We need to remove 1 to truncate the input but the first sequence has a length 1. \n",
            "We need to remove 1 to truncate the input but the first sequence has a length 1. \n",
            "We need to remove 1 to truncate the input but the first sequence has a length 1. \n",
            "We need to remove 1 to truncate the input but the first sequence has a length 1. \n",
            "We need to remove 1 to truncate the input but the first sequence has a length 1. \n",
            "We need to remove 1 to truncate the input but the first sequence has a length 1. \n",
            "We need to remove 1 to truncate the input but the first sequence has a length 1. \n",
            "We need to remove 1 to truncate the input but the first sequence has a length 1. \n",
            "We need to remove 1 to truncate the input but the first sequence has a length 1. \n",
            "We need to remove 1 to truncate the input but the first sequence has a length 1. \n",
            "We need to remove 1 to truncate the input but the first sequence has a length 1. \n",
            "We need to remove 1 to truncate the input but the first sequence has a length 1. \n",
            "We need to remove 1 to truncate the input but the first sequence has a length 1. \n",
            "We need to remove 1 to truncate the input but the first sequence has a length 1. \n",
            "We need to remove 1 to truncate the input but the first sequence has a length 1. \n",
            "We need to remove 1 to truncate the input but the first sequence has a length 1. \n",
            "We need to remove 1 to truncate the input but the first sequence has a length 1. \n",
            "We need to remove 1 to truncate the input but the first sequence has a length 1. \n",
            "We need to remove 1 to truncate the input but the first sequence has a length 1. \n",
            "We need to remove 1 to truncate the input but the first sequence has a length 1. \n",
            "We need to remove 1 to truncate the input but the first sequence has a length 1. \n",
            "We need to remove 1 to truncate the input but the first sequence has a length 1. \n",
            "We need to remove 1 to truncate the input but the first sequence has a length 1. \n",
            "We need to remove 1 to truncate the input but the first sequence has a length 1. \n",
            "We need to remove 1 to truncate the input but the first sequence has a length 1. \n",
            "We need to remove 1 to truncate the input but the first sequence has a length 1. \n",
            "We need to remove 1 to truncate the input but the first sequence has a length 1. \n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "72fef21b279042508422c9d35cd6a106",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/40 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "We need to remove 1 to truncate the input but the first sequence has a length 1. \n",
            "We need to remove 1 to truncate the input but the first sequence has a length 1. \n",
            "We need to remove 1 to truncate the input but the first sequence has a length 1. \n",
            "We need to remove 1 to truncate the input but the first sequence has a length 1. \n",
            "We need to remove 1 to truncate the input but the first sequence has a length 1. \n",
            "We need to remove 1 to truncate the input but the first sequence has a length 1. \n",
            "We need to remove 1 to truncate the input but the first sequence has a length 1. \n",
            "We need to remove 1 to truncate the input but the first sequence has a length 1. \n",
            "We need to remove 1 to truncate the input but the first sequence has a length 1. \n",
            "We need to remove 1 to truncate the input but the first sequence has a length 1. \n",
            "We need to remove 1 to truncate the input but the first sequence has a length 1. \n",
            "We need to remove 1 to truncate the input but the first sequence has a length 1. \n",
            "We need to remove 1 to truncate the input but the first sequence has a length 1. \n",
            "We need to remove 1 to truncate the input but the first sequence has a length 1. \n",
            "We need to remove 1 to truncate the input but the first sequence has a length 1. \n",
            "We need to remove 1 to truncate the input but the first sequence has a length 1. \n",
            "We need to remove 1 to truncate the input but the first sequence has a length 1. \n",
            "We need to remove 1 to truncate the input but the first sequence has a length 1. \n",
            "We need to remove 1 to truncate the input but the first sequence has a length 1. \n",
            "We need to remove 1 to truncate the input but the first sequence has a length 1. \n",
            "We need to remove 1 to truncate the input but the first sequence has a length 1. \n",
            "We need to remove 1 to truncate the input but the first sequence has a length 1. \n",
            "We need to remove 1 to truncate the input but the first sequence has a length 1. \n",
            "We need to remove 1 to truncate the input but the first sequence has a length 1. \n",
            "We need to remove 1 to truncate the input but the first sequence has a length 1. \n",
            "We need to remove 1 to truncate the input but the first sequence has a length 1. \n",
            "We need to remove 1 to truncate the input but the first sequence has a length 1. \n",
            "We need to remove 1 to truncate the input but the first sequence has a length 1. \n",
            "We need to remove 1 to truncate the input but the first sequence has a length 1. \n",
            "We need to remove 1 to truncate the input but the first sequence has a length 1. \n",
            "We need to remove 1 to truncate the input but the first sequence has a length 1. \n",
            "We need to remove 1 to truncate the input but the first sequence has a length 1. \n",
            "We need to remove 1 to truncate the input but the first sequence has a length 1. \n",
            "We need to remove 1 to truncate the input but the first sequence has a length 1. \n",
            "We need to remove 1 to truncate the input but the first sequence has a length 1. \n",
            "We need to remove 1 to truncate the input but the first sequence has a length 1. \n",
            "We need to remove 1 to truncate the input but the first sequence has a length 1. \n",
            "We need to remove 1 to truncate the input but the first sequence has a length 1. \n",
            "We need to remove 1 to truncate the input but the first sequence has a length 1. \n",
            "We need to remove 1 to truncate the input but the first sequence has a length 1. \n",
            "/mnt/c/Users/Shiqi/Desktop/APHM/env001/lib/python3.12/site-packages/transformers/training_args.py:1545: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='13' max='20' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [13/20 01:47 < 01:08, 0.10 it/s, Epoch 3/5]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>No log</td>\n",
              "      <td>1.940154</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>No log</td>\n",
              "      <td>1.980491</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>No log</td>\n",
              "      <td>1.622367</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/mnt/c/Users/Shiqi/Desktop/APHM/env001/lib/python3.12/site-packages/transformers/modeling_utils.py:2618: UserWarning: Moving the following attributes in the config to the generation config: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0}. You are seeing this warning because you've set generation parameters in the model config, as opposed to in the generation config.\n",
            "  warnings.warn(\n",
            "\n",
            "KeyboardInterrupt\n",
            "\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "import os\n",
        "import pandas as pd\n",
        "from transformers import BartTokenizer, BartForConditionalGeneration, Seq2SeqTrainer, Seq2SeqTrainingArguments\n",
        "from transformers import EarlyStoppingCallback\n",
        "from datasets import Dataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "import torch\n",
        "import logging\n",
        "\n",
        "# -----------------------------------\n",
        "# 1. Configuration and Setup\n",
        "# -----------------------------------\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "log_dir = os.getcwd()\n",
        "predicted_results_dir = os.path.join(log_dir, \"Outputs/Predicted_Results\")\n",
        "os.makedirs(predicted_results_dir, exist_ok=True)\n",
        "\n",
        "# Logging Setup\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# -----------------------------------\n",
        "# 2. Load BART Model and Tokenizer\n",
        "# -----------------------------------\n",
        "tokenizer = BartTokenizer.from_pretrained(\"facebook/bart-large\")\n",
        "bart_model = BartForConditionalGeneration.from_pretrained(\"facebook/bart-large\").to(device)\n",
        "\n",
        "# -----------------------------------\n",
        "# 3. Mapping Likert Values to Labels\n",
        "# -----------------------------------\n",
        "def map_likert_to_labels(row):\n",
        "    modalites_de_reponse = row['modalités de réponse']\n",
        "    likert_value = row['échelle de Likert']\n",
        "\n",
        "    if modalites_de_reponse == \"0,1,2\":\n",
        "        return [\"negative\", \"neutral\", \"positive\"][likert_value]\n",
        "    elif modalites_de_reponse == \"0,1,2,3,4\":\n",
        "        return [\"strong negative\", \"negative\", \"neutral\", \"positive\", \"strong positive\"][likert_value]\n",
        "\n",
        "# Load the dataset (item_bank)\n",
        "df = item_bank.head(100)[['libellé', 'réponse', 'échelle de Likert', 'modalités de réponse']]\n",
        "\n",
        "# Apply the Likert value-to-label mapping\n",
        "df['label'] = df.apply(map_likert_to_labels, axis=1)\n",
        "\n",
        "# Split the data into training and test sets\n",
        "train_df, test_df = train_test_split(df, test_size=0.4, shuffle=True, random_state=42)\n",
        "\n",
        "# Convert the data into Hugging Face Dataset format\n",
        "train_dataset = Dataset.from_pandas(train_df)\n",
        "test_dataset = Dataset.from_pandas(test_df)\n",
        "\n",
        "# -----------------------------------\n",
        "# 4. Fine-tuning BART Model with Explicit Constraints\n",
        "# -----------------------------------\n",
        "def tokenize_function(examples):\n",
        "    inputs = [\"Question: \" + q + \" Response: \" + r for q, r in zip(examples['libellé'], examples['réponse'])]\n",
        "    outputs = [str(label) for label in examples['label']]\n",
        "    model_inputs = tokenizer(inputs, max_length=128, truncation=True, padding=\"max_length\")\n",
        "    labels = tokenizer(outputs, max_length=2, truncation=True, padding=\"max_length\").input_ids\n",
        "    model_inputs['labels'] = labels\n",
        "    return model_inputs\n",
        "\n",
        "train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
        "test_dataset = test_dataset.map(tokenize_function, batched=True)\n",
        "\n",
        "# Set format for Trainer to understand the labels\n",
        "train_dataset.set_format(type=\"torch\", columns=['input_ids', 'attention_mask', 'labels'])\n",
        "test_dataset.set_format(type=\"torch\", columns=['input_ids', 'attention_mask', 'labels'])\n",
        "\n",
        "# Fine-tuning BART with Early Stopping and Learning Rate Scheduler\n",
        "training_args = Seq2SeqTrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=16,\n",
        "    num_train_epochs=5,\n",
        "    weight_decay=0.01,\n",
        "    learning_rate=5e-5,\n",
        "    lr_scheduler_type=\"linear\",\n",
        "    load_best_model_at_end=True,\n",
        "    save_total_limit=2,\n",
        "    save_strategy=\"epoch\",\n",
        "    metric_for_best_model=\"eval_loss\"\n",
        ")\n",
        "\n",
        "trainer = Seq2SeqTrainer(\n",
        "    model=bart_model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=test_dataset,\n",
        "    tokenizer=tokenizer,\n",
        "    callbacks=[EarlyStoppingCallback(early_stopping_patience=2)]\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "trainer.train()\n",
        "\n",
        "# Save the fine-tuned model\n",
        "bart_model.save_pretrained(\"./fine_tuned_bart_likert\")\n",
        "tokenizer.save_pretrained(\"./fine_tuned_bart_likert\")\n",
        "\n",
        "# -----------------------------------\n",
        "# 5. Predicting Sentiment with Constrained Labels Using BART\n",
        "# -----------------------------------\n",
        "from difflib import get_close_matches\n",
        "def generate_sentiment(input_text, modalites_de_reponse):\n",
        "    if modalites_de_reponse == \"0,1,2\":\n",
        "        possible_labels = [\"negative\", \"neutral\", \"positive\"]\n",
        "    elif modalites_de_reponse == \"0,1,2,3,4\":\n",
        "        possible_labels = [\"strong negative\", \"negative\", \"neutral\", \"positive\", \"strong positive\"]\n",
        "\n",
        "    # Create the prompt\n",
        "    prompt = f\"Given the response: '{input_text}', predict the sentiment from {possible_labels}.\"\n",
        "\n",
        "    # Tokenize the input\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\", max_length=128, truncation=True, padding=\"max_length\").to(device)\n",
        "\n",
        "    # Log tokenization info for debugging\n",
        "    input_length = inputs[\"input_ids\"].shape[1]\n",
        "    logger.info(f\"Tokenized input length: {input_length} tokens for prompt: {prompt}\")\n",
        "\n",
        "    if input_length <= 1:\n",
        "        logger.warning(f\"Input too short after tokenization: {input_text}\")\n",
        "        return None\n",
        "\n",
        "    # Generate the output (sentiment label)\n",
        "    outputs = bart_model.generate(**inputs, max_length=50, num_beams=5, early_stopping=True)\n",
        "\n",
        "    # Decode the output and extract the generated sentiment\n",
        "    generated = tokenizer.decode(outputs[0], skip_special_tokens=True).strip().lower()\n",
        "\n",
        "    # Try to match the generated sentiment with one of the possible labels (using close matching)\n",
        "    matched_label = get_close_matches(generated, possible_labels, n=1, cutoff=0.6)\n",
        "\n",
        "    # Return the best match or None if no close match is found\n",
        "    return matched_label[0] if matched_label else None\n",
        "\n",
        "# Apply BART sentiment prediction to each row and create a new column 'predict_label'\n",
        "df['predict_label'] = df.apply(lambda row: generate_sentiment(row['réponse'], row['modalités de réponse']), axis=1)\n",
        "\n",
        "# -----------------------------------\n",
        "# 6. Save the Results\n",
        "# -----------------------------------\n",
        "output_file = os.path.join(predicted_results_dir, \"BART_predicted_sentiments.xlsx\")\n",
        "df.to_excel(output_file, index=False)\n",
        "logger.info(f\"Results saved to '{output_file}'\")\n",
        "\n",
        "# Display a sample of the results\n",
        "df[['réponse', 'échelle de Likert', 'modalités de réponse', 'label', 'predict_label']].head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "-js2ZN1VQKvT",
        "outputId": "cacfe0c5-f742-4a9f-8d02-716636a3a7e0",
        "colab": {
          "referenced_widgets": [
            "3a9e2ea496df4e79940260b8033eae2b",
            "a538a0db83ac4797b0f781ddc53c7b9b"
          ]
        }
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3a9e2ea496df4e79940260b8033eae2b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/60 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "We need to remove 1 to truncate the input but the first sequence has a length 1. \n",
            "We need to remove 1 to truncate the input but the first sequence has a length 1. \n",
            "We need to remove 1 to truncate the input but the first sequence has a length 1. \n",
            "We need to remove 1 to truncate the input but the first sequence has a length 1. \n",
            "We need to remove 1 to truncate the input but the first sequence has a length 1. \n",
            "We need to remove 1 to truncate the input but the first sequence has a length 1. \n",
            "We need to remove 1 to truncate the input but the first sequence has a length 1. \n",
            "We need to remove 1 to truncate the input but the first sequence has a length 1. \n",
            "We need to remove 1 to truncate the input but the first sequence has a length 1. \n",
            "We need to remove 1 to truncate the input but the first sequence has a length 1. \n",
            "We need to remove 1 to truncate the input but the first sequence has a length 1. \n",
            "We need to remove 1 to truncate the input but the first sequence has a length 1. \n",
            "We need to remove 1 to truncate the input but the first sequence has a length 1. \n",
            "We need to remove 1 to truncate the input but the first sequence has a length 1. \n",
            "We need to remove 1 to truncate the input but the first sequence has a length 1. \n",
            "We need to remove 1 to truncate the input but the first sequence has a length 1. \n",
            "We need to remove 1 to truncate the input but the first sequence has a length 1. \n",
            "We need to remove 1 to truncate the input but the first sequence has a length 1. \n",
            "We need to remove 1 to truncate the input but the first sequence has a length 1. \n",
            "We need to remove 1 to truncate the input but the first sequence has a length 1. \n",
            "We need to remove 1 to truncate the input but the first sequence has a length 1. \n",
            "We need to remove 1 to truncate the input but the first sequence has a length 1. \n",
            "We need to remove 1 to truncate the input but the first sequence has a length 1. \n",
            "We need to remove 1 to truncate the input but the first sequence has a length 1. \n",
            "We need to remove 1 to truncate the input but the first sequence has a length 1. \n",
            "We need to remove 1 to truncate the input but the first sequence has a length 1. \n",
            "We need to remove 1 to truncate the input but the first sequence has a length 1. \n",
            "We need to remove 1 to truncate the input but the first sequence has a length 1. \n",
            "We need to remove 1 to truncate the input but the first sequence has a length 1. \n",
            "We need to remove 1 to truncate the input but the first sequence has a length 1. \n",
            "We need to remove 1 to truncate the input but the first sequence has a length 1. \n",
            "We need to remove 1 to truncate the input but the first sequence has a length 1. \n",
            "We need to remove 1 to truncate the input but the first sequence has a length 1. \n",
            "We need to remove 1 to truncate the input but the first sequence has a length 1. \n",
            "We need to remove 1 to truncate the input but the first sequence has a length 1. \n",
            "We need to remove 1 to truncate the input but the first sequence has a length 1. \n",
            "We need to remove 1 to truncate the input but the first sequence has a length 1. \n",
            "We need to remove 1 to truncate the input but the first sequence has a length 1. \n",
            "We need to remove 1 to truncate the input but the first sequence has a length 1. \n",
            "We need to remove 1 to truncate the input but the first sequence has a length 1. \n",
            "We need to remove 1 to truncate the input but the first sequence has a length 1. \n",
            "We need to remove 1 to truncate the input but the first sequence has a length 1. \n",
            "We need to remove 1 to truncate the input but the first sequence has a length 1. \n",
            "We need to remove 1 to truncate the input but the first sequence has a length 1. \n",
            "We need to remove 1 to truncate the input but the first sequence has a length 1. \n",
            "We need to remove 1 to truncate the input but the first sequence has a length 1. \n",
            "We need to remove 1 to truncate the input but the first sequence has a length 1. \n",
            "We need to remove 1 to truncate the input but the first sequence has a length 1. \n",
            "We need to remove 1 to truncate the input but the first sequence has a length 1. \n",
            "We need to remove 1 to truncate the input but the first sequence has a length 1. \n",
            "We need to remove 1 to truncate the input but the first sequence has a length 1. \n",
            "We need to remove 1 to truncate the input but the first sequence has a length 1. \n",
            "We need to remove 1 to truncate the input but the first sequence has a length 1. \n",
            "We need to remove 1 to truncate the input but the first sequence has a length 1. \n",
            "We need to remove 1 to truncate the input but the first sequence has a length 1. \n",
            "We need to remove 1 to truncate the input but the first sequence has a length 1. \n",
            "We need to remove 1 to truncate the input but the first sequence has a length 1. \n",
            "We need to remove 1 to truncate the input but the first sequence has a length 1. \n",
            "We need to remove 1 to truncate the input but the first sequence has a length 1. \n",
            "We need to remove 1 to truncate the input but the first sequence has a length 1. \n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a538a0db83ac4797b0f781ddc53c7b9b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/40 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "We need to remove 1 to truncate the input but the first sequence has a length 1. \n",
            "We need to remove 1 to truncate the input but the first sequence has a length 1. \n",
            "We need to remove 1 to truncate the input but the first sequence has a length 1. \n",
            "We need to remove 1 to truncate the input but the first sequence has a length 1. \n",
            "We need to remove 1 to truncate the input but the first sequence has a length 1. \n",
            "We need to remove 1 to truncate the input but the first sequence has a length 1. \n",
            "We need to remove 1 to truncate the input but the first sequence has a length 1. \n",
            "We need to remove 1 to truncate the input but the first sequence has a length 1. \n",
            "We need to remove 1 to truncate the input but the first sequence has a length 1. \n",
            "We need to remove 1 to truncate the input but the first sequence has a length 1. \n",
            "We need to remove 1 to truncate the input but the first sequence has a length 1. \n",
            "We need to remove 1 to truncate the input but the first sequence has a length 1. \n",
            "We need to remove 1 to truncate the input but the first sequence has a length 1. \n",
            "We need to remove 1 to truncate the input but the first sequence has a length 1. \n",
            "We need to remove 1 to truncate the input but the first sequence has a length 1. \n",
            "We need to remove 1 to truncate the input but the first sequence has a length 1. \n",
            "We need to remove 1 to truncate the input but the first sequence has a length 1. \n",
            "We need to remove 1 to truncate the input but the first sequence has a length 1. \n",
            "We need to remove 1 to truncate the input but the first sequence has a length 1. \n",
            "We need to remove 1 to truncate the input but the first sequence has a length 1. \n",
            "We need to remove 1 to truncate the input but the first sequence has a length 1. \n",
            "We need to remove 1 to truncate the input but the first sequence has a length 1. \n",
            "We need to remove 1 to truncate the input but the first sequence has a length 1. \n",
            "We need to remove 1 to truncate the input but the first sequence has a length 1. \n",
            "We need to remove 1 to truncate the input but the first sequence has a length 1. \n",
            "We need to remove 1 to truncate the input but the first sequence has a length 1. \n",
            "We need to remove 1 to truncate the input but the first sequence has a length 1. \n",
            "We need to remove 1 to truncate the input but the first sequence has a length 1. \n",
            "We need to remove 1 to truncate the input but the first sequence has a length 1. \n",
            "We need to remove 1 to truncate the input but the first sequence has a length 1. \n",
            "We need to remove 1 to truncate the input but the first sequence has a length 1. \n",
            "We need to remove 1 to truncate the input but the first sequence has a length 1. \n",
            "We need to remove 1 to truncate the input but the first sequence has a length 1. \n",
            "We need to remove 1 to truncate the input but the first sequence has a length 1. \n",
            "We need to remove 1 to truncate the input but the first sequence has a length 1. \n",
            "We need to remove 1 to truncate the input but the first sequence has a length 1. \n",
            "We need to remove 1 to truncate the input but the first sequence has a length 1. \n",
            "We need to remove 1 to truncate the input but the first sequence has a length 1. \n",
            "We need to remove 1 to truncate the input but the first sequence has a length 1. \n",
            "We need to remove 1 to truncate the input but the first sequence has a length 1. \n",
            "/mnt/c/Users/Shiqi/Desktop/APHM/env001/lib/python3.12/site-packages/transformers/training_args.py:1545: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='20' max='20' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [20/20 07:06, Epoch 5/5]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>No log</td>\n",
              "      <td>1.180512</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>No log</td>\n",
              "      <td>1.253313</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.834856</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.780185</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.685677</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/mnt/c/Users/Shiqi/Desktop/APHM/env001/lib/python3.12/site-packages/transformers/modeling_utils.py:2618: UserWarning: Moving the following attributes in the config to the generation config: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0}. You are seeing this warning because you've set generation parameters in the model config, as opposed to in the generation config.\n",
            "  warnings.warn(\n",
            "There were missing keys in the checkpoint model loaded: ['model.encoder.embed_tokens.weight', 'model.decoder.embed_tokens.weight', 'lm_head.weight'].\n",
            "INFO:__main__:Tokenized input length: 128 tokens for prompt: Predict the sentiment of the response: 'Oui, j’ai toujours pu joindre un professionnel de santé lorsque j’en avais besoin, que ce soit par téléphone ou par email. Par exemple, une fois, j'ai eu une crise d'angoisse en pleine nuit, et j'ai rapidement été mis(e) en relation avec quelqu'un qui a pu m'aider à me calmer.' from ['negative', 'neutral', 'positive'].\n",
            "INFO:__main__:Tokenized input length: 128 tokens for prompt: Predict the sentiment of the response: 'Non, j'ai souvent eu du mal à joindre un professionnel quand j’en avais besoin. Un jour, j'ai essayé de les appeler pendant plus d'une heure, mais je suis tombé(e) sur un répondeur à chaque fois, ce qui m'a laissé(e) très anxieux(se).' from ['negative', 'neutral', 'positive'].\n",
            "INFO:__main__:Tokenized input length: 128 tokens for prompt: Predict the sentiment of the response: 'Parfois, c'était facile, mais il y avait aussi des moments où il était difficile d’avoir quelqu’un en ligne, notamment en dehors des heures normales de bureau.' from ['negative', 'neutral', 'positive'].\n",
            "INFO:__main__:Tokenized input length: 128 tokens for prompt: Predict the sentiment of the response: 'Oui, je n'ai jamais eu de problème pour contacter un professionnel de santé. Chaque fois que j'avais une question ou un besoin urgent, il y avait toujours quelqu'un disponible pour me répondre, et cela m’a beaucoup rassuré(e).' from ['negative', 'neutral', 'positive'].\n",
            "INFO:__main__:Tokenized input length: 128 tokens for prompt: Predict the sentiment of the response: 'Non, j’ai souvent dû attendre longtemps avant de pouvoir parler à quelqu'un. Par exemple, un jour où j’avais besoin d’ajuster mes médicaments, je n’ai pu parler à un médecin qu’après trois jours d'attente.' from ['negative', 'neutral', 'positive'].\n",
            "INFO:__main__:Tokenized input length: 128 tokens for prompt: Predict the sentiment of the response: 'Ça dépendait des moments. Parfois, c’était facile de les joindre, mais à d'autres moments, surtout lors des jours fériés ou week-ends, c'était presque impossible.' from ['negative', 'neutral', 'positive'].\n",
            "INFO:__main__:Tokenized input length: 128 tokens for prompt: Predict the sentiment of the response: 'Oui, à chaque fois que j'ai eu besoin de contacter un professionnel de santé, je n'ai jamais rencontré de difficulté. J’ai même pu obtenir des consultations téléphoniques rapidement, ce qui a vraiment facilité ma prise en charge.' from ['negative', 'neutral', 'positive'].\n",
            "INFO:__main__:Tokenized input length: 128 tokens for prompt: Predict the sentiment of the response: 'Non, c'était assez compliqué. J’ai souvent dû passer par plusieurs services avant de trouver la bonne personne à contacter, ce qui a créé beaucoup de frustration et d'inquiétude.' from ['negative', 'neutral', 'positive'].\n",
            "INFO:__main__:Tokenized input length: 128 tokens for prompt: Predict the sentiment of the response: 'La plupart du temps, c’était relativement facile, mais il y a eu des moments où l’on m’a renvoyé de service en service, et cela a retardé ma prise en charge.' from ['negative', 'neutral', 'positive'].\n",
            "INFO:__main__:Tokenized input length: 128 tokens for prompt: Predict the sentiment of the response: 'Oui, j’ai toujours pu contacter un professionnel lorsque j’en avais besoin, même en dehors des heures normales. Cela a beaucoup contribué à mon sentiment de sécurité durant ma période de traitement.' from ['negative', 'neutral', 'positive'].\n",
            "INFO:__main__:Tokenized input length: 128 tokens for prompt: Predict the sentiment of the response: 'Non, il y avait souvent des délais importants pour obtenir une réponse, même en cas d'urgence. Un jour, j'ai dû attendre plus de 24 heures pour avoir un retour sur un traitement qui me causait des effets secondaires graves.' from ['negative', 'neutral', 'positive'].\n",
            "INFO:__main__:Tokenized input length: 128 tokens for prompt: Predict the sentiment of the response: 'C’était généralement facile, mais il y a eu des moments où j’aurais aimé que ce soit plus simple, notamment lorsqu'il s'agissait de joindre un spécialiste en urgence.' from ['negative', 'neutral', 'positive'].\n",
            "INFO:__main__:Tokenized input length: 128 tokens for prompt: Predict the sentiment of the response: 'Oui, j'ai toujours pu parler à quelqu'un rapidement. Une fois, j'ai même eu une consultation en urgence alors que je n’avais pas de rendez-vous, ce qui m’a beaucoup aidé(e) à gérer une situation stressante.' from ['negative', 'neutral', 'positive'].\n",
            "INFO:__main__:Tokenized input length: 128 tokens for prompt: Predict the sentiment of the response: 'Non, il y a eu plusieurs fois où j'ai essayé de contacter un professionnel sans succès. Cela m’a vraiment frustré(e) et fait sentir que je n'étais pas bien pris(e) en charge.' from ['negative', 'neutral', 'positive'].\n",
            "INFO:__main__:Tokenized input length: 128 tokens for prompt: Predict the sentiment of the response: 'Parfois c'était simple, mais d'autres fois, j'avais l'impression que le personnel était débordé et que c'était difficile d'obtenir de l'aide rapidement.' from ['negative', 'neutral', 'positive'].\n",
            "INFO:__main__:Tokenized input length: 128 tokens for prompt: Predict the sentiment of the response: 'Oui, j’ai toujours trouvé que c’était facile de joindre un professionnel, même si c'était pour poser des questions simples. Leur disponibilité m’a vraiment rassuré(e) durant tout mon suivi.' from ['negative', 'neutral', 'positive'].\n",
            "INFO:__main__:Tokenized input length: 128 tokens for prompt: Predict the sentiment of the response: 'Non, j'ai souvent eu du mal à contacter quelqu'un, surtout en période de crise. J’ai souvent été redirigé(e) vers un répondeur ou un service qui ne pouvait pas répondre à mes questions urgentes.' from ['negative', 'neutral', 'positive'].\n",
            "INFO:__main__:Tokenized input length: 128 tokens for prompt: Predict the sentiment of the response: 'La plupart du temps, c’était facile, mais il y a eu des moments où j’aurais préféré que ce soit plus rapide, surtout en situation d’urgence.' from ['negative', 'neutral', 'positive'].\n",
            "INFO:__main__:Tokenized input length: 128 tokens for prompt: Predict the sentiment of the response: 'Oui, j’ai toujours pu contacter un professionnel de santé facilement, et ils étaient réactifs. Cela m’a beaucoup aidé(e), notamment lorsque j’avais des questions urgentes sur mon traitement.' from ['negative', 'neutral', 'positive'].\n",
            "INFO:__main__:Tokenized input length: 128 tokens for prompt: Predict the sentiment of the response: 'Non, j'ai souvent ressenti une absence totale de réponse lorsque j'essayais de contacter quelqu'un. Cela m’a laissé(e) désemparé(e), notamment en période de grande anxiété.' from ['negative', 'neutral', 'positive'].\n",
            "INFO:__main__:Tokenized input length: 128 tokens for prompt: Predict the sentiment of the response: 'Parfois, c'était facile de les joindre, mais il y a eu des moments où c’était frustrant, surtout quand j'avais vraiment besoin d’aide et qu’il n’y avait personne de disponible.' from ['negative', 'neutral', 'positive'].\n",
            "INFO:__main__:Tokenized input length: 128 tokens for prompt: Predict the sentiment of the response: 'Oui, c’était toujours facile d’avoir quelqu’un en ligne. Que ce soit pour des questions sur mon traitement ou mes rendez-vous, je n'ai jamais eu de souci à les joindre rapidement.' from ['negative', 'neutral', 'positive'].\n",
            "INFO:__main__:Tokenized input length: 128 tokens for prompt: Predict the sentiment of the response: 'Non, j’ai souvent dû attendre longtemps pour obtenir une réponse. Une fois, j'ai appelé plusieurs fois dans la même journée sans obtenir de réponse avant le lendemain.' from ['negative', 'neutral', 'positive'].\n",
            "INFO:__main__:Tokenized input length: 128 tokens for prompt: Predict the sentiment of the response: 'C'était facile dans l'ensemble, mais il y avait des moments où j'aurais préféré une réponse plus rapide, surtout quand j'étais en crise.' from ['negative', 'neutral', 'positive'].\n",
            "INFO:__main__:Tokenized input length: 128 tokens for prompt: Predict the sentiment of the response: 'Oui, à chaque fois que j'avais un besoin urgent, j'ai pu contacter un professionnel. Ils ont toujours été disponibles pour répondre à mes questions, même en dehors des heures normales.' from ['negative', 'neutral', 'positive'].\n",
            "INFO:__main__:Tokenized input length: 128 tokens for prompt: Predict the sentiment of the response: 'Non, il m'a fallu plusieurs tentatives pour joindre quelqu’un. J'ai souvent eu l'impression que les lignes étaient surchargées ou que le personnel était débordé, ce qui retardait ma prise en charge.' from ['negative', 'neutral', 'positive'].\n",
            "INFO:__main__:Tokenized input length: 128 tokens for prompt: Predict the sentiment of the response: 'Parfois, j’ai pu obtenir une réponse rapidement, mais il y a eu des fois où j’ai dû attendre plus longtemps que ce que j'aurais espéré, surtout en période de grande angoisse.' from ['negative', 'neutral', 'positive'].\n",
            "INFO:__main__:Tokenized input length: 128 tokens for prompt: Predict the sentiment of the response: 'Oui, j’ai toujours trouvé quelqu’un disponible pour m’aider rapidement, que ce soit par téléphone ou par message. Cela m’a beaucoup rassuré(e) dans les moments difficiles.' from ['negative', 'neutral', 'positive'].\n",
            "INFO:__main__:Tokenized input length: 128 tokens for prompt: Predict the sentiment of the response: 'Non, il y a eu plusieurs fois où j'ai appelé sans obtenir de réponse pendant des jours. Cela a créé beaucoup d'anxiété, car j'avais besoin de conseils sur mon traitement.' from ['negative', 'neutral', 'positive'].\n",
            "INFO:__main__:Tokenized input length: 128 tokens for prompt: Predict the sentiment of the response: 'En général, c'était assez simple de les contacter, mais il y a eu des moments où l'équipe soignante semblait débordée et il fallait attendre plus longtemps pour obtenir une réponse.' from ['negative', 'neutral', 'positive'].\n",
            "INFO:__main__:Tokenized input length: 128 tokens for prompt: Predict the sentiment of the response: 'Oui, j'ai pu facilement joindre un professionnel de santé à chaque fois que j’en ai eu besoin. Ils étaient toujours disponibles pour m’accompagner et me conseiller, même en dehors des heures habituelles.' from ['negative', 'neutral', 'positive'].\n",
            "INFO:__main__:Tokenized input length: 128 tokens for prompt: Predict the sentiment of the response: 'Non, j’ai souvent dû faire plusieurs tentatives avant de réussir à parler à quelqu’un. Cela a créé une frustration importante, surtout quand j'avais des urgences médicales.' from ['negative', 'neutral', 'positive'].\n",
            "INFO:__main__:Tokenized input length: 128 tokens for prompt: Predict the sentiment of the response: 'La plupart du temps, j’ai réussi à obtenir des réponses rapidement, mais il y a eu des moments où c'était plus difficile, surtout pendant les week-ends et les jours fériés.' from ['negative', 'neutral', 'positive'].\n",
            "INFO:__main__:Tokenized input length: 128 tokens for prompt: Predict the sentiment of the response: 'Oui, chaque fois que j'avais une question ou une inquiétude, je pouvais facilement joindre quelqu'un pour me répondre, ce qui m’a vraiment aidé(e) à gérer mes angoisses.' from ['negative', 'neutral', 'positive'].\n",
            "INFO:__main__:Tokenized input length: 128 tokens for prompt: Predict the sentiment of the response: 'Non, c'était compliqué de joindre quelqu’un rapidement. J'ai souvent eu l’impression que mes besoins n'étaient pas prioritaires, et cela a créé beaucoup d’anxiété.' from ['negative', 'neutral', 'positive'].\n",
            "INFO:__main__:Tokenized input length: 128 tokens for prompt: Predict the sentiment of the response: 'Parfois c’était facile de les joindre, mais il y a eu des moments où j'aurais aimé que ce soit plus simple, surtout lorsque j'étais en situation de crise et que j’avais besoin de réponses urgentes.' from ['negative', 'neutral', 'positive'].\n",
            "INFO:__main__:Tokenized input length: 128 tokens for prompt: Predict the sentiment of the response: 'Oui, à chaque fois que j’ai eu besoin d’aide, j’ai pu facilement contacter un professionnel. Le fait de pouvoir parler à quelqu’un rapidement m’a énormément rassuré(e).' from ['negative', 'neutral', 'positive'].\n",
            "INFO:__main__:Tokenized input length: 128 tokens for prompt: Predict the sentiment of the response: 'Non, j’ai souvent dû passer par plusieurs intermédiaires avant de trouver quelqu’un qui pouvait répondre à mes questions, ce qui a ralenti ma prise en charge.' from ['negative', 'neutral', 'positive'].\n",
            "INFO:__main__:Tokenized input length: 128 tokens for prompt: Predict the sentiment of the response: 'Globalement, j’ai réussi à obtenir des réponses rapidement, mais il y a eu des moments où l'attente était plus longue que prévu, surtout pendant les périodes de forte affluence.' from ['negative', 'neutral', 'positive'].\n",
            "INFO:__main__:Tokenized input length: 128 tokens for prompt: Predict the sentiment of the response: 'Oui, chaque fois que j'avais une inquiétude, j'ai pu parler rapidement à un professionnel de santé. Cela a vraiment aidé à diminuer mon stress, notamment en période de crise.' from ['negative', 'neutral', 'positive'].\n",
            "INFO:__main__:Tokenized input length: 128 tokens for prompt: Predict the sentiment of the response: 'Non, il m'a fallu plusieurs tentatives avant de réussir à obtenir de l’aide. Cela m’a laissé(e) démuni(e), car j’avais vraiment besoin d’une réponse urgente à propos de mon traitement.' from ['negative', 'neutral', 'positive'].\n",
            "INFO:__main__:Tokenized input length: 128 tokens for prompt: Predict the sentiment of the response: 'C'était parfois facile de les joindre, mais il y a eu des moments où j’ai dû attendre plusieurs jours avant de recevoir une réponse, ce qui a créé de l’inquiétude.' from ['negative', 'neutral', 'positive'].\n",
            "INFO:__main__:Tokenized input length: 128 tokens for prompt: Predict the sentiment of the response: 'Oui, j’ai toujours réussi à parler à un professionnel de santé quand j’en avais besoin, même pour des petites questions. Cela m’a beaucoup aidé(e) à me sentir accompagné(e).' from ['negative', 'neutral', 'positive'].\n",
            "INFO:__main__:Tokenized input length: 128 tokens for prompt: Predict the sentiment of the response: 'Non, je n’ai souvent pas réussi à obtenir de réponse dans un délai raisonnable. Il y avait des jours où j’appelais plusieurs fois sans jamais réussir à parler à quelqu’un.' from ['negative', 'neutral', 'positive'].\n",
            "INFO:__main__:Tokenized input length: 128 tokens for prompt: Predict the sentiment of the response: 'Globalement, ça s’est bien passé, mais il y a eu des moments où j’ai eu du mal à obtenir une réponse rapide, ce qui a parfois prolongé mon anxiété.' from ['negative', 'neutral', 'positive'].\n",
            "INFO:__main__:Tokenized input length: 128 tokens for prompt: Predict the sentiment of the response: 'Oui, chaque fois que j'ai eu une question, j’ai pu rapidement parler à quelqu'un qui pouvait me répondre. Cela m'a beaucoup aidé(e) à gérer mes préoccupations en lien avec mon traitement.' from ['negative', 'neutral', 'positive'].\n",
            "INFO:__main__:Tokenized input length: 128 tokens for prompt: Predict the sentiment of the response: 'Non, c’était très compliqué d'obtenir une réponse rapidement. Il y a eu des moments où j'avais besoin de conseils médicaux urgents et je n’ai pas pu parler à quelqu’un avant plusieurs jours.' from ['negative', 'neutral', 'positive'].\n",
            "INFO:__main__:Tokenized input length: 128 tokens for prompt: Predict the sentiment of the response: 'Parfois c’était simple, mais il y a eu des moments où j’ai dû attendre longtemps avant de pouvoir joindre un professionnel, ce qui a augmenté mon sentiment d’incertitude.' from ['negative', 'neutral', 'positive'].\n",
            "INFO:__main__:Tokenized input length: 128 tokens for prompt: Predict the sentiment of the response: 'Oui, à chaque fois que j'avais une inquiétude ou un besoin, j'ai pu contacter un professionnel sans difficulté. Cela a vraiment contribué à réduire mon stress.' from ['negative', 'neutral', 'positive'].\n",
            "INFO:__main__:Tokenized input length: 128 tokens for prompt: Predict the sentiment of the response: 'Non, il y avait souvent des délais avant de pouvoir parler à quelqu’un. Cela m’a fait me sentir laissé(e) pour compte, surtout dans les moments où j’avais des questions urgentes.' from ['negative', 'neutral', 'positive'].\n",
            "INFO:__main__:Tokenized input length: 128 tokens for prompt: Predict the sentiment of the response: 'Oui, j’ai pu facilement reporter mes rendez-vous chaque fois que j’en ai eu besoin. Par exemple, il y a une fois où j’étais malade et j'ai simplement appelé le secrétariat pour déplacer ma séance, et ils ont immédiatement trouvé une nouvelle date sans aucune difficulté.' from ['negative', 'neutral', 'positive'].\n",
            "INFO:__main__:Tokenized input length: 128 tokens for prompt: Predict the sentiment of the response: 'Non, c’était assez compliqué de faire reporter un rendez-vous. J’ai dû insister plusieurs fois et donner de nombreuses explications avant qu’ils acceptent de changer la date, ce qui a été une source de stress pour moi.' from ['negative', 'neutral', 'positive'].\n",
            "INFO:__main__:Tokenized input length: 128 tokens for prompt: Predict the sentiment of the response: 'Ça dépend des fois, parfois c’était facile de déplacer un rendez-vous, mais il y a eu des moments où j’ai dû attendre plusieurs jours avant de recevoir une nouvelle proposition de date.' from ['negative', 'neutral', 'positive'].\n",
            "INFO:__main__:Tokenized input length: 128 tokens for prompt: Predict the sentiment of the response: 'Oui, chaque fois que j'avais besoin de changer une date, le secrétariat a toujours été réactif et compréhensif. Ils m’ont proposé plusieurs créneaux pour que je puisse choisir ce qui me convenait le mieux.' from ['negative', 'neutral', 'positive'].\n",
            "INFO:__main__:Tokenized input length: 128 tokens for prompt: Predict the sentiment of the response: 'Non, j'ai rencontré plusieurs difficultés à faire changer mes rendez-vous. Une fois, j'ai eu un empêchement et malgré mes appels répétés, il n'y avait aucune disponibilité avant plusieurs semaines, ce qui a retardé ma prise en charge.' from ['negative', 'neutral', 'positive'].\n",
            "INFO:__main__:Tokenized input length: 128 tokens for prompt: Predict the sentiment of the response: 'C’était assez simple la plupart du temps, mais il y a eu des occasions où ils avaient du mal à trouver des disponibilités rapidement, et j’ai dû attendre un moment avant de pouvoir reprogrammer.' from ['negative', 'neutral', 'positive'].\n",
            "INFO:__main__:Tokenized input length: 128 tokens for prompt: Predict the sentiment of the response: 'Oui, j’ai toujours pu facilement reporter mes rendez-vous. Une fois, j'avais un conflit avec une autre consultation, et l'équipe a trouvé une nouvelle date sans aucune difficulté, ce qui a rendu le processus très fluide.' from ['negative', 'neutral', 'positive'].\n",
            "INFO:__main__:Tokenized input length: 128 tokens for prompt: Predict the sentiment of the response: 'Non, c'était compliqué. À chaque fois que j'ai essayé de déplacer un rendez-vous, on me disait que les créneaux étaient pleins pendant plusieurs semaines, ce qui m'a laissé(e) très frustré(e).' from ['negative', 'neutral', 'positive'].\n",
            "INFO:__main__:Tokenized input length: 128 tokens for prompt: Predict the sentiment of the response: 'Ça dépendait de la période. Quand ils étaient moins occupés, c’était facile, mais durant les périodes de forte affluence, il était parfois difficile de trouver un nouveau créneau rapidement.' from ['negative', 'neutral', 'positive'].\n",
            "INFO:__main__:Tokenized input length: 128 tokens for prompt: Predict the sentiment of the response: 'Oui, j’ai toujours trouvé que c’était facile de reporter mes rendez-vous, même à la dernière minute. Cela m'a beaucoup aidé(e) car j’avais parfois des contraintes imprévues qui nécessitaient un ajustement rapide.' from ['negative', 'neutral', 'positive'].\n",
            "INFO:__main__:Tokenized input length: 128 tokens for prompt: Predict the sentiment of the response: 'Non, j'ai rencontré de nombreuses difficultés à faire déplacer mes rendez-vous. Il y avait rarement des disponibilités proches, et j’ai souvent dû insister pour qu’on me propose quelque chose plus tôt.' from ['negative', 'neutral', 'positive'].\n",
            "INFO:__main__:Tokenized input length: 128 tokens for prompt: Predict the sentiment of the response: 'La plupart du temps, j’ai réussi à reporter mes rendez-vous sans trop de complications, mais il y a eu des moments où les délais de disponibilité étaient un peu longs.' from ['negative', 'neutral', 'positive'].\n",
            "INFO:__main__:Tokenized input length: 128 tokens for prompt: Predict the sentiment of the response: 'Oui, chaque fois que j’ai eu besoin de changer une date, ils étaient très réactifs et trouvaient toujours un créneau qui convenait rapidement. Cela m’a permis de gérer mon emploi du temps sans souci.' from ['negative', 'neutral', 'positive'].\n",
            "INFO:__main__:Tokenized input length: 128 tokens for prompt: Predict the sentiment of the response: 'Non, j’ai souvent eu du mal à faire déplacer mes rendez-vous. Une fois, ils m'ont dit qu’il n’y avait pas de place avant un mois, ce qui a retardé mon suivi de manière importante.' from ['negative', 'neutral', 'positive'].\n",
            "INFO:__main__:Tokenized input length: 128 tokens for prompt: Predict the sentiment of the response: 'Parfois c’était facile, mais il y a eu des moments où les disponibilités étaient rares, et j’ai dû attendre plusieurs jours avant d’obtenir une nouvelle date.' from ['negative', 'neutral', 'positive'].\n",
            "INFO:__main__:Tokenized input length: 128 tokens for prompt: Predict the sentiment of the response: 'Oui, chaque fois que j’avais un empêchement, je pouvais rapidement contacter le secrétariat pour déplacer mon rendez-vous. Ils étaient toujours arrangeants, même pour des demandes de dernière minute.' from ['negative', 'neutral', 'positive'].\n",
            "INFO:__main__:Tokenized input length: 128 tokens for prompt: Predict the sentiment of the response: 'Non, ça a souvent été compliqué. Il y avait rarement des créneaux disponibles rapidement, et cela m’a souvent forcé(e) à annuler complètement mes rendez-vous au lieu de les déplacer.' from ['negative', 'neutral', 'positive'].\n",
            "INFO:__main__:Tokenized input length: 128 tokens for prompt: Predict the sentiment of the response: 'Ça s'est bien passé dans l’ensemble, mais il y a eu des moments où il a été plus difficile de trouver un nouveau créneau à cause de l’affluence.' from ['negative', 'neutral', 'positive'].\n",
            "INFO:__main__:Tokenized input length: 128 tokens for prompt: Predict the sentiment of the response: 'Oui, j’ai toujours trouvé que c’était facile de faire reporter un rendez-vous, même à la dernière minute. Cela a vraiment aidé à m’adapter aux changements dans mon emploi du temps.' from ['negative', 'neutral', 'positive'].\n",
            "INFO:__main__:Tokenized input length: 128 tokens for prompt: Predict the sentiment of the response: 'Non, il m’a souvent été difficile de déplacer un rendez-vous, surtout quand j’avais besoin d’un créneau proche. Cela m’a causé beaucoup de stress car je devais jongler avec d’autres engagements importants.' from ['negative', 'neutral', 'positive'].\n",
            "INFO:__main__:Tokenized input length: 128 tokens for prompt: Predict the sentiment of the response: 'Parfois c’était facile, mais à d’autres moments, ils avaient du mal à trouver des créneaux, surtout pendant les périodes où il y avait beaucoup de demandes.' from ['negative', 'neutral', 'positive'].\n",
            "INFO:__main__:Tokenized input length: 128 tokens for prompt: Predict the sentiment of the response: 'Oui, chaque fois que j’avais besoin de changer un rendez-vous, j’ai pu facilement obtenir une nouvelle date sans problème. Le personnel était toujours très compréhensif.' from ['negative', 'neutral', 'positive'].\n",
            "INFO:__main__:Tokenized input length: 128 tokens for prompt: Predict the sentiment of the response: 'Non, ça a été compliqué. À plusieurs reprises, j’ai dû reporter mes rendez-vous, mais les délais pour obtenir une nouvelle date étaient très longs, ce qui m’a beaucoup frustré(e).' from ['negative', 'neutral', 'positive'].\n",
            "INFO:__main__:Tokenized input length: 128 tokens for prompt: Predict the sentiment of the response: 'La plupart du temps, j’ai pu déplacer mes rendez-vous sans trop de difficultés, mais il y a eu des moments où les nouvelles disponibilités étaient assez éloignées.' from ['negative', 'neutral', 'positive'].\n",
            "INFO:__main__:Tokenized input length: 128 tokens for prompt: Predict the sentiment of the response: 'Oui, j’ai toujours pu faire reporter mes rendez-vous sans problème. Que ce soit pour des raisons professionnelles ou personnelles, ils trouvaient toujours un moyen de m’arranger.' from ['negative', 'neutral', 'positive'].\n",
            "INFO:__main__:Tokenized input length: 128 tokens for prompt: Predict the sentiment of the response: 'Non, c'était souvent très difficile de trouver une nouvelle date quand je devais reporter un rendez-vous. Les créneaux disponibles étaient souvent éloignés, ce qui a perturbé mon suivi.' from ['negative', 'neutral', 'positive'].\n",
            "INFO:__main__:Tokenized input length: 128 tokens for prompt: Predict the sentiment of the response: 'Ça dépendait de la période. Parfois c'était facile, mais il y avait des moments où j'avais du mal à trouver un nouveau créneau dans un délai raisonnable.' from ['negative', 'neutral', 'positive'].\n",
            "INFO:__main__:Tokenized input length: 128 tokens for prompt: Predict the sentiment of the response: 'Oui, chaque fois que j’ai eu besoin de reporter un rendez-vous, le secrétariat a été très réactif. Ils ont toujours été disponibles pour m’aider à trouver une solution rapidement.' from ['negative', 'neutral', 'positive'].\n",
            "INFO:__main__:Tokenized input length: 128 tokens for prompt: Predict the sentiment of the response: 'Non, j’ai rencontré plusieurs difficultés pour faire changer mes rendez-vous. Souvent, ils me proposaient des dates beaucoup trop éloignées, ce qui a ralenti mon suivi.' from ['negative', 'neutral', 'positive'].\n",
            "INFO:__main__:Tokenized input length: 128 tokens for prompt: Predict the sentiment of the response: 'La plupart du temps, c’était assez simple de déplacer un rendez-vous, mais il y a eu des moments où les disponibilités étaient rares et j’ai dû attendre plus longtemps que prévu.' from ['negative', 'neutral', 'positive'].\n",
            "INFO:__main__:Tokenized input length: 128 tokens for prompt: Predict the sentiment of the response: 'Oui, j’ai toujours pu faire reporter mes rendez-vous sans difficulté. Ils étaient toujours disponibles pour me proposer une nouvelle date adaptée à mon emploi du temps.' from ['negative', 'neutral', 'positive'].\n",
            "INFO:__main__:Tokenized input length: 128 tokens for prompt: Predict the sentiment of the response: 'Non, j’ai souvent dû attendre des semaines pour obtenir une nouvelle date après avoir reporté un rendez-vous, ce qui a été frustrant car cela retardait mon suivi médical.' from ['negative', 'neutral', 'positive'].\n",
            "INFO:__main__:Tokenized input length: 128 tokens for prompt: Predict the sentiment of the response: 'Parfois c'était simple, mais il y a eu des périodes où il était difficile de trouver un nouveau créneau proche, surtout en période de forte affluence.' from ['negative', 'neutral', 'positive'].\n",
            "INFO:__main__:Tokenized input length: 128 tokens for prompt: Predict the sentiment of the response: 'Oui, chaque fois que j'avais un imprévu, j’ai pu facilement déplacer mes rendez-vous sans problème. Ils étaient très flexibles, ce qui m'a aidé(e) à bien gérer mon suivi.' from ['negative', 'neutral', 'positive'].\n",
            "INFO:__main__:Tokenized input length: 128 tokens for prompt: Predict the sentiment of the response: 'Non, il y avait souvent des délais importants pour obtenir un nouveau rendez-vous. Cela a créé beaucoup d’anxiété car j'avais besoin de consultations régulières.' from ['negative', 'neutral', 'positive'].\n",
            "INFO:__main__:Tokenized input length: 128 tokens for prompt: Predict the sentiment of the response: 'Ça dépendait de la disponibilité. Parfois c'était simple, mais à d'autres moments, j'avais du mal à obtenir un créneau proche pour reporter un rendez-vous.' from ['negative', 'neutral', 'positive'].\n",
            "INFO:__main__:Tokenized input length: 128 tokens for prompt: Predict the sentiment of the response: 'Oui, j’ai toujours trouvé que c’était facile de reporter un rendez-vous. Le personnel était très compréhensif et trouvait toujours des solutions rapidement.' from ['negative', 'neutral', 'positive'].\n",
            "INFO:__main__:Tokenized input length: 128 tokens for prompt: Predict the sentiment of the response: 'Non, à chaque fois que j'essayais de reporter un rendez-vous, on me disait que les créneaux disponibles étaient pleins pour plusieurs semaines, ce qui était très frustrant.' from ['negative', 'neutral', 'positive'].\n",
            "INFO:__main__:Tokenized input length: 128 tokens for prompt: Predict the sentiment of the response: 'Globalement, c’était facile de reporter un rendez-vous, mais il y a eu des moments où j’ai dû patienter plusieurs jours avant d’obtenir une nouvelle date.' from ['negative', 'neutral', 'positive'].\n",
            "INFO:__main__:Tokenized input length: 128 tokens for prompt: Predict the sentiment of the response: 'Oui, à chaque fois que j’avais un imprévu, j’ai pu facilement déplacer mes rendez-vous sans souci. Cela m’a permis de bien gérer mon emploi du temps tout en continuant mon traitement.' from ['negative', 'neutral', 'positive'].\n",
            "INFO:__main__:Tokenized input length: 128 tokens for prompt: Predict the sentiment of the response: 'Non, c'était souvent compliqué de trouver une nouvelle date qui convenait, surtout quand les créneaux étaient complets. Cela a causé beaucoup de retard dans mon suivi.' from ['negative', 'neutral', 'positive'].\n",
            "INFO:__main__:Tokenized input length: 128 tokens for prompt: Predict the sentiment of the response: 'Ça dépendait des périodes. Parfois, c'était facile de reporter un rendez-vous, mais à d'autres moments, il y avait beaucoup de demande et les délais étaient longs.' from ['negative', 'neutral', 'positive'].\n",
            "INFO:__main__:Tokenized input length: 128 tokens for prompt: Predict the sentiment of the response: 'Oui, j'ai toujours pu reporter mes rendez-vous sans difficulté. Cela m’a beaucoup aidé(e), surtout lorsque j’avais des contraintes professionnelles ou personnelles imprévues.' from ['negative', 'neutral', 'positive'].\n",
            "INFO:__main__:Tokenized input length: 128 tokens for prompt: Predict the sentiment of the response: 'Non, il m’a souvent été difficile de faire déplacer mes rendez-vous, surtout quand j’avais besoin de changer de date à la dernière minute. Les créneaux étaient souvent pleins pendant plusieurs semaines.' from ['negative', 'neutral', 'positive'].\n",
            "INFO:__main__:Tokenized input length: 128 tokens for prompt: Predict the sentiment of the response: 'Parfois c’était facile, mais il y a eu des moments où il était difficile de trouver des disponibilités proches, surtout pendant les périodes de forte affluence.' from ['negative', 'neutral', 'positive'].\n",
            "INFO:__main__:Tokenized input length: 128 tokens for prompt: Predict the sentiment of the response: 'Oui, chaque fois que j’avais un imprévu, j’ai pu reporter mes rendez-vous sans problème. Cela m’a beaucoup rassuré(e) car j'avais besoin de flexibilité.' from ['negative', 'neutral', 'positive'].\n",
            "INFO:__main__:Tokenized input length: 128 tokens for prompt: Predict the sentiment of the response: 'Non, c’était souvent difficile de reporter un rendez-vous, surtout quand les créneaux disponibles étaient limités. Cela m’a causé beaucoup de stress.' from ['negative', 'neutral', 'positive'].\n",
            "INFO:__main__:Tokenized input length: 128 tokens for prompt: Predict the sentiment of the response: 'Globalement, c’était facile de faire déplacer un rendez-vous, mais il y avait des moments où les disponibilités étaient plus rares, ce qui allongeait le temps d’attente.' from ['negative', 'neutral', 'positive'].\n",
            "INFO:__main__:Tokenized input length: 128 tokens for prompt: Predict the sentiment of the response: 'Oui, j’ai toujours pu faire reporter mes rendez-vous sans souci. Ils étaient toujours disponibles pour trouver une nouvelle date adaptée à mes besoins.' from ['negative', 'neutral', 'positive'].\n",
            "INFO:__main__:Tokenized input length: 128 tokens for prompt: Predict the sentiment of the response: 'Non, il m’a souvent été difficile de faire changer mes rendez-vous, surtout quand il y avait beaucoup de demandes et que les créneaux étaient pleins.' from ['negative', 'neutral', 'positive'].\n",
            "INFO:__main__:Results saved to '/mnt/c/Users/Shiqi/Desktop/APHM/Outputs/Predicted_Results/BART_predicted_sentiments_response_only.xlsx'\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CPU times: user 2min 4s, sys: 12.6 s, total: 2min 16s\n",
            "Wall time: 7min 29s\n",
            "CPU times: user 2min 4s, sys: 12.6 s, total: 2min 16s\n",
            "Wall time: 7min 29s\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>réponse</th>\n",
              "      <th>échelle de Likert</th>\n",
              "      <th>modalités de réponse</th>\n",
              "      <th>label</th>\n",
              "      <th>predict_label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Oui, j’ai toujours pu joindre un professionnel...</td>\n",
              "      <td>2</td>\n",
              "      <td>0,1,2</td>\n",
              "      <td>positive</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Non, j'ai souvent eu du mal à joindre un profe...</td>\n",
              "      <td>0</td>\n",
              "      <td>0,1,2</td>\n",
              "      <td>negative</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Parfois, c'était facile, mais il y avait aussi...</td>\n",
              "      <td>1</td>\n",
              "      <td>0,1,2</td>\n",
              "      <td>neutral</td>\n",
              "      <td>neutral</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Oui, je n'ai jamais eu de problème pour contac...</td>\n",
              "      <td>2</td>\n",
              "      <td>0,1,2</td>\n",
              "      <td>positive</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Non, j’ai souvent dû attendre longtemps avant ...</td>\n",
              "      <td>0</td>\n",
              "      <td>0,1,2</td>\n",
              "      <td>negative</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                             réponse  échelle de Likert  \\\n",
              "0  Oui, j’ai toujours pu joindre un professionnel...                  2   \n",
              "1  Non, j'ai souvent eu du mal à joindre un profe...                  0   \n",
              "2  Parfois, c'était facile, mais il y avait aussi...                  1   \n",
              "3  Oui, je n'ai jamais eu de problème pour contac...                  2   \n",
              "4  Non, j’ai souvent dû attendre longtemps avant ...                  0   \n",
              "\n",
              "  modalités de réponse     label predict_label  \n",
              "0                0,1,2  positive      positive  \n",
              "1                0,1,2  negative      negative  \n",
              "2                0,1,2   neutral       neutral  \n",
              "3                0,1,2  positive      positive  \n",
              "4                0,1,2  negative      negative  "
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "%%time\n",
        "import os\n",
        "import pandas as pd\n",
        "from transformers import BartTokenizer, BartForConditionalGeneration, Seq2SeqTrainer, Seq2SeqTrainingArguments\n",
        "from transformers import EarlyStoppingCallback\n",
        "from datasets import Dataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "import torch\n",
        "import logging\n",
        "\n",
        "# -----------------------------------\n",
        "# 1. Configuration and Setup\n",
        "# -----------------------------------\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "log_dir = os.getcwd()\n",
        "predicted_results_dir = os.path.join(log_dir, \"Outputs/Predicted_Results\")\n",
        "os.makedirs(predicted_results_dir, exist_ok=True)\n",
        "\n",
        "# Logging Setup\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# -----------------------------------\n",
        "# 2. Load BART Model and Tokenizer\n",
        "# -----------------------------------\n",
        "tokenizer = BartTokenizer.from_pretrained(\"facebook/bart-large\")\n",
        "bart_model = BartForConditionalGeneration.from_pretrained(\"facebook/bart-large\").to(device)\n",
        "\n",
        "# -----------------------------------\n",
        "# 3. Mapping Likert Values to Labels\n",
        "# -----------------------------------\n",
        "def map_likert_to_labels(row):\n",
        "    modalites_de_reponse = row['modalités de réponse']\n",
        "    likert_value = row['échelle de Likert']\n",
        "\n",
        "    if modalites_de_reponse == \"0,1,2\":\n",
        "        return [\"negative\", \"neutral\", \"positive\"][likert_value]\n",
        "    elif modalites_de_reponse == \"0,1,2,3,4\":\n",
        "        return [\"strong negative\", \"negative\", \"neutral\", \"positive\", \"strong positive\"][likert_value]\n",
        "\n",
        "# Load the dataset (item_bank)\n",
        "df = item_bank.head(100)[['libellé', 'réponse', 'échelle de Likert', 'modalités de réponse']]\n",
        "\n",
        "# Apply the Likert value-to-label mapping\n",
        "df['label'] = df.apply(map_likert_to_labels, axis=1)\n",
        "\n",
        "# Split the data into training and test sets\n",
        "train_df, test_df = train_test_split(df, test_size=0.4, shuffle=True, random_state=42)\n",
        "\n",
        "# Convert the data into Hugging Face Dataset format\n",
        "train_dataset = Dataset.from_pandas(train_df)\n",
        "test_dataset = Dataset.from_pandas(test_df)\n",
        "\n",
        "# -----------------------------------\n",
        "# 4. Fine-tuning BART Model with Explicit Constraints (Only Response)\n",
        "# -----------------------------------\n",
        "def tokenize_function(examples):\n",
        "    # Only use response in the input (no question)\n",
        "    inputs = examples['réponse']\n",
        "    outputs = [str(label) for label in examples['label']]\n",
        "    model_inputs = tokenizer(inputs, max_length=128, truncation=True, padding=\"max_length\")\n",
        "    labels = tokenizer(outputs, max_length=2, truncation=True, padding=\"max_length\").input_ids\n",
        "    model_inputs['labels'] = labels\n",
        "    return model_inputs\n",
        "\n",
        "train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
        "test_dataset = test_dataset.map(tokenize_function, batched=True)\n",
        "\n",
        "# Set format for Trainer to understand the labels\n",
        "train_dataset.set_format(type=\"torch\", columns=['input_ids', 'attention_mask', 'labels'])\n",
        "test_dataset.set_format(type=\"torch\", columns=['input_ids', 'attention_mask', 'labels'])\n",
        "\n",
        "# Fine-tuning BART with Early Stopping and Learning Rate Scheduler\n",
        "training_args = Seq2SeqTrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=16,\n",
        "    num_train_epochs=5,\n",
        "    weight_decay=0.01,\n",
        "    learning_rate=5e-5,\n",
        "    lr_scheduler_type=\"linear\",\n",
        "    load_best_model_at_end=True,\n",
        "    save_total_limit=2,\n",
        "    save_strategy=\"epoch\",\n",
        "    metric_for_best_model=\"eval_loss\"\n",
        ")\n",
        "\n",
        "trainer = Seq2SeqTrainer(\n",
        "    model=bart_model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=test_dataset,\n",
        "    tokenizer=tokenizer,\n",
        "    callbacks=[EarlyStoppingCallback(early_stopping_patience=2)]\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "trainer.train()\n",
        "\n",
        "# Save the fine-tuned model\n",
        "bart_model.save_pretrained(\"./fine_tuned_bart_likert\")\n",
        "tokenizer.save_pretrained(\"./fine_tuned_bart_likert\")\n",
        "\n",
        "# -----------------------------------\n",
        "# 5. Predicting Sentiment with Constrained Labels Using BART (Only Response)\n",
        "# -----------------------------------\n",
        "from difflib import get_close_matches\n",
        "\n",
        "def generate_sentiment(input_text, modalites_de_reponse):\n",
        "    if modalites_de_reponse == \"0,1,2\":\n",
        "        possible_labels = [\"negative\", \"neutral\", \"positive\"]\n",
        "    elif modalites_de_reponse == \"0,1,2,3,4\":\n",
        "        possible_labels = [\"strong negative\", \"negative\", \"neutral\", \"positive\", \"strong positive\"]\n",
        "\n",
        "    # Create the prompt using only the response\n",
        "    prompt = f\"Predict the sentiment of the response: '{input_text}' from {possible_labels}.\"\n",
        "\n",
        "    # Tokenize the input\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\", max_length=128, truncation=True, padding=\"max_length\").to(device)\n",
        "\n",
        "    # Log tokenization info for debugging\n",
        "    input_length = inputs[\"input_ids\"].shape[1]\n",
        "    logger.info(f\"Tokenized input length: {input_length} tokens for prompt: {prompt}\")\n",
        "\n",
        "    if input_length <= 1:\n",
        "        logger.warning(f\"Input too short after tokenization: {input_text}\")\n",
        "        return None\n",
        "\n",
        "    # Generate the output (sentiment label)\n",
        "    outputs = bart_model.generate(**inputs, max_length=50, num_beams=5, early_stopping=True)\n",
        "\n",
        "    # Decode the output and extract the generated sentiment\n",
        "    generated = tokenizer.decode(outputs[0], skip_special_tokens=True).strip().lower()\n",
        "\n",
        "    # Try to match the generated sentiment with one of the possible labels (using close matching)\n",
        "    matched_label = get_close_matches(generated, possible_labels, n=1, cutoff=0.6)\n",
        "\n",
        "    # Return the best match or None if no close match is found\n",
        "    return matched_label[0] if matched_label else None\n",
        "\n",
        "# Apply BART sentiment prediction to each row and create a new column 'predict_label'\n",
        "df['predict_label'] = df.apply(lambda row: generate_sentiment(row['réponse'], row['modalités de réponse']), axis=1)\n",
        "\n",
        "# -----------------------------------\n",
        "# 6. Save the Results\n",
        "# -----------------------------------\n",
        "output_file = os.path.join(predicted_results_dir, \"BART_predicted_sentiments_response_only.xlsx\")\n",
        "df.to_excel(output_file, index=False)\n",
        "logger.info(f\"Results saved to '{output_file}'\")\n",
        "\n",
        "# Display a sample of the results\n",
        "df[['réponse', 'échelle de Likert', 'modalités de réponse', 'label', 'predict_label']].head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ld2QuPbKQKvT"
      },
      "source": [
        "### 7.3.3 [T5 - Fine-Tuning & Prompt Engineering](#toc7_3_3_)\n",
        "<div style=\"background-color: lightskyblue; width: 100%; height: 30px; padding: 10px; display: center; justify-content: center; align-items: center;\">\n",
        "  <p style=\"text-align: center; font-size: 20px; font-weight:bold;\">\n",
        "    T5 - Fine-Tuning & Prompt Engineering\n",
        "  </p>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rMWtwncEQKvT"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b-vg5bHzQKvT",
        "outputId": "3a6fe32b-1b3a-4a48-a99c-ae141b06c1f4",
        "colab": {
          "referenced_widgets": [
            "dd9099d6fa57459d98aeb9e59d87aeee",
            "2183fe23b6f04d65a1aa616da558bd6c"
          ]
        }
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "dd9099d6fa57459d98aeb9e59d87aeee",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/60 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2183fe23b6f04d65a1aa616da558bd6c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/40 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/mnt/c/Users/Shiqi/Desktop/APHM/env001/lib/python3.12/site-packages/transformers/training_args.py:1545: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='20' max='20' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [20/20 01:01, Epoch 5/5]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>No log</td>\n",
              "      <td>3.513800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>No log</td>\n",
              "      <td>2.723027</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>No log</td>\n",
              "      <td>1.995758</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>No log</td>\n",
              "      <td>1.580740</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>No log</td>\n",
              "      <td>1.441074</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "There were missing keys in the checkpoint model loaded: ['encoder.embed_tokens.weight', 'decoder.embed_tokens.weight', 'lm_head.weight'].\n",
            "Results saved to '/mnt/c/Users/Shiqi/Desktop/APHM/Outputs/Predicted_Results/T5_predicted_sentiments.xlsx'\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>réponse</th>\n",
              "      <th>échelle de Likert</th>\n",
              "      <th>modalités de réponse</th>\n",
              "      <th>label</th>\n",
              "      <th>predict_label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Oui, j’ai toujours pu joindre un professionnel...</td>\n",
              "      <td>2</td>\n",
              "      <td>0,1,2</td>\n",
              "      <td>positive</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Non, j'ai souvent eu du mal à joindre un profe...</td>\n",
              "      <td>0</td>\n",
              "      <td>0,1,2</td>\n",
              "      <td>negative</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Parfois, c'était facile, mais il y avait aussi...</td>\n",
              "      <td>1</td>\n",
              "      <td>0,1,2</td>\n",
              "      <td>neutral</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Oui, je n'ai jamais eu de problème pour contac...</td>\n",
              "      <td>2</td>\n",
              "      <td>0,1,2</td>\n",
              "      <td>positive</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Non, j’ai souvent dû attendre longtemps avant ...</td>\n",
              "      <td>0</td>\n",
              "      <td>0,1,2</td>\n",
              "      <td>negative</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                             réponse  échelle de Likert  \\\n",
              "0  Oui, j’ai toujours pu joindre un professionnel...                  2   \n",
              "1  Non, j'ai souvent eu du mal à joindre un profe...                  0   \n",
              "2  Parfois, c'était facile, mais il y avait aussi...                  1   \n",
              "3  Oui, je n'ai jamais eu de problème pour contac...                  2   \n",
              "4  Non, j’ai souvent dû attendre longtemps avant ...                  0   \n",
              "\n",
              "  modalités de réponse     label predict_label  \n",
              "0                0,1,2  positive          None  \n",
              "1                0,1,2  negative          None  \n",
              "2                0,1,2   neutral          None  \n",
              "3                0,1,2  positive          None  \n",
              "4                0,1,2  negative          None  "
            ]
          },
          "execution_count": 56,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "from transformers import T5Tokenizer, T5ForConditionalGeneration, Seq2SeqTrainer, Seq2SeqTrainingArguments\n",
        "from transformers import EarlyStoppingCallback\n",
        "from datasets import Dataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "import torch\n",
        "import logging\n",
        "\n",
        "# -----------------------------------\n",
        "# 1. Configuration and Setup\n",
        "# -----------------------------------\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "log_dir = os.getcwd()\n",
        "predicted_results_dir = os.path.join(log_dir, \"Outputs/Predicted_Results\")\n",
        "os.makedirs(predicted_results_dir, exist_ok=True)\n",
        "\n",
        "# Logging Setup\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# -----------------------------------\n",
        "# 2. Load T5 Model and Tokenizer\n",
        "# -----------------------------------\n",
        "tokenizer = T5Tokenizer.from_pretrained(\"t5-small\")\n",
        "t5_model = T5ForConditionalGeneration.from_pretrained(\"t5-small\").to(device)\n",
        "\n",
        "# -----------------------------------\n",
        "# 3. Mapping Likert Values to Labels\n",
        "# -----------------------------------\n",
        "def map_likert_to_labels(row):\n",
        "    modalites_de_reponse = row['modalités de réponse']\n",
        "    likert_value = row['échelle de Likert']\n",
        "\n",
        "    if modalites_de_reponse == \"0,1,2\":\n",
        "        return [\"negative\", \"neutral\", \"positive\"][likert_value]\n",
        "    elif modalites_de_reponse == \"0,1,2,3,4\":\n",
        "        return [\"strong negative\", \"negative\", \"neutral\", \"positive\", \"strong positive\"][likert_value]\n",
        "\n",
        "# Load the dataset (item_bank)\n",
        "df = item_bank.head(100)[['libellé', 'réponse', 'échelle de Likert', 'modalités de réponse']]\n",
        "\n",
        "# Apply the Likert value-to-label mapping\n",
        "df['label'] = df.apply(map_likert_to_labels, axis=1)\n",
        "\n",
        "# Split the data into training and test sets\n",
        "train_df, test_df = train_test_split(df, test_size=0.4, shuffle=True, random_state=42)\n",
        "\n",
        "# Convert the data into Hugging Face Dataset format\n",
        "train_dataset = Dataset.from_pandas(train_df)\n",
        "test_dataset = Dataset.from_pandas(test_df)\n",
        "\n",
        "# -----------------------------------\n",
        "# 4. Fine-tuning T5 Model with Explicit Constraints\n",
        "# -----------------------------------\n",
        "def tokenize_function(examples):\n",
        "    inputs = [\"Question: \" + q + \" Response: \" + r for q, r in zip(examples['libellé'], examples['réponse'])]\n",
        "    outputs = [str(label) for label in examples['label']]\n",
        "    model_inputs = tokenizer(inputs, max_length=128, truncation=True, padding=\"max_length\")\n",
        "    labels = tokenizer(outputs, max_length=2, truncation=True, padding=\"max_length\").input_ids\n",
        "    model_inputs['labels'] = labels\n",
        "    return model_inputs\n",
        "\n",
        "train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
        "test_dataset = test_dataset.map(tokenize_function, batched=True)\n",
        "\n",
        "# Set format for Trainer to understand the labels\n",
        "train_dataset.set_format(type=\"torch\", columns=['input_ids', 'attention_mask', 'labels'])\n",
        "test_dataset.set_format(type=\"torch\", columns=['input_ids', 'attention_mask', 'labels'])\n",
        "\n",
        "# Fine-tuning T5 with Early Stopping and Learning Rate Scheduler\n",
        "training_args = Seq2SeqTrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=16,\n",
        "    num_train_epochs=5,\n",
        "    weight_decay=0.01,\n",
        "    learning_rate=5e-5,\n",
        "    lr_scheduler_type=\"linear\",\n",
        "    load_best_model_at_end=True,\n",
        "    save_total_limit=2,\n",
        "    save_strategy=\"epoch\",\n",
        "    metric_for_best_model=\"eval_loss\"\n",
        ")\n",
        "\n",
        "trainer = Seq2SeqTrainer(\n",
        "    model=t5_model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=test_dataset,\n",
        "    tokenizer=tokenizer,\n",
        "    callbacks=[EarlyStoppingCallback(early_stopping_patience=2)]\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "trainer.train()\n",
        "\n",
        "# Save the fine-tuned model\n",
        "t5_model.save_pretrained(\"./fine_tuned_t5_likert\")\n",
        "tokenizer.save_pretrained(\"./fine_tuned_t5_likert\")\n",
        "\n",
        "# -----------------------------------\n",
        "# 5. Predicting Sentiment with Constrained Labels Using T5\n",
        "# -----------------------------------\n",
        "from difflib import get_close_matches\n",
        "def generate_sentiment(input_text, modalites_de_reponse):\n",
        "    if modalites_de_reponse == \"0,1,2\":\n",
        "        possible_labels = [\"negative\", \"neutral\", \"positive\"]\n",
        "    elif modalites_de_reponse == \"0,1,2,3,4\":\n",
        "        possible_labels = [\"strong negative\", \"negative\", \"neutral\", \"positive\", \"strong positive\"]\n",
        "\n",
        "    # Create the prompt\n",
        "    prompt = f\"Given the response: '{input_text}', predict the sentiment from {possible_labels}.\"\n",
        "\n",
        "    # Tokenize the input\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
        "\n",
        "    # Generate the output (sentiment label)\n",
        "    outputs = t5_model.generate(**inputs, max_length=10, num_beams=5, early_stopping=True)\n",
        "\n",
        "    # Decode the output and extract the generated sentiment\n",
        "    generated = tokenizer.decode(outputs[0], skip_special_tokens=True).strip().lower()\n",
        "\n",
        "    # Try to match the generated sentiment with one of the possible labels (using close matching)\n",
        "    matched_label = get_close_matches(generated, possible_labels, n=1, cutoff=0.6)\n",
        "\n",
        "    # Return the best match or None if no close match is found\n",
        "    return matched_label[0] if matched_label else None\n",
        "\n",
        "# Apply T5 sentiment prediction to each row and create a new column 'predict_label'\n",
        "df['predict_label'] = df.apply(lambda row: generate_sentiment(row['réponse'], row['modalités de réponse']), axis=1)\n",
        "\n",
        "# -----------------------------------\n",
        "# 6. Save the Results\n",
        "# -----------------------------------\n",
        "output_file = os.path.join(predicted_results_dir, \"T5_predicted_sentiments.xlsx\")\n",
        "df.to_excel(output_file, index=False)\n",
        "logger.info(f\"Results saved to '{output_file}'\")\n",
        "\n",
        "# Display a sample of the results\n",
        "df[['réponse', 'échelle de Likert', 'modalités de réponse', 'label', 'predict_label']].head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9t5_iPghQKvT",
        "outputId": "026ddb31-2ae5-4ce6-f124-7c986428b526",
        "colab": {
          "referenced_widgets": [
            "4a8862803f744fd4b8b9ee174e8fed54",
            "35e5e9cfe3ef493ea49936cb9a73a662",
            "9f838ce4452b4f229d6b3feb79f1ec00",
            "0b92b5aea7074041a7adb96d8ef3336f",
            "97b215f66f7c4fa6b416268a3249f9e4",
            "ad9502dfe4b749fcb9151a72cd29b92e",
            "8d89aff2b0064098b3dda8396ee446c6"
          ]
        }
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4a8862803f744fd4b8b9ee174e8fed54",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/1.40k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "35e5e9cfe3ef493ea49936cb9a73a662",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/990M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9f838ce4452b4f229d6b3feb79f1ec00",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0b92b5aea7074041a7adb96d8ef3336f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/2.54k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "97b215f66f7c4fa6b416268a3249f9e4",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ad9502dfe4b749fcb9151a72cd29b92e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/2.20k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8d89aff2b0064098b3dda8396ee446c6",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/2.42M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<pad> Le temps est bon aujourd'hui.</s>\n"
          ]
        }
      ],
      "source": [
        "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
        "\n",
        "# Load the Flan-T5 model\n",
        "model = T5ForConditionalGeneration.from_pretrained('google/flan-t5-base')\n",
        "tokenizer = T5Tokenizer.from_pretrained('google/flan-t5-base')\n",
        "\n",
        "# Input text\n",
        "input_text = \"Translate English to French: The weather is nice today.\"\n",
        "\n",
        "# Tokenize input\n",
        "inputs = tokenizer(input_text, return_tensors='pt')\n",
        "\n",
        "# Generate output\n",
        "outputs = model.generate(inputs['input_ids'], max_length=40, num_beams=4, early_stopping=True)\n",
        "\n",
        "# Decode and print the result\n",
        "print(tokenizer.decode(outputs[0]))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q90S76R9QKvT",
        "outputId": "0d8c941f-1242-4c06-b995-bfbac3cbb3ac"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<pad> positive</s>\n"
          ]
        }
      ],
      "source": [
        "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
        "\n",
        "# Load the Flan-T5 model\n",
        "model = T5ForConditionalGeneration.from_pretrained('google/flan-t5-base')\n",
        "tokenizer = T5Tokenizer.from_pretrained('google/flan-t5-base')\n",
        "\n",
        "# Input text\n",
        "input_text = \"Tell the sentiment: The weather is very nice today.\"\n",
        "\n",
        "# Tokenize input\n",
        "inputs = tokenizer(input_text, return_tensors='pt')\n",
        "\n",
        "# Generate output\n",
        "outputs = model.generate(inputs['input_ids'], max_length=40, num_beams=4, early_stopping=True)\n",
        "\n",
        "# Decode and print the result\n",
        "print(tokenizer.decode(outputs[0]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tWQgNaoIQKvU",
        "outputId": "f754eeb6-ce09-4cbb-c827-32481cf46fa8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<pad> negative</s>\n"
          ]
        }
      ],
      "source": [
        "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
        "\n",
        "# Load the Flan-T5 model\n",
        "model = T5ForConditionalGeneration.from_pretrained('google/flan-t5-base')\n",
        "tokenizer = T5Tokenizer.from_pretrained('google/flan-t5-base')\n",
        "\n",
        "# Input text\n",
        "input_text = \"Tell the sentiment: The weather is very bad today.\"\n",
        "\n",
        "# Tokenize input\n",
        "inputs = tokenizer(input_text, return_tensors='pt')\n",
        "\n",
        "# Generate output\n",
        "outputs = model.generate(inputs['input_ids'], max_length=40, num_beams=4, early_stopping=True)\n",
        "\n",
        "# Decode and print the result\n",
        "print(tokenizer.decode(outputs[0]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RZuE4osPQKvd",
        "outputId": "4492286f-8461-4e92-8ad4-808364694c9c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<pad> negative</s>\n"
          ]
        }
      ],
      "source": [
        "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
        "\n",
        "# Load the Flan-T5 model\n",
        "model = T5ForConditionalGeneration.from_pretrained('google/flan-t5-base')\n",
        "tokenizer = T5Tokenizer.from_pretrained('google/flan-t5-base')\n",
        "\n",
        "# Input text\n",
        "input_text = \"Classify the sentiment as one of these: very negative, negative, neutral, positive, very positive. The shirt is too small.\"\n",
        "\n",
        "# Tokenize input\n",
        "inputs = tokenizer(input_text, return_tensors='pt')\n",
        "\n",
        "# Generate output\n",
        "outputs = model.generate(inputs['input_ids'], max_length=40, num_beams=4, early_stopping=True)\n",
        "\n",
        "# Decode and print the result\n",
        "print(tokenizer.decode(outputs[0]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lWiuiFJPQKvd",
        "outputId": "6dd553db-9d9d-48fb-8b45-cda566e39a63"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Numéro patient</th>\n",
              "      <th>item</th>\n",
              "      <th>modalités de réponse</th>\n",
              "      <th>libellé</th>\n",
              "      <th>réponse</th>\n",
              "      <th>échelle de Likert</th>\n",
              "      <th>combined_qr</th>\n",
              "      <th>predict_sentiment_réponse</th>\n",
              "      <th>predict_sentiment_combined</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>ACC1</td>\n",
              "      <td>0,1,2</td>\n",
              "      <td>Avez-vous trouvé que vous avez pu facilement c...</td>\n",
              "      <td>Oui, j’ai toujours pu joindre un professionnel...</td>\n",
              "      <td>2</td>\n",
              "      <td>Avez-vous trouvé que vous avez pu facilement c...</td>\n",
              "      <td>positive</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>ACC1</td>\n",
              "      <td>0,1,2</td>\n",
              "      <td>Avez-vous trouvé que vous avez pu facilement c...</td>\n",
              "      <td>Non, j'ai souvent eu du mal à joindre un profe...</td>\n",
              "      <td>0</td>\n",
              "      <td>Avez-vous trouvé que vous avez pu facilement c...</td>\n",
              "      <td>negative</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>ACC1</td>\n",
              "      <td>0,1,2</td>\n",
              "      <td>Avez-vous trouvé que vous avez pu facilement c...</td>\n",
              "      <td>Parfois, c'était facile, mais il y avait aussi...</td>\n",
              "      <td>1</td>\n",
              "      <td>Avez-vous trouvé que vous avez pu facilement c...</td>\n",
              "      <td>negative</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>ACC1</td>\n",
              "      <td>0,1,2</td>\n",
              "      <td>Avez-vous trouvé que vous avez pu facilement c...</td>\n",
              "      <td>Oui, je n'ai jamais eu de problème pour contac...</td>\n",
              "      <td>2</td>\n",
              "      <td>Avez-vous trouvé que vous avez pu facilement c...</td>\n",
              "      <td>positive</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>ACC1</td>\n",
              "      <td>0,1,2</td>\n",
              "      <td>Avez-vous trouvé que vous avez pu facilement c...</td>\n",
              "      <td>Non, j’ai souvent dû attendre longtemps avant ...</td>\n",
              "      <td>0</td>\n",
              "      <td>Avez-vous trouvé que vous avez pu facilement c...</td>\n",
              "      <td>negative</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>95</th>\n",
              "      <td>46</td>\n",
              "      <td>ACC4</td>\n",
              "      <td>0,1,2</td>\n",
              "      <td>Avez-vous trouvé que vous avez pu facilement f...</td>\n",
              "      <td>Oui, chaque fois que j’avais un imprévu, j’ai ...</td>\n",
              "      <td>2</td>\n",
              "      <td>Avez-vous trouvé que vous avez pu facilement f...</td>\n",
              "      <td>positive</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>96</th>\n",
              "      <td>47</td>\n",
              "      <td>ACC4</td>\n",
              "      <td>0,1,2</td>\n",
              "      <td>Avez-vous trouvé que vous avez pu facilement f...</td>\n",
              "      <td>Non, c’était souvent difficile de reporter un ...</td>\n",
              "      <td>0</td>\n",
              "      <td>Avez-vous trouvé que vous avez pu facilement f...</td>\n",
              "      <td>negative</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>97</th>\n",
              "      <td>48</td>\n",
              "      <td>ACC4</td>\n",
              "      <td>0,1,2</td>\n",
              "      <td>Avez-vous trouvé que vous avez pu facilement f...</td>\n",
              "      <td>Globalement, c’était facile de faire déplacer ...</td>\n",
              "      <td>1</td>\n",
              "      <td>Avez-vous trouvé que vous avez pu facilement f...</td>\n",
              "      <td>negative</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>98</th>\n",
              "      <td>49</td>\n",
              "      <td>ACC4</td>\n",
              "      <td>0,1,2</td>\n",
              "      <td>Avez-vous trouvé que vous avez pu facilement f...</td>\n",
              "      <td>Oui, j’ai toujours pu faire reporter mes rende...</td>\n",
              "      <td>2</td>\n",
              "      <td>Avez-vous trouvé que vous avez pu facilement f...</td>\n",
              "      <td>positive</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>99</th>\n",
              "      <td>50</td>\n",
              "      <td>ACC4</td>\n",
              "      <td>0,1,2</td>\n",
              "      <td>Avez-vous trouvé que vous avez pu facilement f...</td>\n",
              "      <td>Non, il m’a souvent été difficile de faire cha...</td>\n",
              "      <td>0</td>\n",
              "      <td>Avez-vous trouvé que vous avez pu facilement f...</td>\n",
              "      <td>negative</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>100 rows × 9 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "    Numéro patient  item modalités de réponse  \\\n",
              "0                1  ACC1                0,1,2   \n",
              "1                2  ACC1                0,1,2   \n",
              "2                3  ACC1                0,1,2   \n",
              "3                4  ACC1                0,1,2   \n",
              "4                5  ACC1                0,1,2   \n",
              "..             ...   ...                  ...   \n",
              "95              46  ACC4                0,1,2   \n",
              "96              47  ACC4                0,1,2   \n",
              "97              48  ACC4                0,1,2   \n",
              "98              49  ACC4                0,1,2   \n",
              "99              50  ACC4                0,1,2   \n",
              "\n",
              "                                              libellé  \\\n",
              "0   Avez-vous trouvé que vous avez pu facilement c...   \n",
              "1   Avez-vous trouvé que vous avez pu facilement c...   \n",
              "2   Avez-vous trouvé que vous avez pu facilement c...   \n",
              "3   Avez-vous trouvé que vous avez pu facilement c...   \n",
              "4   Avez-vous trouvé que vous avez pu facilement c...   \n",
              "..                                                ...   \n",
              "95  Avez-vous trouvé que vous avez pu facilement f...   \n",
              "96  Avez-vous trouvé que vous avez pu facilement f...   \n",
              "97  Avez-vous trouvé que vous avez pu facilement f...   \n",
              "98  Avez-vous trouvé que vous avez pu facilement f...   \n",
              "99  Avez-vous trouvé que vous avez pu facilement f...   \n",
              "\n",
              "                                              réponse  échelle de Likert  \\\n",
              "0   Oui, j’ai toujours pu joindre un professionnel...                  2   \n",
              "1   Non, j'ai souvent eu du mal à joindre un profe...                  0   \n",
              "2   Parfois, c'était facile, mais il y avait aussi...                  1   \n",
              "3   Oui, je n'ai jamais eu de problème pour contac...                  2   \n",
              "4   Non, j’ai souvent dû attendre longtemps avant ...                  0   \n",
              "..                                                ...                ...   \n",
              "95  Oui, chaque fois que j’avais un imprévu, j’ai ...                  2   \n",
              "96  Non, c’était souvent difficile de reporter un ...                  0   \n",
              "97  Globalement, c’était facile de faire déplacer ...                  1   \n",
              "98  Oui, j’ai toujours pu faire reporter mes rende...                  2   \n",
              "99  Non, il m’a souvent été difficile de faire cha...                  0   \n",
              "\n",
              "                                          combined_qr  \\\n",
              "0   Avez-vous trouvé que vous avez pu facilement c...   \n",
              "1   Avez-vous trouvé que vous avez pu facilement c...   \n",
              "2   Avez-vous trouvé que vous avez pu facilement c...   \n",
              "3   Avez-vous trouvé que vous avez pu facilement c...   \n",
              "4   Avez-vous trouvé que vous avez pu facilement c...   \n",
              "..                                                ...   \n",
              "95  Avez-vous trouvé que vous avez pu facilement f...   \n",
              "96  Avez-vous trouvé que vous avez pu facilement f...   \n",
              "97  Avez-vous trouvé que vous avez pu facilement f...   \n",
              "98  Avez-vous trouvé que vous avez pu facilement f...   \n",
              "99  Avez-vous trouvé que vous avez pu facilement f...   \n",
              "\n",
              "   predict_sentiment_réponse predict_sentiment_combined  \n",
              "0                   positive                   positive  \n",
              "1                   negative                   negative  \n",
              "2                   negative                   negative  \n",
              "3                   positive                   positive  \n",
              "4                   negative                   negative  \n",
              "..                       ...                        ...  \n",
              "95                  positive                   positive  \n",
              "96                  negative                   negative  \n",
              "97                  negative                   positive  \n",
              "98                  positive                   positive  \n",
              "99                  negative                   negative  \n",
              "\n",
              "[100 rows x 9 columns]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
        "df=item_bank.copy()\n",
        "df=df.head(100)\n",
        "df['combined_qr'] = df['libellé'] + ' ' + df['réponse'].astype(str)\n",
        "\n",
        "\n",
        "# Load the Flan-T5 model and tokenizer\n",
        "model = T5ForConditionalGeneration.from_pretrained('google/flan-t5-base')\n",
        "tokenizer = T5Tokenizer.from_pretrained('google/flan-t5-base')\n",
        "\n",
        "def predict_sentiment(text):\n",
        "    # Create the input prompt for the model\n",
        "    input_text = f\"Tell the sentiment: {text}\"\n",
        "\n",
        "    # Tokenize the input text\n",
        "    inputs = tokenizer(input_text, return_tensors='pt')\n",
        "\n",
        "    # Generate the prediction from the model\n",
        "    outputs = model.generate(inputs['input_ids'], max_length=40, num_beams=4, early_stopping=True)\n",
        "\n",
        "    # Decode the predicted token and return the result\n",
        "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "# Apply the sentiment prediction to the \"réponse\" column and store the result in a new column \"predict_sentiment\"\n",
        "df['predict_sentiment_réponse'] = df['réponse'].apply(predict_sentiment)\n",
        "# Apply the sentiment prediction to the \"réponse\" column and store the result in a new column \"predict_sentiment\"\n",
        "df['predict_sentiment_combined'] = df['combined_qr'].apply(predict_sentiment)\n",
        "\n",
        "display(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-8weZgQvQKvd",
        "outputId": "0da1d5cd-5dac-4e09-8811-e1b08722bbce"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Numéro patient</th>\n",
              "      <th>item</th>\n",
              "      <th>modalités de réponse</th>\n",
              "      <th>libellé</th>\n",
              "      <th>réponse</th>\n",
              "      <th>échelle de Likert</th>\n",
              "      <th>combined_qr</th>\n",
              "      <th>predict_sentiment_réponse</th>\n",
              "      <th>predict_sentiment_combined</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>ACC1</td>\n",
              "      <td>0,1,2</td>\n",
              "      <td>Avez-vous trouvé que vous avez pu facilement c...</td>\n",
              "      <td>Oui, j’ai toujours pu joindre un professionnel...</td>\n",
              "      <td>2</td>\n",
              "      <td>Avez-vous trouvé que vous avez pu facilement c...</td>\n",
              "      <td>positive</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>ACC1</td>\n",
              "      <td>0,1,2</td>\n",
              "      <td>Avez-vous trouvé que vous avez pu facilement c...</td>\n",
              "      <td>Non, j'ai souvent eu du mal à joindre un profe...</td>\n",
              "      <td>0</td>\n",
              "      <td>Avez-vous trouvé que vous avez pu facilement c...</td>\n",
              "      <td>negative</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>ACC1</td>\n",
              "      <td>0,1,2</td>\n",
              "      <td>Avez-vous trouvé que vous avez pu facilement c...</td>\n",
              "      <td>Parfois, c'était facile, mais il y avait aussi...</td>\n",
              "      <td>1</td>\n",
              "      <td>Avez-vous trouvé que vous avez pu facilement c...</td>\n",
              "      <td>negative</td>\n",
              "      <td>very positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>ACC1</td>\n",
              "      <td>0,1,2</td>\n",
              "      <td>Avez-vous trouvé que vous avez pu facilement c...</td>\n",
              "      <td>Oui, je n'ai jamais eu de problème pour contac...</td>\n",
              "      <td>2</td>\n",
              "      <td>Avez-vous trouvé que vous avez pu facilement c...</td>\n",
              "      <td>positive</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>ACC1</td>\n",
              "      <td>0,1,2</td>\n",
              "      <td>Avez-vous trouvé que vous avez pu facilement c...</td>\n",
              "      <td>Non, j’ai souvent dû attendre longtemps avant ...</td>\n",
              "      <td>0</td>\n",
              "      <td>Avez-vous trouvé que vous avez pu facilement c...</td>\n",
              "      <td>negative</td>\n",
              "      <td>very negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>95</th>\n",
              "      <td>46</td>\n",
              "      <td>ACC4</td>\n",
              "      <td>0,1,2</td>\n",
              "      <td>Avez-vous trouvé que vous avez pu facilement f...</td>\n",
              "      <td>Oui, chaque fois que j’avais un imprévu, j’ai ...</td>\n",
              "      <td>2</td>\n",
              "      <td>Avez-vous trouvé que vous avez pu facilement f...</td>\n",
              "      <td>positive</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>96</th>\n",
              "      <td>47</td>\n",
              "      <td>ACC4</td>\n",
              "      <td>0,1,2</td>\n",
              "      <td>Avez-vous trouvé que vous avez pu facilement f...</td>\n",
              "      <td>Non, c’était souvent difficile de reporter un ...</td>\n",
              "      <td>0</td>\n",
              "      <td>Avez-vous trouvé que vous avez pu facilement f...</td>\n",
              "      <td>negative</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>97</th>\n",
              "      <td>48</td>\n",
              "      <td>ACC4</td>\n",
              "      <td>0,1,2</td>\n",
              "      <td>Avez-vous trouvé que vous avez pu facilement f...</td>\n",
              "      <td>Globalement, c’était facile de faire déplacer ...</td>\n",
              "      <td>1</td>\n",
              "      <td>Avez-vous trouvé que vous avez pu facilement f...</td>\n",
              "      <td>negative</td>\n",
              "      <td>very positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>98</th>\n",
              "      <td>49</td>\n",
              "      <td>ACC4</td>\n",
              "      <td>0,1,2</td>\n",
              "      <td>Avez-vous trouvé que vous avez pu facilement f...</td>\n",
              "      <td>Oui, j’ai toujours pu faire reporter mes rende...</td>\n",
              "      <td>2</td>\n",
              "      <td>Avez-vous trouvé que vous avez pu facilement f...</td>\n",
              "      <td>positive</td>\n",
              "      <td>very positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>99</th>\n",
              "      <td>50</td>\n",
              "      <td>ACC4</td>\n",
              "      <td>0,1,2</td>\n",
              "      <td>Avez-vous trouvé que vous avez pu facilement f...</td>\n",
              "      <td>Non, il m’a souvent été difficile de faire cha...</td>\n",
              "      <td>0</td>\n",
              "      <td>Avez-vous trouvé que vous avez pu facilement f...</td>\n",
              "      <td>negative</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>100 rows × 9 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "    Numéro patient  item modalités de réponse  \\\n",
              "0                1  ACC1                0,1,2   \n",
              "1                2  ACC1                0,1,2   \n",
              "2                3  ACC1                0,1,2   \n",
              "3                4  ACC1                0,1,2   \n",
              "4                5  ACC1                0,1,2   \n",
              "..             ...   ...                  ...   \n",
              "95              46  ACC4                0,1,2   \n",
              "96              47  ACC4                0,1,2   \n",
              "97              48  ACC4                0,1,2   \n",
              "98              49  ACC4                0,1,2   \n",
              "99              50  ACC4                0,1,2   \n",
              "\n",
              "                                              libellé  \\\n",
              "0   Avez-vous trouvé que vous avez pu facilement c...   \n",
              "1   Avez-vous trouvé que vous avez pu facilement c...   \n",
              "2   Avez-vous trouvé que vous avez pu facilement c...   \n",
              "3   Avez-vous trouvé que vous avez pu facilement c...   \n",
              "4   Avez-vous trouvé que vous avez pu facilement c...   \n",
              "..                                                ...   \n",
              "95  Avez-vous trouvé que vous avez pu facilement f...   \n",
              "96  Avez-vous trouvé que vous avez pu facilement f...   \n",
              "97  Avez-vous trouvé que vous avez pu facilement f...   \n",
              "98  Avez-vous trouvé que vous avez pu facilement f...   \n",
              "99  Avez-vous trouvé que vous avez pu facilement f...   \n",
              "\n",
              "                                              réponse  échelle de Likert  \\\n",
              "0   Oui, j’ai toujours pu joindre un professionnel...                  2   \n",
              "1   Non, j'ai souvent eu du mal à joindre un profe...                  0   \n",
              "2   Parfois, c'était facile, mais il y avait aussi...                  1   \n",
              "3   Oui, je n'ai jamais eu de problème pour contac...                  2   \n",
              "4   Non, j’ai souvent dû attendre longtemps avant ...                  0   \n",
              "..                                                ...                ...   \n",
              "95  Oui, chaque fois que j’avais un imprévu, j’ai ...                  2   \n",
              "96  Non, c’était souvent difficile de reporter un ...                  0   \n",
              "97  Globalement, c’était facile de faire déplacer ...                  1   \n",
              "98  Oui, j’ai toujours pu faire reporter mes rende...                  2   \n",
              "99  Non, il m’a souvent été difficile de faire cha...                  0   \n",
              "\n",
              "                                          combined_qr  \\\n",
              "0   Avez-vous trouvé que vous avez pu facilement c...   \n",
              "1   Avez-vous trouvé que vous avez pu facilement c...   \n",
              "2   Avez-vous trouvé que vous avez pu facilement c...   \n",
              "3   Avez-vous trouvé que vous avez pu facilement c...   \n",
              "4   Avez-vous trouvé que vous avez pu facilement c...   \n",
              "..                                                ...   \n",
              "95  Avez-vous trouvé que vous avez pu facilement f...   \n",
              "96  Avez-vous trouvé que vous avez pu facilement f...   \n",
              "97  Avez-vous trouvé que vous avez pu facilement f...   \n",
              "98  Avez-vous trouvé que vous avez pu facilement f...   \n",
              "99  Avez-vous trouvé que vous avez pu facilement f...   \n",
              "\n",
              "   predict_sentiment_réponse predict_sentiment_combined  \n",
              "0                   positive                   positive  \n",
              "1                   negative                   negative  \n",
              "2                   negative              very positive  \n",
              "3                   positive                   positive  \n",
              "4                   negative              very negative  \n",
              "..                       ...                        ...  \n",
              "95                  positive                   positive  \n",
              "96                  negative                   negative  \n",
              "97                  negative              very positive  \n",
              "98                  positive              very positive  \n",
              "99                  negative                   negative  \n",
              "\n",
              "[100 rows x 9 columns]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
        "df=item_bank.copy()\n",
        "df=df.head(100)\n",
        "df['combined_qr'] = df['libellé'] + ' ' + df['réponse'].astype(str)\n",
        "\n",
        "\n",
        "# Load the Flan-T5 model and tokenizer\n",
        "model = T5ForConditionalGeneration.from_pretrained('google/flan-t5-base')\n",
        "tokenizer = T5Tokenizer.from_pretrained('google/flan-t5-base')\n",
        "\n",
        "def predict_sentiment(text):\n",
        "    # Create the input prompt for the model\n",
        "    input_text = f\"Classify the sentiment as one of these: very negative, negative, neutral, positive, very positive: {text}\"\n",
        "\n",
        "    # Tokenize the input text\n",
        "    inputs = tokenizer(input_text, return_tensors='pt')\n",
        "\n",
        "    # Generate the prediction from the model\n",
        "    outputs = model.generate(inputs['input_ids'], max_length=40, num_beams=4, early_stopping=True)\n",
        "\n",
        "    # Decode the predicted token and return the result\n",
        "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "# Apply the sentiment prediction to the \"réponse\" column and store the result in a new column \"predict_sentiment\"\n",
        "df['predict_sentiment_réponse'] = df['réponse'].apply(predict_sentiment)\n",
        "# Apply the sentiment prediction to the \"réponse\" column and store the result in a new column \"predict_sentiment\"\n",
        "df['predict_sentiment_combined'] = df['combined_qr'].apply(predict_sentiment)\n",
        "display(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L4G8UekdQKve",
        "outputId": "9f4320d2-f2d2-443b-ae21-00eb5e3b0aad"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Numéro patient</th>\n",
              "      <th>item</th>\n",
              "      <th>modalités de réponse</th>\n",
              "      <th>libellé</th>\n",
              "      <th>réponse</th>\n",
              "      <th>échelle de Likert</th>\n",
              "      <th>combined_qr</th>\n",
              "      <th>label</th>\n",
              "      <th>predict_sentiment_réponse</th>\n",
              "      <th>predict_sentiment_combined</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>ACC1</td>\n",
              "      <td>0,1,2</td>\n",
              "      <td>Avez-vous trouvé que vous avez pu facilement c...</td>\n",
              "      <td>Oui, j’ai toujours pu joindre un professionnel...</td>\n",
              "      <td>2</td>\n",
              "      <td>Avez-vous trouvé que vous avez pu facilement c...</td>\n",
              "      <td>positive</td>\n",
              "      <td>positive</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>ACC1</td>\n",
              "      <td>0,1,2</td>\n",
              "      <td>Avez-vous trouvé que vous avez pu facilement c...</td>\n",
              "      <td>Non, j'ai souvent eu du mal à joindre un profe...</td>\n",
              "      <td>0</td>\n",
              "      <td>Avez-vous trouvé que vous avez pu facilement c...</td>\n",
              "      <td>negative</td>\n",
              "      <td>negative</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>ACC1</td>\n",
              "      <td>0,1,2</td>\n",
              "      <td>Avez-vous trouvé que vous avez pu facilement c...</td>\n",
              "      <td>Parfois, c'était facile, mais il y avait aussi...</td>\n",
              "      <td>1</td>\n",
              "      <td>Avez-vous trouvé que vous avez pu facilement c...</td>\n",
              "      <td>neutral</td>\n",
              "      <td>negative</td>\n",
              "      <td>very positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>ACC1</td>\n",
              "      <td>0,1,2</td>\n",
              "      <td>Avez-vous trouvé que vous avez pu facilement c...</td>\n",
              "      <td>Oui, je n'ai jamais eu de problème pour contac...</td>\n",
              "      <td>2</td>\n",
              "      <td>Avez-vous trouvé que vous avez pu facilement c...</td>\n",
              "      <td>positive</td>\n",
              "      <td>positive</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>ACC1</td>\n",
              "      <td>0,1,2</td>\n",
              "      <td>Avez-vous trouvé que vous avez pu facilement c...</td>\n",
              "      <td>Non, j’ai souvent dû attendre longtemps avant ...</td>\n",
              "      <td>0</td>\n",
              "      <td>Avez-vous trouvé que vous avez pu facilement c...</td>\n",
              "      <td>negative</td>\n",
              "      <td>negative</td>\n",
              "      <td>very negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>95</th>\n",
              "      <td>46</td>\n",
              "      <td>ACC4</td>\n",
              "      <td>0,1,2</td>\n",
              "      <td>Avez-vous trouvé que vous avez pu facilement f...</td>\n",
              "      <td>Oui, chaque fois que j’avais un imprévu, j’ai ...</td>\n",
              "      <td>2</td>\n",
              "      <td>Avez-vous trouvé que vous avez pu facilement f...</td>\n",
              "      <td>positive</td>\n",
              "      <td>positive</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>96</th>\n",
              "      <td>47</td>\n",
              "      <td>ACC4</td>\n",
              "      <td>0,1,2</td>\n",
              "      <td>Avez-vous trouvé que vous avez pu facilement f...</td>\n",
              "      <td>Non, c’était souvent difficile de reporter un ...</td>\n",
              "      <td>0</td>\n",
              "      <td>Avez-vous trouvé que vous avez pu facilement f...</td>\n",
              "      <td>negative</td>\n",
              "      <td>negative</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>97</th>\n",
              "      <td>48</td>\n",
              "      <td>ACC4</td>\n",
              "      <td>0,1,2</td>\n",
              "      <td>Avez-vous trouvé que vous avez pu facilement f...</td>\n",
              "      <td>Globalement, c’était facile de faire déplacer ...</td>\n",
              "      <td>1</td>\n",
              "      <td>Avez-vous trouvé que vous avez pu facilement f...</td>\n",
              "      <td>neutral</td>\n",
              "      <td>negative</td>\n",
              "      <td>very positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>98</th>\n",
              "      <td>49</td>\n",
              "      <td>ACC4</td>\n",
              "      <td>0,1,2</td>\n",
              "      <td>Avez-vous trouvé que vous avez pu facilement f...</td>\n",
              "      <td>Oui, j’ai toujours pu faire reporter mes rende...</td>\n",
              "      <td>2</td>\n",
              "      <td>Avez-vous trouvé que vous avez pu facilement f...</td>\n",
              "      <td>positive</td>\n",
              "      <td>positive</td>\n",
              "      <td>very positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>99</th>\n",
              "      <td>50</td>\n",
              "      <td>ACC4</td>\n",
              "      <td>0,1,2</td>\n",
              "      <td>Avez-vous trouvé que vous avez pu facilement f...</td>\n",
              "      <td>Non, il m’a souvent été difficile de faire cha...</td>\n",
              "      <td>0</td>\n",
              "      <td>Avez-vous trouvé que vous avez pu facilement f...</td>\n",
              "      <td>negative</td>\n",
              "      <td>negative</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>100 rows × 10 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "    Numéro patient  item modalités de réponse  \\\n",
              "0                1  ACC1                0,1,2   \n",
              "1                2  ACC1                0,1,2   \n",
              "2                3  ACC1                0,1,2   \n",
              "3                4  ACC1                0,1,2   \n",
              "4                5  ACC1                0,1,2   \n",
              "..             ...   ...                  ...   \n",
              "95              46  ACC4                0,1,2   \n",
              "96              47  ACC4                0,1,2   \n",
              "97              48  ACC4                0,1,2   \n",
              "98              49  ACC4                0,1,2   \n",
              "99              50  ACC4                0,1,2   \n",
              "\n",
              "                                              libellé  \\\n",
              "0   Avez-vous trouvé que vous avez pu facilement c...   \n",
              "1   Avez-vous trouvé que vous avez pu facilement c...   \n",
              "2   Avez-vous trouvé que vous avez pu facilement c...   \n",
              "3   Avez-vous trouvé que vous avez pu facilement c...   \n",
              "4   Avez-vous trouvé que vous avez pu facilement c...   \n",
              "..                                                ...   \n",
              "95  Avez-vous trouvé que vous avez pu facilement f...   \n",
              "96  Avez-vous trouvé que vous avez pu facilement f...   \n",
              "97  Avez-vous trouvé que vous avez pu facilement f...   \n",
              "98  Avez-vous trouvé que vous avez pu facilement f...   \n",
              "99  Avez-vous trouvé que vous avez pu facilement f...   \n",
              "\n",
              "                                              réponse  échelle de Likert  \\\n",
              "0   Oui, j’ai toujours pu joindre un professionnel...                  2   \n",
              "1   Non, j'ai souvent eu du mal à joindre un profe...                  0   \n",
              "2   Parfois, c'était facile, mais il y avait aussi...                  1   \n",
              "3   Oui, je n'ai jamais eu de problème pour contac...                  2   \n",
              "4   Non, j’ai souvent dû attendre longtemps avant ...                  0   \n",
              "..                                                ...                ...   \n",
              "95  Oui, chaque fois que j’avais un imprévu, j’ai ...                  2   \n",
              "96  Non, c’était souvent difficile de reporter un ...                  0   \n",
              "97  Globalement, c’était facile de faire déplacer ...                  1   \n",
              "98  Oui, j’ai toujours pu faire reporter mes rende...                  2   \n",
              "99  Non, il m’a souvent été difficile de faire cha...                  0   \n",
              "\n",
              "                                          combined_qr     label  \\\n",
              "0   Avez-vous trouvé que vous avez pu facilement c...  positive   \n",
              "1   Avez-vous trouvé que vous avez pu facilement c...  negative   \n",
              "2   Avez-vous trouvé que vous avez pu facilement c...   neutral   \n",
              "3   Avez-vous trouvé que vous avez pu facilement c...  positive   \n",
              "4   Avez-vous trouvé que vous avez pu facilement c...  negative   \n",
              "..                                                ...       ...   \n",
              "95  Avez-vous trouvé que vous avez pu facilement f...  positive   \n",
              "96  Avez-vous trouvé que vous avez pu facilement f...  negative   \n",
              "97  Avez-vous trouvé que vous avez pu facilement f...   neutral   \n",
              "98  Avez-vous trouvé que vous avez pu facilement f...  positive   \n",
              "99  Avez-vous trouvé que vous avez pu facilement f...  negative   \n",
              "\n",
              "   predict_sentiment_réponse predict_sentiment_combined  \n",
              "0                   positive                   positive  \n",
              "1                   negative                   negative  \n",
              "2                   negative              very positive  \n",
              "3                   positive                   positive  \n",
              "4                   negative              very negative  \n",
              "..                       ...                        ...  \n",
              "95                  positive                   positive  \n",
              "96                  negative                   negative  \n",
              "97                  negative              very positive  \n",
              "98                  positive              very positive  \n",
              "99                  negative                   negative  \n",
              "\n",
              "[100 rows x 10 columns]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "df=item_bank.copy()\n",
        "df=df.head(100)\n",
        "df['combined_qr'] = df['libellé'] + ' ' + df['réponse'].astype(str)\n",
        "\n",
        "# Define a function to map 'modalités de réponse' to the corresponding label\n",
        "def map_to_label(row):\n",
        "    modalites_de_reponse = row['modalités de réponse']\n",
        "    likert_value = row['échelle de Likert']\n",
        "\n",
        "    if modalites_de_reponse == \"0,1,2\":\n",
        "        return [\"negative\", \"neutral\", \"positive\"][likert_value]\n",
        "    elif modalites_de_reponse == \"0,1,2,3,4\":\n",
        "        return [\"strong negative\", \"negative\", \"neutral\", \"positive\", \"strong positive\"][likert_value]\n",
        "\n",
        "# Apply the mapping function to create a new 'label' column\n",
        "df['label'] = df.apply(map_to_label, axis=1)\n",
        "\n",
        "# Load the Flan-T5 model and tokenizer\n",
        "model = T5ForConditionalGeneration.from_pretrained('google/flan-t5-base')\n",
        "tokenizer = T5Tokenizer.from_pretrained('google/flan-t5-base')\n",
        "\n",
        "def predict_sentiment(text):\n",
        "    # Create the input prompt for the model\n",
        "    input_text = f\"Classify the sentiment as one of these: very negative, negative, neutral, positive, very positive: {text}\"\n",
        "\n",
        "    # Tokenize the input text\n",
        "    inputs = tokenizer(input_text, return_tensors='pt')\n",
        "\n",
        "    # Generate the prediction from the model\n",
        "    outputs = model.generate(inputs['input_ids'], max_length=40, num_beams=4, early_stopping=True)\n",
        "\n",
        "    # Decode the predicted token and return the result\n",
        "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "# Apply the sentiment prediction to the \"réponse\" column and store the result in a new column \"predict_sentiment\"\n",
        "df['predict_sentiment_réponse'] = df['réponse'].apply(predict_sentiment)\n",
        "# Apply the sentiment prediction to the \"réponse\" column and store the result in a new column \"predict_sentiment\"\n",
        "df['predict_sentiment_combined'] = df['combined_qr'].apply(predict_sentiment)\n",
        "display(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vslDX8hoQKve",
        "outputId": "e05037d1-f841-4a3a-da72-dece2b4a9c48"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Numéro patient</th>\n",
              "      <th>item</th>\n",
              "      <th>modalités de réponse</th>\n",
              "      <th>libellé</th>\n",
              "      <th>réponse</th>\n",
              "      <th>échelle de Likert</th>\n",
              "      <th>gold_label_str</th>\n",
              "      <th>combined_qr</th>\n",
              "      <th>predict_sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>ACC1</td>\n",
              "      <td>0,1,2</td>\n",
              "      <td>Avez-vous trouvé que vous avez pu facilement c...</td>\n",
              "      <td>Oui, j’ai toujours pu joindre un professionnel...</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>Avez-vous trouvé que vous avez pu facilement c...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>ACC1</td>\n",
              "      <td>0,1,2</td>\n",
              "      <td>Avez-vous trouvé que vous avez pu facilement c...</td>\n",
              "      <td>Non, j'ai souvent eu du mal à joindre un profe...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>Avez-vous trouvé que vous avez pu facilement c...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>ACC1</td>\n",
              "      <td>0,1,2</td>\n",
              "      <td>Avez-vous trouvé que vous avez pu facilement c...</td>\n",
              "      <td>Parfois, c'était facile, mais il y avait aussi...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>Avez-vous trouvé que vous avez pu facilement c...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>ACC1</td>\n",
              "      <td>0,1,2</td>\n",
              "      <td>Avez-vous trouvé que vous avez pu facilement c...</td>\n",
              "      <td>Oui, je n'ai jamais eu de problème pour contac...</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>Avez-vous trouvé que vous avez pu facilement c...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>ACC1</td>\n",
              "      <td>0,1,2</td>\n",
              "      <td>Avez-vous trouvé que vous avez pu facilement c...</td>\n",
              "      <td>Non, j’ai souvent dû attendre longtemps avant ...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>Avez-vous trouvé que vous avez pu facilement c...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>95</th>\n",
              "      <td>46</td>\n",
              "      <td>ACC4</td>\n",
              "      <td>0,1,2</td>\n",
              "      <td>Avez-vous trouvé que vous avez pu facilement f...</td>\n",
              "      <td>Oui, chaque fois que j’avais un imprévu, j’ai ...</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>Avez-vous trouvé que vous avez pu facilement f...</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>96</th>\n",
              "      <td>47</td>\n",
              "      <td>ACC4</td>\n",
              "      <td>0,1,2</td>\n",
              "      <td>Avez-vous trouvé que vous avez pu facilement f...</td>\n",
              "      <td>Non, c’était souvent difficile de reporter un ...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>Avez-vous trouvé que vous avez pu facilement f...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>97</th>\n",
              "      <td>48</td>\n",
              "      <td>ACC4</td>\n",
              "      <td>0,1,2</td>\n",
              "      <td>Avez-vous trouvé que vous avez pu facilement f...</td>\n",
              "      <td>Globalement, c’était facile de faire déplacer ...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>Avez-vous trouvé que vous avez pu facilement f...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>98</th>\n",
              "      <td>49</td>\n",
              "      <td>ACC4</td>\n",
              "      <td>0,1,2</td>\n",
              "      <td>Avez-vous trouvé que vous avez pu facilement f...</td>\n",
              "      <td>Oui, j’ai toujours pu faire reporter mes rende...</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>Avez-vous trouvé que vous avez pu facilement f...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>99</th>\n",
              "      <td>50</td>\n",
              "      <td>ACC4</td>\n",
              "      <td>0,1,2</td>\n",
              "      <td>Avez-vous trouvé que vous avez pu facilement f...</td>\n",
              "      <td>Non, il m’a souvent été difficile de faire cha...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>Avez-vous trouvé que vous avez pu facilement f...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>100 rows × 9 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "    Numéro patient  item modalités de réponse  \\\n",
              "0                1  ACC1                0,1,2   \n",
              "1                2  ACC1                0,1,2   \n",
              "2                3  ACC1                0,1,2   \n",
              "3                4  ACC1                0,1,2   \n",
              "4                5  ACC1                0,1,2   \n",
              "..             ...   ...                  ...   \n",
              "95              46  ACC4                0,1,2   \n",
              "96              47  ACC4                0,1,2   \n",
              "97              48  ACC4                0,1,2   \n",
              "98              49  ACC4                0,1,2   \n",
              "99              50  ACC4                0,1,2   \n",
              "\n",
              "                                              libellé  \\\n",
              "0   Avez-vous trouvé que vous avez pu facilement c...   \n",
              "1   Avez-vous trouvé que vous avez pu facilement c...   \n",
              "2   Avez-vous trouvé que vous avez pu facilement c...   \n",
              "3   Avez-vous trouvé que vous avez pu facilement c...   \n",
              "4   Avez-vous trouvé que vous avez pu facilement c...   \n",
              "..                                                ...   \n",
              "95  Avez-vous trouvé que vous avez pu facilement f...   \n",
              "96  Avez-vous trouvé que vous avez pu facilement f...   \n",
              "97  Avez-vous trouvé que vous avez pu facilement f...   \n",
              "98  Avez-vous trouvé que vous avez pu facilement f...   \n",
              "99  Avez-vous trouvé que vous avez pu facilement f...   \n",
              "\n",
              "                                              réponse  échelle de Likert  \\\n",
              "0   Oui, j’ai toujours pu joindre un professionnel...                  2   \n",
              "1   Non, j'ai souvent eu du mal à joindre un profe...                  0   \n",
              "2   Parfois, c'était facile, mais il y avait aussi...                  1   \n",
              "3   Oui, je n'ai jamais eu de problème pour contac...                  2   \n",
              "4   Non, j’ai souvent dû attendre longtemps avant ...                  0   \n",
              "..                                                ...                ...   \n",
              "95  Oui, chaque fois que j’avais un imprévu, j’ai ...                  2   \n",
              "96  Non, c’était souvent difficile de reporter un ...                  0   \n",
              "97  Globalement, c’était facile de faire déplacer ...                  1   \n",
              "98  Oui, j’ai toujours pu faire reporter mes rende...                  2   \n",
              "99  Non, il m’a souvent été difficile de faire cha...                  0   \n",
              "\n",
              "   gold_label_str                                        combined_qr  \\\n",
              "0               2  Avez-vous trouvé que vous avez pu facilement c...   \n",
              "1               0  Avez-vous trouvé que vous avez pu facilement c...   \n",
              "2               1  Avez-vous trouvé que vous avez pu facilement c...   \n",
              "3               2  Avez-vous trouvé que vous avez pu facilement c...   \n",
              "4               0  Avez-vous trouvé que vous avez pu facilement c...   \n",
              "..            ...                                                ...   \n",
              "95              2  Avez-vous trouvé que vous avez pu facilement f...   \n",
              "96              0  Avez-vous trouvé que vous avez pu facilement f...   \n",
              "97              1  Avez-vous trouvé que vous avez pu facilement f...   \n",
              "98              2  Avez-vous trouvé que vous avez pu facilement f...   \n",
              "99              0  Avez-vous trouvé que vous avez pu facilement f...   \n",
              "\n",
              "   predict_sentiment  \n",
              "0                  0  \n",
              "1                  0  \n",
              "2                  0  \n",
              "3                  0  \n",
              "4                  0  \n",
              "..               ...  \n",
              "95                 4  \n",
              "96                 0  \n",
              "97                 0  \n",
              "98                 0  \n",
              "99                 0  \n",
              "\n",
              "[100 rows x 9 columns]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
        "\n",
        "# Load the Flan-T5 model and tokenizer\n",
        "model = T5ForConditionalGeneration.from_pretrained('google/flan-t5-base')\n",
        "tokenizer = T5Tokenizer.from_pretrained('google/flan-t5-base')\n",
        "\n",
        "df=item_bank.copy()\n",
        "df=df.head(100)\n",
        "df['combined_qr'] = df['libellé'] + ' ' + df['réponse'].astype(str)\n",
        "\n",
        "# Convert integer gold labels (0-4) to string representations (\"0\", \"1\", \"2\", \"3\", \"4\")\n",
        "def map_integer_to_string(label):\n",
        "    return str(label)  # Simply convert integers to strings\n",
        "\n",
        "# Apply the conversion\n",
        "df['gold_label_str'] = df['échelle de Likert'].apply(map_integer_to_string)\n",
        "\n",
        "# Tokenize the input and make predictions using Flan-T5\n",
        "def predict_sentiment(text):\n",
        "    # Create the input prompt for the model\n",
        "    input_text = f\"Classify the sentiment from 0 to 4: {text}\"\n",
        "\n",
        "    # Tokenize the input text\n",
        "    inputs = tokenizer(input_text, return_tensors='pt')\n",
        "\n",
        "    # Generate the prediction from the model\n",
        "    outputs = model.generate(inputs['input_ids'], max_length=40, num_beams=4, early_stopping=True)\n",
        "\n",
        "    # Decode the predicted token and return the result\n",
        "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "# Apply the sentiment prediction to the \"réponse\" column and store the result in a new column \"predict_sentiment\"\n",
        "df['predict_sentiment'] = df['réponse'].apply(predict_sentiment)\n",
        "\n",
        "display(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x4_UckT_QKve",
        "outputId": "b6bebeb5-a197-45a1-e72a-db6b2761aac1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting Trial 1 with hyperparameters: {'learning_rate': 5e-05, 'batch_size': 8}\n",
            "Trial 1 Fold 1 is processing...\n",
            "Validation Accuracy (réponse): 0.375\n",
            "Validation Accuracy (combined): 0.3125\n",
            "Trial 1 Fold 2 is processing...\n",
            "Validation Accuracy (réponse): 0.875\n",
            "Validation Accuracy (combined): 0.8125\n",
            "Trial 1 Fold 3 is processing...\n",
            "Validation Accuracy (réponse): 0.75\n",
            "Validation Accuracy (combined): 0.625\n",
            "Trial 1 Fold 4 is processing...\n",
            "Validation Accuracy (réponse): 0.5625\n",
            "Validation Accuracy (combined): 0.5\n",
            "Trial 1 Fold 5 is processing...\n",
            "Validation Accuracy (réponse): 0.5625\n",
            "Validation Accuracy (combined): 0.5625\n",
            "Average accuracy for Trial 1: 0.59375\n",
            "Starting Trial 2 with hyperparameters: {'learning_rate': 3e-05, 'batch_size': 16}\n",
            "Trial 2 Fold 1 is processing...\n",
            "Validation Accuracy (réponse): 0.375\n",
            "Validation Accuracy (combined): 0.3125\n",
            "Trial 2 Fold 2 is processing...\n",
            "Validation Accuracy (réponse): 0.875\n",
            "Validation Accuracy (combined): 0.8125\n",
            "Trial 2 Fold 3 is processing...\n",
            "Validation Accuracy (réponse): 0.75\n",
            "Validation Accuracy (combined): 0.625\n",
            "Trial 2 Fold 4 is processing...\n",
            "Validation Accuracy (réponse): 0.5625\n",
            "Validation Accuracy (combined): 0.5\n",
            "Trial 2 Fold 5 is processing...\n",
            "Validation Accuracy (réponse): 0.5625\n",
            "Validation Accuracy (combined): 0.5625\n",
            "Average accuracy for Trial 2: 0.59375\n",
            "Starting Trial 3 with hyperparameters: {'learning_rate': 2e-05, 'batch_size': 32}\n",
            "Trial 3 Fold 1 is processing...\n",
            "Validation Accuracy (réponse): 0.375\n",
            "Validation Accuracy (combined): 0.3125\n",
            "Trial 3 Fold 2 is processing...\n",
            "Validation Accuracy (réponse): 0.875\n",
            "Validation Accuracy (combined): 0.8125\n",
            "Trial 3 Fold 3 is processing...\n",
            "Validation Accuracy (réponse): 0.75\n",
            "Validation Accuracy (combined): 0.625\n",
            "Trial 3 Fold 4 is processing...\n",
            "Validation Accuracy (réponse): 0.5625\n",
            "Validation Accuracy (combined): 0.5\n",
            "Trial 3 Fold 5 is processing...\n",
            "Validation Accuracy (réponse): 0.5625\n",
            "Validation Accuracy (combined): 0.5625\n",
            "Average accuracy for Trial 3: 0.59375\n",
            "Starting Trial 4 with hyperparameters: {'learning_rate': 1e-05, 'batch_size': 8}\n",
            "Trial 4 Fold 1 is processing...\n",
            "Validation Accuracy (réponse): 0.375\n",
            "Validation Accuracy (combined): 0.3125\n",
            "Trial 4 Fold 2 is processing...\n",
            "Validation Accuracy (réponse): 0.875\n",
            "Validation Accuracy (combined): 0.8125\n",
            "Trial 4 Fold 3 is processing...\n",
            "Validation Accuracy (réponse): 0.75\n",
            "Validation Accuracy (combined): 0.625\n",
            "Trial 4 Fold 4 is processing...\n",
            "Validation Accuracy (réponse): 0.5625\n",
            "Validation Accuracy (combined): 0.5\n",
            "Trial 4 Fold 5 is processing...\n",
            "Validation Accuracy (réponse): 0.5625\n",
            "Validation Accuracy (combined): 0.5625\n",
            "Average accuracy for Trial 4: 0.59375\n",
            "Starting Trial 5 with hyperparameters: {'learning_rate': 7e-05, 'batch_size': 16}\n",
            "Trial 5 Fold 1 is processing...\n",
            "Validation Accuracy (réponse): 0.375\n",
            "Validation Accuracy (combined): 0.3125\n",
            "Trial 5 Fold 2 is processing...\n",
            "Validation Accuracy (réponse): 0.875\n",
            "Validation Accuracy (combined): 0.8125\n",
            "Trial 5 Fold 3 is processing...\n",
            "Validation Accuracy (réponse): 0.75\n",
            "Validation Accuracy (combined): 0.625\n",
            "Trial 5 Fold 4 is processing...\n",
            "Validation Accuracy (réponse): 0.5625\n",
            "Validation Accuracy (combined): 0.5\n",
            "Trial 5 Fold 5 is processing...\n",
            "Validation Accuracy (réponse): 0.5625\n",
            "Validation Accuracy (combined): 0.5625\n",
            "Average accuracy for Trial 5: 0.59375\n",
            "Test Accuracy (réponse): 0.8\n",
            "Test Accuracy (combined): 0.65\n",
            "Trial accuracies (validation set): [0.59375, 0.59375, 0.59375, 0.59375, 0.59375]\n",
            "Best validation accuracy: 0.59375\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split, KFold\n",
        "from sklearn.metrics import accuracy_score\n",
        "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
        "import torch\n",
        "\n",
        "# Load the Flan-T5 model and tokenizer\n",
        "tokenizer = T5Tokenizer.from_pretrained('google/flan-t5-base')\n",
        "\n",
        "# Function to map 'modalités de réponse' to corresponding label\n",
        "def map_to_label(row):\n",
        "    modalites_de_reponse = row['modalités de réponse']\n",
        "    likert_value = row['échelle de Likert']\n",
        "    if modalites_de_reponse == \"0,1,2\":\n",
        "        return [\"negative\", \"neutral\", \"positive\"][likert_value]\n",
        "    elif modalites_de_reponse == \"0,1,2,3,4\":\n",
        "        return [\"strong negative\", \"negative\", \"neutral\", \"positive\", \"strong positive\"][likert_value]\n",
        "\n",
        "# Function to predict sentiment using the model\n",
        "def predict_sentiment(model, text):\n",
        "    input_text = f\"Classify the sentiment as one of these: very negative, negative, neutral, positive, very positive: {text}\"\n",
        "    inputs = tokenizer(input_text, return_tensors='pt')\n",
        "    outputs = model.generate(inputs['input_ids'], max_length=40, num_beams=4, early_stopping=True)\n",
        "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "# 1. Split the dataset into 80% training-validation and 20% test\n",
        "df = item_bank.copy().head(100)\n",
        "df['combined_qr'] = df['libellé'] + ' ' + df['réponse'].astype(str)\n",
        "df['label'] = df.apply(map_to_label, axis=1)\n",
        "\n",
        "train_val_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define 5 sets of hyperparameters for tuning\n",
        "hyperparameter_trials = [\n",
        "    {'learning_rate': 5e-5, 'batch_size': 8},\n",
        "    {'learning_rate': 3e-5, 'batch_size': 16},\n",
        "    {'learning_rate': 2e-5, 'batch_size': 32},\n",
        "    {'learning_rate': 1e-5, 'batch_size': 8},\n",
        "    {'learning_rate': 7e-5, 'batch_size': 16},\n",
        "]\n",
        "\n",
        "# 2. Set up 5-fold cross-validation on the 80% training-validation set\n",
        "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "best_val_accuracy = 0\n",
        "best_model_state = None  # This will store the state dict of the best model\n",
        "\n",
        "# Store accuracy for each trial\n",
        "trial_accuracies = []\n",
        "\n",
        "# Loop over each hyperparameter trial\n",
        "for trial_num, hyperparams in enumerate(hyperparameter_trials, start=1):\n",
        "    print(f\"Starting Trial {trial_num} with hyperparameters: {hyperparams}\")\n",
        "\n",
        "    trial_fold_accuracies = []\n",
        "\n",
        "    # Initialize a fresh model for each trial\n",
        "    model = T5ForConditionalGeneration.from_pretrained('google/flan-t5-base')\n",
        "\n",
        "    # Loop over each fold in cross-validation\n",
        "    for fold, (train_index, val_index) in enumerate(kf.split(train_val_df), start=1):\n",
        "        print(f\"Trial {trial_num} Fold {fold} is processing...\")\n",
        "\n",
        "        # Get the training and validation sets for this fold\n",
        "        train_df = train_val_df.iloc[train_index]\n",
        "        val_df = train_val_df.iloc[val_index]\n",
        "\n",
        "        # Fine-tuning would be done here (not implemented in this code snippet)\n",
        "        # You can customize fine-tuning with hyperparams['learning_rate'], etc.\n",
        "\n",
        "        # Predict on validation set for 'réponse' and 'combined_qr'\n",
        "        val_preds_réponse = val_df['réponse'].apply(lambda x: predict_sentiment(model, x))\n",
        "        val_preds_combined = val_df['combined_qr'].apply(lambda x: predict_sentiment(model, x))\n",
        "\n",
        "        # Calculate accuracy for both 'réponse' and 'combined_qr'\n",
        "        val_accuracy_réponse = accuracy_score(val_df['label'], val_preds_réponse)\n",
        "        val_accuracy_combined = accuracy_score(val_df['label'], val_preds_combined)\n",
        "\n",
        "        print(f\"Validation Accuracy (réponse): {val_accuracy_réponse}\")\n",
        "        print(f\"Validation Accuracy (combined): {val_accuracy_combined}\")\n",
        "\n",
        "        # Store accuracy for this fold\n",
        "        fold_avg_accuracy = (val_accuracy_réponse + val_accuracy_combined) / 2\n",
        "        trial_fold_accuracies.append(fold_avg_accuracy)\n",
        "\n",
        "    # Calculate average accuracy for this trial (across all folds)\n",
        "    avg_trial_accuracy = sum(trial_fold_accuracies) / len(trial_fold_accuracies)\n",
        "    print(f\"Average accuracy for Trial {trial_num}: {avg_trial_accuracy}\")\n",
        "\n",
        "    # Track the best model based on validation accuracy\n",
        "    if avg_trial_accuracy > best_val_accuracy:\n",
        "        best_val_accuracy = avg_trial_accuracy\n",
        "        best_model_state = model.state_dict()  # Save the model's state dict for the best model\n",
        "\n",
        "    # Store the average accuracy for this trial\n",
        "    trial_accuracies.append(avg_trial_accuracy)\n",
        "\n",
        "# 3. Load the best model's state and apply it to the test set\n",
        "best_model = T5ForConditionalGeneration.from_pretrained('google/flan-t5-base')\n",
        "best_model.load_state_dict(best_model_state)  # Load the saved best model\n",
        "\n",
        "# Predictions on test set\n",
        "test_preds_réponse = test_df['réponse'].apply(lambda x: predict_sentiment(best_model, x))\n",
        "test_preds_combined = test_df['combined_qr'].apply(lambda x: predict_sentiment(best_model, x))\n",
        "\n",
        "# Calculate accuracy on test set for both 'réponse' and 'combined_qr'\n",
        "test_accuracy_réponse = accuracy_score(test_df['label'], test_preds_réponse)\n",
        "test_accuracy_combined = accuracy_score(test_df['label'], test_preds_combined)\n",
        "\n",
        "print(f\"Test Accuracy (réponse): {test_accuracy_réponse}\")\n",
        "print(f\"Test Accuracy (combined): {test_accuracy_combined}\")\n",
        "\n",
        "# Output results\n",
        "print(f\"Trial accuracies (validation set): {trial_accuracies}\")\n",
        "print(f\"Best validation accuracy: {best_val_accuracy}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3N4KD76jQKve",
        "outputId": "038d22bb-25c2-492a-d63e-e4127a4d2b80"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Numéro patient</th>\n",
              "      <th>item</th>\n",
              "      <th>modalités de réponse</th>\n",
              "      <th>libellé</th>\n",
              "      <th>réponse</th>\n",
              "      <th>échelle de Likert</th>\n",
              "      <th>gold_label_str</th>\n",
              "      <th>combined_qr</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>83</th>\n",
              "      <td>34</td>\n",
              "      <td>ACC4</td>\n",
              "      <td>0,1,2</td>\n",
              "      <td>Avez-vous trouvé que vous avez pu facilement f...</td>\n",
              "      <td>Oui, chaque fois que j'avais un imprévu, j’ai ...</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>Avez-vous trouvé que vous avez pu facilement f...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>53</th>\n",
              "      <td>4</td>\n",
              "      <td>ACC4</td>\n",
              "      <td>0,1,2</td>\n",
              "      <td>Avez-vous trouvé que vous avez pu facilement f...</td>\n",
              "      <td>Oui, chaque fois que j'avais besoin de changer...</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>Avez-vous trouvé que vous avez pu facilement f...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>70</th>\n",
              "      <td>21</td>\n",
              "      <td>ACC4</td>\n",
              "      <td>0,1,2</td>\n",
              "      <td>Avez-vous trouvé que vous avez pu facilement f...</td>\n",
              "      <td>Parfois c’était facile, mais à d’autres moment...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>Avez-vous trouvé que vous avez pu facilement f...</td>\n",
              "      <td>neutral</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>45</th>\n",
              "      <td>46</td>\n",
              "      <td>ACC1</td>\n",
              "      <td>0,1,2</td>\n",
              "      <td>Avez-vous trouvé que vous avez pu facilement c...</td>\n",
              "      <td>Oui, chaque fois que j'ai eu une question, j’a...</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>Avez-vous trouvé que vous avez pu facilement c...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>44</th>\n",
              "      <td>45</td>\n",
              "      <td>ACC1</td>\n",
              "      <td>0,1,2</td>\n",
              "      <td>Avez-vous trouvé que vous avez pu facilement c...</td>\n",
              "      <td>Globalement, ça s’est bien passé, mais il y a ...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>Avez-vous trouvé que vous avez pu facilement c...</td>\n",
              "      <td>neutral</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39</th>\n",
              "      <td>40</td>\n",
              "      <td>ACC1</td>\n",
              "      <td>0,1,2</td>\n",
              "      <td>Avez-vous trouvé que vous avez pu facilement c...</td>\n",
              "      <td>Oui, chaque fois que j'avais une inquiétude, j...</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>Avez-vous trouvé que vous avez pu facilement c...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>23</td>\n",
              "      <td>ACC1</td>\n",
              "      <td>0,1,2</td>\n",
              "      <td>Avez-vous trouvé que vous avez pu facilement c...</td>\n",
              "      <td>Non, j’ai souvent dû attendre longtemps pour o...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>Avez-vous trouvé que vous avez pu facilement c...</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>80</th>\n",
              "      <td>31</td>\n",
              "      <td>ACC4</td>\n",
              "      <td>0,1,2</td>\n",
              "      <td>Avez-vous trouvé que vous avez pu facilement f...</td>\n",
              "      <td>Oui, j’ai toujours pu faire reporter mes rende...</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>Avez-vous trouvé que vous avez pu facilement f...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>11</td>\n",
              "      <td>ACC1</td>\n",
              "      <td>0,1,2</td>\n",
              "      <td>Avez-vous trouvé que vous avez pu facilement c...</td>\n",
              "      <td>Non, il y avait souvent des délais importants ...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>Avez-vous trouvé que vous avez pu facilement c...</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>ACC1</td>\n",
              "      <td>0,1,2</td>\n",
              "      <td>Avez-vous trouvé que vous avez pu facilement c...</td>\n",
              "      <td>Oui, j’ai toujours pu joindre un professionnel...</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>Avez-vous trouvé que vous avez pu facilement c...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>19</td>\n",
              "      <td>ACC1</td>\n",
              "      <td>0,1,2</td>\n",
              "      <td>Avez-vous trouvé que vous avez pu facilement c...</td>\n",
              "      <td>Oui, j’ai toujours pu contacter un professionn...</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>Avez-vous trouvé que vous avez pu facilement c...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>31</td>\n",
              "      <td>ACC1</td>\n",
              "      <td>0,1,2</td>\n",
              "      <td>Avez-vous trouvé que vous avez pu facilement c...</td>\n",
              "      <td>Oui, j'ai pu facilement joindre un professionn...</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>Avez-vous trouvé que vous avez pu facilement c...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>73</th>\n",
              "      <td>24</td>\n",
              "      <td>ACC4</td>\n",
              "      <td>0,1,2</td>\n",
              "      <td>Avez-vous trouvé que vous avez pu facilement f...</td>\n",
              "      <td>La plupart du temps, j’ai pu déplacer mes rend...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>Avez-vous trouvé que vous avez pu facilement f...</td>\n",
              "      <td>neutral</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33</th>\n",
              "      <td>34</td>\n",
              "      <td>ACC1</td>\n",
              "      <td>0,1,2</td>\n",
              "      <td>Avez-vous trouvé que vous avez pu facilement c...</td>\n",
              "      <td>Oui, chaque fois que j'avais une question ou u...</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>Avez-vous trouvé que vous avez pu facilement c...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>90</th>\n",
              "      <td>41</td>\n",
              "      <td>ACC4</td>\n",
              "      <td>0,1,2</td>\n",
              "      <td>Avez-vous trouvé que vous avez pu facilement f...</td>\n",
              "      <td>Non, c'était souvent compliqué de trouver une ...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>Avez-vous trouvé que vous avez pu facilement f...</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>ACC1</td>\n",
              "      <td>0,1,2</td>\n",
              "      <td>Avez-vous trouvé que vous avez pu facilement c...</td>\n",
              "      <td>Non, j’ai souvent dû attendre longtemps avant ...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>Avez-vous trouvé que vous avez pu facilement c...</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>76</th>\n",
              "      <td>27</td>\n",
              "      <td>ACC4</td>\n",
              "      <td>0,1,2</td>\n",
              "      <td>Avez-vous trouvé que vous avez pu facilement f...</td>\n",
              "      <td>Ça dépendait de la période. Parfois c'était fa...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>Avez-vous trouvé que vous avez pu facilement f...</td>\n",
              "      <td>neutral</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>77</th>\n",
              "      <td>28</td>\n",
              "      <td>ACC4</td>\n",
              "      <td>0,1,2</td>\n",
              "      <td>Avez-vous trouvé que vous avez pu facilement f...</td>\n",
              "      <td>Oui, chaque fois que j’ai eu besoin de reporte...</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>Avez-vous trouvé que vous avez pu facilement f...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>13</td>\n",
              "      <td>ACC1</td>\n",
              "      <td>0,1,2</td>\n",
              "      <td>Avez-vous trouvé que vous avez pu facilement c...</td>\n",
              "      <td>Oui, j'ai toujours pu parler à quelqu'un rapid...</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>Avez-vous trouvé que vous avez pu facilement c...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31</th>\n",
              "      <td>32</td>\n",
              "      <td>ACC1</td>\n",
              "      <td>0,1,2</td>\n",
              "      <td>Avez-vous trouvé que vous avez pu facilement c...</td>\n",
              "      <td>Non, j’ai souvent dû faire plusieurs tentative...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>Avez-vous trouvé que vous avez pu facilement c...</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    Numéro patient  item modalités de réponse  \\\n",
              "83              34  ACC4                0,1,2   \n",
              "53               4  ACC4                0,1,2   \n",
              "70              21  ACC4                0,1,2   \n",
              "45              46  ACC1                0,1,2   \n",
              "44              45  ACC1                0,1,2   \n",
              "39              40  ACC1                0,1,2   \n",
              "22              23  ACC1                0,1,2   \n",
              "80              31  ACC4                0,1,2   \n",
              "10              11  ACC1                0,1,2   \n",
              "0                1  ACC1                0,1,2   \n",
              "18              19  ACC1                0,1,2   \n",
              "30              31  ACC1                0,1,2   \n",
              "73              24  ACC4                0,1,2   \n",
              "33              34  ACC1                0,1,2   \n",
              "90              41  ACC4                0,1,2   \n",
              "4                5  ACC1                0,1,2   \n",
              "76              27  ACC4                0,1,2   \n",
              "77              28  ACC4                0,1,2   \n",
              "12              13  ACC1                0,1,2   \n",
              "31              32  ACC1                0,1,2   \n",
              "\n",
              "                                              libellé  \\\n",
              "83  Avez-vous trouvé que vous avez pu facilement f...   \n",
              "53  Avez-vous trouvé que vous avez pu facilement f...   \n",
              "70  Avez-vous trouvé que vous avez pu facilement f...   \n",
              "45  Avez-vous trouvé que vous avez pu facilement c...   \n",
              "44  Avez-vous trouvé que vous avez pu facilement c...   \n",
              "39  Avez-vous trouvé que vous avez pu facilement c...   \n",
              "22  Avez-vous trouvé que vous avez pu facilement c...   \n",
              "80  Avez-vous trouvé que vous avez pu facilement f...   \n",
              "10  Avez-vous trouvé que vous avez pu facilement c...   \n",
              "0   Avez-vous trouvé que vous avez pu facilement c...   \n",
              "18  Avez-vous trouvé que vous avez pu facilement c...   \n",
              "30  Avez-vous trouvé que vous avez pu facilement c...   \n",
              "73  Avez-vous trouvé que vous avez pu facilement f...   \n",
              "33  Avez-vous trouvé que vous avez pu facilement c...   \n",
              "90  Avez-vous trouvé que vous avez pu facilement f...   \n",
              "4   Avez-vous trouvé que vous avez pu facilement c...   \n",
              "76  Avez-vous trouvé que vous avez pu facilement f...   \n",
              "77  Avez-vous trouvé que vous avez pu facilement f...   \n",
              "12  Avez-vous trouvé que vous avez pu facilement c...   \n",
              "31  Avez-vous trouvé que vous avez pu facilement c...   \n",
              "\n",
              "                                              réponse  échelle de Likert  \\\n",
              "83  Oui, chaque fois que j'avais un imprévu, j’ai ...                  2   \n",
              "53  Oui, chaque fois que j'avais besoin de changer...                  2   \n",
              "70  Parfois c’était facile, mais à d’autres moment...                  1   \n",
              "45  Oui, chaque fois que j'ai eu une question, j’a...                  2   \n",
              "44  Globalement, ça s’est bien passé, mais il y a ...                  1   \n",
              "39  Oui, chaque fois que j'avais une inquiétude, j...                  2   \n",
              "22  Non, j’ai souvent dû attendre longtemps pour o...                  0   \n",
              "80  Oui, j’ai toujours pu faire reporter mes rende...                  2   \n",
              "10  Non, il y avait souvent des délais importants ...                  0   \n",
              "0   Oui, j’ai toujours pu joindre un professionnel...                  2   \n",
              "18  Oui, j’ai toujours pu contacter un professionn...                  2   \n",
              "30  Oui, j'ai pu facilement joindre un professionn...                  2   \n",
              "73  La plupart du temps, j’ai pu déplacer mes rend...                  1   \n",
              "33  Oui, chaque fois que j'avais une question ou u...                  2   \n",
              "90  Non, c'était souvent compliqué de trouver une ...                  0   \n",
              "4   Non, j’ai souvent dû attendre longtemps avant ...                  0   \n",
              "76  Ça dépendait de la période. Parfois c'était fa...                  1   \n",
              "77  Oui, chaque fois que j’ai eu besoin de reporte...                  2   \n",
              "12  Oui, j'ai toujours pu parler à quelqu'un rapid...                  2   \n",
              "31  Non, j’ai souvent dû faire plusieurs tentative...                  0   \n",
              "\n",
              "   gold_label_str                                        combined_qr     label  \n",
              "83              2  Avez-vous trouvé que vous avez pu facilement f...  positive  \n",
              "53              2  Avez-vous trouvé que vous avez pu facilement f...  positive  \n",
              "70              1  Avez-vous trouvé que vous avez pu facilement f...   neutral  \n",
              "45              2  Avez-vous trouvé que vous avez pu facilement c...  positive  \n",
              "44              1  Avez-vous trouvé que vous avez pu facilement c...   neutral  \n",
              "39              2  Avez-vous trouvé que vous avez pu facilement c...  positive  \n",
              "22              0  Avez-vous trouvé que vous avez pu facilement c...  negative  \n",
              "80              2  Avez-vous trouvé que vous avez pu facilement f...  positive  \n",
              "10              0  Avez-vous trouvé que vous avez pu facilement c...  negative  \n",
              "0               2  Avez-vous trouvé que vous avez pu facilement c...  positive  \n",
              "18              2  Avez-vous trouvé que vous avez pu facilement c...  positive  \n",
              "30              2  Avez-vous trouvé que vous avez pu facilement c...  positive  \n",
              "73              1  Avez-vous trouvé que vous avez pu facilement f...   neutral  \n",
              "33              2  Avez-vous trouvé que vous avez pu facilement c...  positive  \n",
              "90              0  Avez-vous trouvé que vous avez pu facilement f...  negative  \n",
              "4               0  Avez-vous trouvé que vous avez pu facilement c...  negative  \n",
              "76              1  Avez-vous trouvé que vous avez pu facilement f...   neutral  \n",
              "77              2  Avez-vous trouvé que vous avez pu facilement f...  positive  \n",
              "12              2  Avez-vous trouvé que vous avez pu facilement c...  positive  \n",
              "31              0  Avez-vous trouvé que vous avez pu facilement c...  negative  "
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "display(test_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xW1OdA9OQKve",
        "outputId": "ea43d980-a6eb-4d5b-dc35-7b7773678f02"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Index: 20 entries, 83 to 31\n",
            "Data columns (total 9 columns):\n",
            " #   Column                Non-Null Count  Dtype \n",
            "---  ------                --------------  ----- \n",
            " 0   Numéro patient        20 non-null     int64 \n",
            " 1   item                  20 non-null     object\n",
            " 2   modalités de réponse  20 non-null     object\n",
            " 3   libellé               20 non-null     object\n",
            " 4   réponse               20 non-null     object\n",
            " 5   échelle de Likert     20 non-null     int64 \n",
            " 6   gold_label_str        20 non-null     object\n",
            " 7   combined_qr           20 non-null     object\n",
            " 8   label                 20 non-null     object\n",
            "dtypes: int64(2), object(7)\n",
            "memory usage: 1.6+ KB\n"
          ]
        }
      ],
      "source": [
        "test_df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hHZqHHcmQKvf",
        "outputId": "50687afb-f0b2-4528-ac6b-6f221c9007a5",
        "colab": {
          "referenced_widgets": [
            "ce56ac305ff24944a3a7b985efccc0a0"
          ]
        }
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ce56ac305ff24944a3a7b985efccc0a0",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/3 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/mnt/c/Users/Shiqi/Desktop/APHM/env001/lib/python3.12/site-packages/transformers/training_args.py:1545: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n",
            "/mnt/c/Users/Shiqi/Desktop/APHM/env001/lib/python3.12/site-packages/transformers/training_args.py:1545: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "ename": "AttributeError",
          "evalue": "`AcceleratorState` object has no attribute `distributed_type`. This happens if `AcceleratorState._reset_state()` was called and an `Accelerator` or `PartialState` was not reinitialized.",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[52], line 54\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Trainer, TrainingArguments\n\u001b[1;32m     44\u001b[0m training_args \u001b[38;5;241m=\u001b[39m TrainingArguments(\n\u001b[1;32m     45\u001b[0m     output_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./results\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     46\u001b[0m     evaluation_strategy\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mepoch\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     51\u001b[0m     weight_decay\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.01\u001b[39m,\n\u001b[1;32m     52\u001b[0m )\n\u001b[0;32m---> 54\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict_sentiment\u001b[39m(text):\n\u001b[1;32m     58\u001b[0m     input_text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclassify sentiment: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtext\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n",
            "File \u001b[0;32m/mnt/c/Users/Shiqi/Desktop/APHM/env001/lib/python3.12/site-packages/transformers/trainer.py:3861\u001b[0m, in \u001b[0;36mTrainer.evaluate\u001b[0;34m(self, eval_dataset, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[1;32m   3858\u001b[0m \u001b[38;5;66;03m# memory metrics - must set up as early as possible\u001b[39;00m\n\u001b[1;32m   3859\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_memory_tracker\u001b[38;5;241m.\u001b[39mstart()\n\u001b[0;32m-> 3861\u001b[0m eval_dataloader \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_eval_dataloader\u001b[49m\u001b[43m(\u001b[49m\u001b[43meval_dataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3862\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_fsdp_xla_v2_enabled:\n\u001b[1;32m   3863\u001b[0m     eval_dataloader \u001b[38;5;241m=\u001b[39m tpu_spmd_dataloader(eval_dataloader)\n",
            "File \u001b[0;32m/mnt/c/Users/Shiqi/Desktop/APHM/env001/lib/python3.12/site-packages/transformers/trainer.py:1013\u001b[0m, in \u001b[0;36mTrainer.get_eval_dataloader\u001b[0;34m(self, eval_dataset)\u001b[0m\n\u001b[1;32m   1010\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1011\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_eval_dataloaders \u001b[38;5;241m=\u001b[39m {dataloader_key: eval_dataloader}\n\u001b[0;32m-> 1013\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maccelerator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprepare\u001b[49m\u001b[43m(\u001b[49m\u001b[43meval_dataloader\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/mnt/c/Users/Shiqi/Desktop/APHM/env001/lib/python3.12/site-packages/accelerate/accelerator.py:1305\u001b[0m, in \u001b[0;36mAccelerator.prepare\u001b[0;34m(self, device_placement, *args)\u001b[0m\n\u001b[1;32m   1294\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   1295\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(obj, torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mModule)\n\u001b[1;32m   1296\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverify_device_map(obj)\n\u001b[1;32m   1297\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdistributed_type \u001b[38;5;241m!=\u001b[39m DistributedType\u001b[38;5;241m.\u001b[39mNO\n\u001b[1;32m   1298\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m os\u001b[38;5;241m.\u001b[39menviron\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mACCELERATE_BYPASS_DEVICE_MAP\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfalse\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrue\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1299\u001b[0m     ):\n\u001b[1;32m   1300\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1301\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou can\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt train a model that has been loaded with `device_map=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m` in any distributed mode.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1302\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m Please rerun your script specifying `--num_processes=1` or by launching with `python \u001b[39m\u001b[38;5;124m{{\u001b[39m\u001b[38;5;124mmyscript.py}}`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1303\u001b[0m         )\n\u001b[0;32m-> 1305\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdistributed_type\u001b[49m \u001b[38;5;241m==\u001b[39m DistributedType\u001b[38;5;241m.\u001b[39mDEEPSPEED:\n\u001b[1;32m   1306\u001b[0m     model_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m   1307\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m args:\n",
            "File \u001b[0;32m/mnt/c/Users/Shiqi/Desktop/APHM/env001/lib/python3.12/site-packages/accelerate/accelerator.py:579\u001b[0m, in \u001b[0;36mAccelerator.distributed_type\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    577\u001b[0m \u001b[38;5;129m@property\u001b[39m\n\u001b[1;32m    578\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdistributed_type\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 579\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstate\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdistributed_type\u001b[49m\n",
            "File \u001b[0;32m/mnt/c/Users/Shiqi/Desktop/APHM/env001/lib/python3.12/site-packages/accelerate/state.py:1125\u001b[0m, in \u001b[0;36mAcceleratorState.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1121\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getattr__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name: \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m   1122\u001b[0m     \u001b[38;5;66;03m# By this point we know that no attributes of `self` contain `name`,\u001b[39;00m\n\u001b[1;32m   1123\u001b[0m     \u001b[38;5;66;03m# so we just modify the error message\u001b[39;00m\n\u001b[1;32m   1124\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_known_attrs:\n\u001b[0;32m-> 1125\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\n\u001b[1;32m   1126\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`AcceleratorState` object has no attribute `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m`. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1127\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis happens if `AcceleratorState._reset_state()` was called and \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1128\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124man `Accelerator` or `PartialState` was not reinitialized.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1129\u001b[0m         )\n\u001b[1;32m   1130\u001b[0m     \u001b[38;5;66;03m# Raise a typical AttributeError\u001b[39;00m\n\u001b[1;32m   1131\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAcceleratorState\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
            "\u001b[0;31mAttributeError\u001b[0m: `AcceleratorState` object has no attribute `distributed_type`. This happens if `AcceleratorState._reset_state()` was called and an `Accelerator` or `PartialState` was not reinitialized."
          ]
        }
      ],
      "source": [
        "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
        "\n",
        "# Load pre-trained T5 tokenizer and model\n",
        "model_name = \"t5-small\"  # or another variant like \"t5-base\", \"t5-large\"\n",
        "tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
        "model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
        "\n",
        "from datasets import Dataset\n",
        "\n",
        "# Example data\n",
        "data = {\n",
        "    \"text\": [\"I love this product!\", \"This is terrible.\", \"It's okay.\"],\n",
        "    \"sentiment\": [\"positive\", \"negative\", \"neutral\"]\n",
        "}\n",
        "\n",
        "# Create a Dataset\n",
        "dataset = Dataset.from_dict(data)\n",
        "\n",
        "# Tokenize the inputs and labels\n",
        "def preprocess_function(examples):\n",
        "    inputs = [f\"classify sentiment: {text}\" for text in examples['text']]\n",
        "    targets = examples['sentiment']\n",
        "    model_inputs = tokenizer(inputs, max_length=512, truncation=True)\n",
        "    labels = tokenizer(targets, max_length=16, truncation=True).input_ids\n",
        "    model_inputs[\"labels\"] = labels\n",
        "    return model_inputs\n",
        "\n",
        "tokenized_dataset = dataset.map(preprocess_function, batched=True)\n",
        "\n",
        "from transformers import Trainer, TrainingArguments\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    learning_rate=5e-5,\n",
        "    per_device_train_batch_size=8,\n",
        "    per_device_eval_batch_size=8,\n",
        "    num_train_epochs=3,\n",
        "    weight_decay=0.01,\n",
        ")\n",
        "\n",
        "from transformers import Trainer, TrainingArguments\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    learning_rate=5e-5,\n",
        "    per_device_train_batch_size=8,\n",
        "    per_device_eval_batch_size=8,\n",
        "    num_train_epochs=3,\n",
        "    weight_decay=0.01,\n",
        ")\n",
        "\n",
        "trainer.evaluate()\n",
        "\n",
        "\n",
        "def predict_sentiment(text):\n",
        "    input_text = f\"classify sentiment: {text}\"\n",
        "    input_ids = tokenizer.encode(input_text, return_tensors=\"pt\")\n",
        "    outputs = model.generate(input_ids)\n",
        "    prediction = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    return prediction\n",
        "\n",
        "# Example prediction\n",
        "print(predict_sentiment(\"I really enjoyed this!\"))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tv0CXgfxQKvf",
        "outputId": "29ab2e60-e07e-48ca-cd87-d929b2b70e13"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Transformers version: 4.45.2\n",
            "Accelerate version: 1.0.0\n"
          ]
        }
      ],
      "source": [
        "from transformers import __version__ as transformers_version\n",
        "from accelerate import __version__ as accelerate_version\n",
        "\n",
        "print(f\"Transformers version: {transformers_version}\")\n",
        "print(f\"Accelerate version: {accelerate_version}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1gpHXnATQKvf"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}